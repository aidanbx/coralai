Why a New Programming Language | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatdevelopdevelopv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiWhy a New Programming LanguageGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Why Taichi>>Why a New Programming LanguageThis is unreleased documentation for Taichi Docs develop version.For up-to-date documentation, see the latest version (v1.6.0).Version: developWhy a New Programming LanguageImagine you'd like to write a new particle-based fluid algorithm. You started simple, didn't spend much time before finding a reference C++/CUDA work online (or derived the work from your labmate, unfortunately). cmake .. && make, you typed. Oops, cmake threw out an error due to a random incompatible third party library. Installed and rebuilt, now it passed. Then you ran it, which immediately segfaulted (without any stacktrace, of course). Then you started gazing at the code, placed the necessary asset files at the right place, fixed a few dangling pointers and reran. It... actually worked, until you plugged in your revised algorithm. Now another big fight with the GPU or CPU code. More often than not, you get lost in the language details.If all these sound too familiar to you, congratulations, you are probably looking at the right solution.Born from the MIT CSAIL lab, Taichi was designed to facilitate computer graphics researchers' everyday life, by helping them quickly implement visual computing and physics simulation algorithms that are executable on GPU. The path Taichi took was an innovative one: Taichi is embedded in Python and uses modern just-in-time (JIT) frameworks (for example LLVM, SPIR-V) to offload the Python source code to native GPU or CPU instructions, offering the performance at both development time and runtime.To be fair, a domain-specific language (DSL) with a Python frontend is not something new. In the past few years, frameworks like Halide, PyTorch, and TVM have matured into the de facto standards in areas such as image processing and deep learning (DL). What distinguishes Taichi the most from these frameworks is its imperative programming paradigm. As a DSL, Taichi is not so specialized in a particular computing pattern. This provides better flexibility. While one may argue that flexibility usually comes at the cost of not being fully optimized, we often find this not the case for a few reasons:Taichi's workload typically does not exhibit an exploitable pattern (e.g., element-wise operations), meaning that the arithmetic intensity is bounded anyway. By simply switching to the GPU backend, one can already enjoy a nice performance gain.Unlike the traditional DL frameworks, where operators are simple math expressions and have to be fused at the graph level to achieve higher arithmetic intensity, Taichi's imperative paradigm makes it quite easy to write a large amount of computation in a single kernel. We call it mega-kernel.Taichi heavily optimizes the source code using various compiler technologies: common subexpression elimination, dead code elimination, control flow graph analysis, etc. These optimizations are backend neutral, because Taichi hosts its own intermediate representation (IR) layer.JIT compilation provides additional optimization opportunities.That said, Taichi goes beyond a Python JIT transpiler. One of the initial design goals is to decouple the computation from the data structures. The mechanism that Taichi provides is a set of generic data containers, called SNode (/ˈsnoʊd/). SNodes can be used to compose hierarchical, dense or sparse, multi-dimensional fields conveniently. Switching between array-of-structures and structure-of-arrays layouts is usually a matter of ≤10 lines of code. This has sparked many use cases in numerical simulation. If you are interested to learn them, please check out Fields (advanced), Spatially Sparse Data Structures, or the original Taichi paper.The concept of decoupling is further extended to the type system. With GPU memory capacity and bandwidth becoming the major bottlenecks nowadays, it is vital to be able to pack more data per memory unit. Since 2021, Taichi has introduced customizable quantized types, allowing for the definition of fixed point or floating point numbers with arbitrary bits (still needs to be under 64). This has allowed an MPM simulation of over 400 million particles on a single GPU device. Learn more details in the QuanTaichi paper.Taichi is intuitive. If you know Python, you know Taichi. If you write Taichi, you awaken your GPU (or CPU as a fallback). Ever since its debut, this simple idea has gained so much popularity, that many were attracted to contribute new backends, including Vulkan, OpenGL and DirectX (working in progress). Without our strong and dedicated community, Taichi would never have been where it is now.Going forward, we see many new opportunities lying ahead, and would like to share some of our vision with you.Academia90% of the research code will be trashed due to the nature of research where assumptions keep being broken and ideas keep being iterated. Swiftly coding without thinking too much about performance may lead to incorrect conclusions, while pre-matured code optimization can be a waste of time and often produces a tangled mess. The high performance and productivity are, therefore, extremely helpful for research projects.Taichi will keep embracing the academia. The key features we have (or plan to have) for high-performance computing research projects include small-scale linear algebra (inside kernels), large-scale sparse systems, and efficient neighbor accessing for both structured and unstructured data.Taichi also provides an automatic differentiation module via source code transformation (at IR level), making it a sweet differentiable simulation tool for machine learning projects.Apps & game engine integrationOne huge advantange of Taichi lies in its portability, thanks to the support for a wide variety of backends. During the development process, we have also recognized the increasing demands from our industry users for multi-platform packaging and deployment. Below shows an experimental demo of integrating Taichi with Unity. By exporting Taichi kernels as SPIR-V shaders, we can easily import them into a Unity project.General-purpose computingWhile originally designed for physics simulation, Taichi has found its application in many other areas that can be boosted by GPU general-purpose computing.taichimd: Interactive, GPU-accelerated Molecular (& Macroscopic) Dynamics using the Taichi programming languageTaichiSLAM: a 3D Dense mapping backend library of SLAM based Taichi-Lang, designed for the aerial swarm.Stannum: Fusing Taichi into PyTorch.Maybe a new frontend?The benefit of adopting the compiler approach is that you can decouple the frontend from the backend. Taichi is currently embedded in Python, but who says it needs to stay that way? Stay tuned :-)Edit this pageLast updated on 9/20/2022 by Taichi GardenerWas this helpful?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Why a New Programming Language | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiWhy a New Programming LanguageGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Why Taichi>>Why a New Programming LanguageVersion: v1.6.0Why a New Programming LanguageImagine you'd like to write a new particle-based fluid algorithm. You started simple, didn't spend much time before finding a reference C++/CUDA work online (or derived the work from your labmate, unfortunately). cmake .. && make, you typed. Oops, cmake threw out an error due to a random incompatible third party library. Installed and rebuilt, now it passed. Then you ran it, which immediately segfaulted (without any stacktrace, of course). Then you started gazing at the code, placed the necessary asset files at the right place, fixed a few dangling pointers and reran. It... actually worked, until you plugged in your revised algorithm. Now another big fight with the GPU or CPU code. More often than not, you get lost in the language details.If all these sound too familiar to you, congratulations, you are probably looking at the right solution.Born from the MIT CSAIL lab, Taichi was designed to facilitate computer graphics researchers' everyday life, by helping them quickly implement visual computing and physics simulation algorithms that are executable on GPU. The path Taichi took was an innovative one: Taichi is embedded in Python and uses modern just-in-time (JIT) frameworks (for example LLVM, SPIR-V) to offload the Python source code to native GPU or CPU instructions, offering the performance at both development time and runtime.To be fair, a domain-specific language (DSL) with a Python frontend is not something new. In the past few years, frameworks like Halide, PyTorch, and TVM have matured into the de facto standards in areas such as image processing and deep learning (DL). What distinguishes Taichi the most from these frameworks is its imperative programming paradigm. As a DSL, Taichi is not so specialized in a particular computing pattern. This provides better flexibility. While one may argue that flexibility usually comes at the cost of not being fully optimized, we often find this not the case for a few reasons:Taichi's workload typically does not exhibit an exploitable pattern (e.g., element-wise operations), meaning that the arithmetic intensity is bounded anyway. By simply switching to the GPU backend, one can already enjoy a nice performance gain.Unlike the traditional DL frameworks, where operators are simple math expressions and have to be fused at the graph level to achieve higher arithmetic intensity, Taichi's imperative paradigm makes it quite easy to write a large amount of computation in a single kernel. We call it mega-kernel.Taichi heavily optimizes the source code using various compiler technologies: common subexpression elimination, dead code elimination, control flow graph analysis, etc. These optimizations are backend neutral, because Taichi hosts its own intermediate representation (IR) layer.JIT compilation provides additional optimization opportunities.That said, Taichi goes beyond a Python JIT transpiler. One of the initial design goals is to decouple the computation from the data structures. The mechanism that Taichi provides is a set of generic data containers, called SNode (/ˈsnoʊd/). SNodes can be used to compose hierarchical, dense or sparse, multi-dimensional fields conveniently. Switching between array-of-structures and structure-of-arrays layouts is usually a matter of ≤10 lines of code. This has sparked many use cases in numerical simulation. If you are interested to learn them, please check out Fields (advanced), Spatially Sparse Data Structures, or the original Taichi paper.The concept of decoupling is further extended to the type system. With GPU memory capacity and bandwidth becoming the major bottlenecks nowadays, it is vital to be able to pack more data per memory unit. Since 2021, Taichi has introduced customizable quantized types, allowing for the definition of fixed point or floating point numbers with arbitrary bits (still needs to be under 64). This has allowed an MPM simulation of over 400 million particles on a single GPU device. Learn more details in the QuanTaichi paper.Taichi is intuitive. If you know Python, you know Taichi. If you write Taichi, you awaken your GPU (or CPU as a fallback). Ever since its debut, this simple idea has gained so much popularity, that many were attracted to contribute new backends, including Vulkan, OpenGL and DirectX (working in progress). Without our strong and dedicated community, Taichi would never have been where it is now.Going forward, we see many new opportunities lying ahead, and would like to share some of our vision with you.Academia90% of the research code will be trashed due to the nature of research where assumptions keep being broken and ideas keep being iterated. Swiftly coding without thinking too much about performance may lead to incorrect conclusions, while pre-matured code optimization can be a waste of time and often produces a tangled mess. The high performance and productivity are, therefore, extremely helpful for research projects.Taichi will keep embracing the academia. The key features we have (or plan to have) for high-performance computing research projects include small-scale linear algebra (inside kernels), large-scale sparse systems, and efficient neighbor accessing for both structured and unstructured data.Taichi also provides an automatic differentiation module via source code transformation (at IR level), making it a sweet differentiable simulation tool for machine learning projects.Apps & game engine integrationOne huge advantange of Taichi lies in its portability, thanks to the support for a wide variety of backends. During the development process, we have also recognized the increasing demands from our industry users for multi-platform packaging and deployment. Below shows an experimental demo of integrating Taichi with Unity. By exporting Taichi kernels as SPIR-V shaders, we can easily import them into a Unity project.General-purpose computingWhile originally designed for physics simulation, Taichi has found its application in many other areas that can be boosted by GPU general-purpose computing.taichimd: Interactive, GPU-accelerated Molecular (& Macroscopic) Dynamics using the Taichi programming languageTaichiSLAM: a 3D Dense mapping backend library of SLAM based Taichi-Lang, designed for the aerial swarm.Stannum: Fusing Taichi into PyTorch.Maybe a new frontend?The benefit of adopting the compiler approach is that you can decouple the frontend from the backend. Taichi is currently embedded in Python, but who says it needs to stay that way? Stay tuned :-)Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Why a New Programming Language | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.5.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiWhy a New Programming LanguageGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Why Taichi>>Why a New Programming LanguageThis is documentation for Taichi Docs v1.5.0, which is no longer actively maintained.For up-to-date documentation, see the latest version (v1.6.0).Version: v1.5.0Why a New Programming LanguageImagine you'd like to write a new particle-based fluid algorithm. You started simple, didn't spend much time before finding a reference C++/CUDA work online (or derived the work from your labmate, unfortunately). cmake .. && make, you typed. Oops, cmake threw out an error due to a random incompatible third party library. Installed and rebuilt, now it passed. Then you ran it, which immediately segfaulted (without any stacktrace, of course). Then you started gazing at the code, placed the necessary asset files at the right place, fixed a few dangling pointers and reran. It... actually worked, until you plugged in your revised algorithm. Now another big fight with the GPU or CPU code. More often than not, you get lost in the language details.If all these sound too familiar to you, congratulations, you are probably looking at the right solution.Born from the MIT CSAIL lab, Taichi was designed to facilitate computer graphics researchers' everyday life, by helping them quickly implement visual computing and physics simulation algorithms that are executable on GPU. The path Taichi took was an innovative one: Taichi is embedded in Python and uses modern just-in-time (JIT) frameworks (for example LLVM, SPIR-V) to offload the Python source code to native GPU or CPU instructions, offering the performance at both development time and runtime.To be fair, a domain-specific language (DSL) with a Python frontend is not something new. In the past few years, frameworks like Halide, PyTorch, and TVM have matured into the de facto standards in areas such as image processing and deep learning (DL). What distinguishes Taichi the most from these frameworks is its imperative programming paradigm. As a DSL, Taichi is not so specialized in a particular computing pattern. This provides better flexibility. While one may argue that flexibility usually comes at the cost of not being fully optimized, we often find this not the case for a few reasons:Taichi's workload typically does not exhibit an exploitable pattern (e.g., element-wise operations), meaning that the arithmetic intensity is bounded anyway. By simply switching to the GPU backend, one can already enjoy a nice performance gain.Unlike the traditional DL frameworks, where operators are simple math expressions and have to be fused at the graph level to achieve higher arithmetic intensity, Taichi's imperative paradigm makes it quite easy to write a large amount of computation in a single kernel. We call it mega-kernel.Taichi heavily optimizes the source code using various compiler technologies: common subexpression elimination, dead code elimination, control flow graph analysis, etc. These optimizations are backend neutral, because Taichi hosts its own intermediate representation (IR) layer.JIT compilation provides additional optimization opportunities.That said, Taichi goes beyond a Python JIT transpiler. One of the initial design goals is to decouple the computation from the data structures. The mechanism that Taichi provides is a set of generic data containers, called SNode (/ˈsnoʊd/). SNodes can be used to compose hierarchical, dense or sparse, multi-dimensional fields conveniently. Switching between array-of-structures and structure-of-arrays layouts is usually a matter of ≤10 lines of code. This has sparked many use cases in numerical simulation. If you are interested to learn them, please check out Fields (advanced), Spatially Sparse Data Structures, or the original Taichi paper.The concept of decoupling is further extended to the type system. With GPU memory capacity and bandwidth becoming the major bottlenecks nowadays, it is vital to be able to pack more data per memory unit. Since 2021, Taichi has introduced customizable quantized types, allowing for the definition of fixed point or floating point numbers with arbitrary bits (still needs to be under 64). This has allowed an MPM simulation of over 400 million particles on a single GPU device. Learn more details in the QuanTaichi paper.Taichi is intuitive. If you know Python, you know Taichi. If you write Taichi, you awaken your GPU (or CPU as a fallback). Ever since its debut, this simple idea has gained so much popularity, that many were attracted to contribute new backends, including Vulkan, OpenGL and DirectX (working in progress). Without our strong and dedicated community, Taichi would never have been where it is now.Going forward, we see many new opportunities lying ahead, and would like to share some of our vision with you.Academia90% of the research code will be trashed due to the nature of research where assumptions keep being broken and ideas keep being iterated. Swiftly coding without thinking too much about performance may lead to incorrect conclusions, while pre-matured code optimization can be a waste of time and often produces a tangled mess. The high performance and productivity are, therefore, extremely helpful for research projects.Taichi will keep embracing the academia. The key features we have (or plan to have) for high-performance computing research projects include small-scale linear algebra (inside kernels), large-scale sparse systems, and efficient neighbor accessing for both structured and unstructured data.Taichi also provides an automatic differentiation module via source code transformation (at IR level), making it a sweet differentiable simulation tool for machine learning projects.Apps & game engine integrationOne huge advantange of Taichi lies in its portability, thanks to the support for a wide variety of backends. During the development process, we have also recognized the increasing demands from our industry users for multi-platform packaging and deployment. Below shows an experimental demo of integrating Taichi with Unity. By exporting Taichi kernels as SPIR-V shaders, we can easily import them into a Unity project.General-purpose computingWhile originally designed for physics simulation, Taichi has found its application in many other areas that can be boosted by GPU general-purpose computing.taichimd: Interactive, GPU-accelerated Molecular (& Macroscopic) Dynamics using the Taichi programming languageTaichiSLAM: a 3D Dense mapping backend library of SLAM based Taichi-Lang, designed for the aerial swarm.Stannum: Fusing Taichi into PyTorch.Maybe a new frontend?The benefit of adopting the compiler approach is that you can decouple the frontend from the backend. Taichi is currently embedded in Python, but who says it needs to stay that way? Stay tuned :-)Edit this pageLast updated on 3/27/2023 by Taichi GardenerWas this helpful?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Glossary | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>GlossaryVersion: v1.6.0On this pageGlossaryAbstract syntax tree (AST)https://en.wikipedia.org/wiki/Abstract_syntax_treeAhead-of-time (AOT)https://en.wikipedia.org/wiki/Ahead-of-time_compilationAliasinghttps://en.wikipedia.org/wiki/Aliasing_(computing)Annotationhttps://docs.python.org/3/glossary.htmlArray of structures (AOS)https://en.wikipedia.org/wiki/AoS_and_SoASee also structure of arrays.Assert statementhttps://docs.python.org/3/reference/simple_stmts.html#the-assert-statementAtomic operationhttps://wiki.osdev.org/Atomic_operationAugmented assignmenthttps://en.wikipedia.org/wiki/Augmented_assignmentAutomatic differentiationhttps://en.wikipedia.org/wiki/Automatic_differentiationBitmaskhttps://en.wikipedia.org/wiki/Mask_(computing)Column-major orderhttps://en.wikipedia.org/wiki/Row-_and_column-major_orderSee also row-major order.Compound typeA compound type is a user-defined array-like or struct-like data type which comprises multiple members of primitive types or other compound types. Supported compound types in Taichi include vectors, metrics, and structs.Compute shaderhttps://www.khronos.org/opengl/wiki/Compute_ShaderCoordinate offsetA coordinate offset refers to a value added to another base value, which is an element in a Taichi field.You can use offsets when defining a field to move the field boundaries.Data-oriented programming (DOP)https://en.wikipedia.org/wiki/Data-oriented_designData racehttps://en.wikipedia.org/wiki/Race_condition#Data_raceDifferentiable programminghttps://en.wikipedia.org/wiki/Differentiable_programmingDomain-specific language (DSL)https://en.wikipedia.org/wiki/Domain-specific_languageExternal arrayExternal arrays refer to data containers available in the Python scope.Taichi supports interaction with the following external arrays - Numpy arrays, PyTorch tensors, and Paddle tensors.FieldA field is a multi-dimensional array of elements. The elements it accepts can be a scalar, a vector, a matrix, or a struct. It is a global data container provided by Taichi and can be accessed from both the Python scope and the Taichi scope.Field shapeThe shape of a field is the number of elements in each dimension.Fragment shaderhttps://en.wikipedia.org/wiki/Shader#Pixel_shadersGlobal variablehttps://en.wikipedia.org/wiki/Global_variableGrid-Stride Loophttps://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/Imperative programminghttps://en.wikipedia.org/wiki/Imperative_programmingInstantiationhttps://en.wikipedia.org/wiki/Instance_(computer_science)Intermediate representation (IR)https://en.wikipedia.org/wiki/Intermediate_representationJust-in-time (JIT) compilationhttps://en.wikipedia.org/wiki/Just-in-time_compilationKernelA kernel is a function decorated with @ti.kernel. A kernel serves as the entry point where Taichi begins to take over the tasks, and it must be called directly by Python code.Lexical-scopedhttps://en.wikipedia.org/wiki/Scope_(computer_science)#Lexical_scopeLocal variablehttps://en.wikipedia.org/wiki/Local_variableLoop unrollinghttps://en.wikipedia.org/wiki/Loop_unrollingMegakernelA megakernel is a Taichi kernel that can deal with a large amount of computation to achieve high arithmetic intensity.MetadataMetadata refers to the two fundamental attributes of a Taichi field, i.e., data type and shape.Use field.dtype and field.shape to retrieve the metadata.Metaprogramminghttps://en.wikipedia.org/wiki/MetaprogrammingObject-oriented programming (OOP)https://en.wikipedia.org/wiki/Object-oriented_programmingPlain old data (POD)https://en.wikipedia.org/wiki/Passive_data_structurePrimitive typePrimitive data types are commonly-used numerical data types from which all other data types are constructed. Supported primitive data types in Taichi include ti.i32 (int32), ti.u8 (uint8), and ti.f64 (float64)Python scopeCode outside of the Taichi scope is in the Python scope. The code in the Python scope is native Python and executed by Python's virtual machine, not by Taichi's runtime.The Python scope corresponds to the host side in CUDA.Row-major orderhttps://en.wikipedia.org/wiki/Row-_and_column-major_orderSee also coloum-major order.Shader storage buffer object (SSBO)https://www.khronos.org/opengl/wiki/Shader_Storage_Buffer_ObjectSparse matrixA matrix is a two-dimensional data object made of m rows and n columns. If a matrix is comprised of mostly zero values, then it is a sparse matrix.Taichi provides APIs for sparse matrices.Static scopeA static scope is a scope of the argument of ti.static, which is a hint for the compiler to evaluate the argument at compile time.Static single assignment (SSA)https://en.wikipedia.org/wiki/Static_single-assignment_formStructure of arrays (SOA)https://en.wikipedia.org/wiki/AoS_and_SoASee also array of structures.Taichi functionA Taichi function is a function decorated with @ti.func.A Taichi function must be called from inside a kernel or from inside another Taichi function.Taichi scopeThe code inside a kernel or a Taichi function is in the Taichi scope. The code in the Taichi scope is compiled by Taichi's runtime and executed in parallel on CPU or GPU devices for high-performance computation.The Taichi scope corresponds to the device side in CUDA.Template signatureTemplate signatures are what distinguish different instantiations of a kernel template.For example, The signature of add(x, 42) is (x, ti.i32), which is the same as that of add(x, 1). Therefore, the latter can reuse the previously compiled binary. The signature of add(y, 42) is (y, ti.i32), a different value from the previous signature, hence a new kernel will be instantiated and compiled.Thread  local storage (TLS)https://en.wikipedia.org/wiki/Thread-local_storageTracebackhttps://en.wikipedia.org/wiki/Stack_traceVertex shaderhttps://en.wikipedia.org/wiki/Shader#Vertex_shadersEdit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Abstract syntax tree (AST)Ahead-of-time (AOT)AliasingAnnotationArray of structures (AOS)Assert statementAtomic operationAugmented assignmentAutomatic differentiationBitmaskColumn-major orderCompound typeCompute shaderCoordinate offsetData-oriented programming (DOP)Data raceDifferentiable programmingDomain-specific language (DSL)External arrayFieldField shapeFragment shaderGlobal variableGrid-Stride LoopImperative programmingInstantiationIntermediate representation (IR)Just-in-time (JIT) compilationKernelLexical-scopedLocal variableLoop unrollingMegakernelMetadataMetaprogrammingObject-oriented programming (OOP)Plain old data (POD)Primitive typePython scopeRow-major orderShader storage buffer object (SSBO)Sparse matrixStatic scopeStatic single assignment (SSA)Structure of arrays (SOA)Taichi functionTaichi scopeTemplate signatureThread  local storage (TLS)TracebackVertex shaderCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Hello, World! | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedHello, World!Accelerate Python with TaichiConduct Physical SimulationAccelerate PyTorch with TaichiKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Get Started>>Hello, World!Version: v1.6.0On this pageHello, World!Taichi is a domain-specific language designed for high-performance, parallel computing, and is embedded in Python.When writing compute-intensive tasks, users can leverage Taichi's high performance computation by following a set of extra rules, and making use of the two decorators @ti.func and @ti.kernel. These decorators instruct Taichi to take over the computation tasks and compile the decorated functions to machine code using its just-in-time (JIT) compiler. As a result, calls to these functions are executed on multi-core CPUs or GPUs and can achieve acceleration by 50x~100x compared to native Python code.Additionally, Taichi also has an ahead-of-time (AOT) system for exporting code to binary/shader files that can be run without the Python environment. See Tutorial: Run Taichi programs in C++ application for more information.PrerequisitesPython: 3.7/3.8/3.9/3.10 (64-bit)OS: Windows, OS X, and Linux (64-bit)InstallationTaichi is available as a PyPI package:pip install taichiCopyHello, world!A basic fractal example, the Julia fractal, can be a good starting point for you to understand the fundamentals of the Taichi programming language.fractal.pyimport taichi as tiimport taichi.math as tmti.init(arch=ti.gpu)n = 320pixels = ti.field(dtype=float, shape=(n * 2, n))@ti.funcdef complex_sqr(z):  # complex square of a 2D vector    return tm.vec2(z[0] * z[0] - z[1] * z[1], 2 * z[0] * z[1])@ti.kerneldef paint(t: float):    for i, j in pixels:  # Parallelized over all pixels        c = tm.vec2(-0.8, tm.cos(t) * 0.2)        z = tm.vec2(i / n - 1, j / n - 0.5) * 2        iterations = 0        while z.norm() < 20 and iterations < 50:            z = complex_sqr(z) + c            iterations += 1        pixels[i, j] = 1 - iterations * 0.02gui = ti.GUI("Julia Set", res=(n * 2, n))i = 0while gui.running:    paint(i * 0.03)    gui.set_image(pixels)    gui.show()    i += 1CopySave the code above to your local machine and run this program, you get the following animation:noteIf you are not using an IDE for running code, you can simply run the code from your terminal by:Go to the directory that contains your .py fileType in python3 filename.py (replace filename with your script's name. Be sure to include the .py extension)Let's dive into this simple Taichi program.Import Taichiimport taichi as tiimport taichi.math as tmCopyThe first two lines import Taichi and its math module. The math module contains built-in vectors and matrices of small dimensions, such as vec2 for 2D real vectors and mat3 for 3×3 real matrices. See the Math Module for more information.Initialize Taichiti.init(arch=ti.gpu)CopyThe argument arch in ti.init() specifies the backend that will execute the compiled code. This backend can be either ti.cpu or ti.gpu. If the ti.gpu option is specified, Taichi will attempt to use the GPU backends in the following order: ti.cuda, ti.vulkan, and ti.opengl/ti.Metal. If no GPU architecture is available, the CPU will be used as the backend.Additionally, you can specify the desired GPU backend directly by setting arch=ti.cuda, for example. Taichi will raise an error if the specified architecture is not available. For further information, refer to the Global Settings section in the Taichi documentation.noteti.init(**kwargs)- Initializes Taichi environment and allows you to customize your Taichi runtime depending on the optional arguments passed into it.Define a Taichi fieldn = 320pixels = ti.field(dtype=float, shape=(n * 2, n))CopyThe function ti.field(dtype, shape) defines a Taichi field whose shape is of shape and whose elements are of type dtype.Field is a fundamental and frequently utilized data structure in Taichi. It can be considered equivalent to NumPy's ndarray or PyTorch's tensor, but with added flexibility. For instance, a Taichi field can be spatially sparse and can be easily changed between different data layouts. Further advanced features of fields will be covered in other scenario-based tutorials. For now, it is sufficient to understand that the field pixels is a dense two-dimensional array.Kernels and functions@ti.funcdef complex_sqr(z):  # complex square of a 2D vector    return tm.vec2(z[0] * z[0] - z[1] * z[1], 2 * z[0] * z[1])@ti.kerneldef paint(t: float):    for i, j in pixels:  # Parallelized over all pixels        c = tm.vec2(-0.8, tm.cos(t) * 0.2)        z = tm.vec2(i / n - 1, j / n - 0.5) * 2        iterations = 0        while z.norm() < 20 and iterations < 50:            z = complex_sqr(z) + c            iterations += 1        pixels[i, j] = 1 - iterations * 0.02CopyThe code above defines two functions, one decorated with @ti.func and the other with @ti.kernel. They are called Taichi function and kernel respectively. Taichi functions and kernels are not executed by Python's interpreter but taken over by Taichi's JIT compiler and deployed to your parallel multi-core CPU or GPU, which is determined by the arch argument in the ti.init() call.The main differences between Taichi functions and kernels are as follows:Kernels serve as the entry points for Taichi to take over the execution. They can be called anywhere in the program, whereas Taichi functions can only be invoked within kernels or other Taichi functions. For instance, in the provided code example, the Taichi function complex_sqr is called within the kernel paint.It is important to note that the arguments and return values of kernels must be type hinted, while Taichi functions do not require type hinting. In the example, the argument t in the kernel paint is type hinted, while the argument z in the Taichi function complex_sqr is not.Taichi supports the use of nested functions, but nested kernels are not supported. Additionally, Taichi does not support recursive calls within Taichi functions.tipNote that, for users familiar with CUDA programming, the ti.func in Taichi is equivalent to the __device__ in CUDA, while the ti.kernel in Taichi corresponds to the __global__ in CUDA.For those familiar with the world of OpenGL, ti.func can be compared to a typical function in GLSL, while ti.kernel can be compared to a compute shader.Parallel for loops@ti.kerneldef paint(t: float):    for i, j in pixels:  # Parallelized over all pixelsCopyThe key to achieving high performance in Taichi lies in efficient iteration. By utilizing parallelized looping, data can be processed more effectively.The code snippet above showcases a for loop at the outermost scope within a Taichi kernel, which is automatically parallelized. The for loop operates on the i and j indices simultaneously, allowing for concurrent execution of iterations.Taichi provides a convenient syntax for parallelizing tasks. Any for loop at the outermost scope within a kernel is automatically parallelized, eliminating the need for manual thread allocation, recycling, and memory management.The field pixels is treated as an iterator, with i and j being integer indices ranging from 0 to 2*n-1 and 0 to n-1, respectively. The (i, j) pairs loop over the sets (0, 0), (0, 1), ..., (0, n-1), (1, 0), (1, 1), ..., (2*n-1, n-1) in parallel.It is important to keep in mind that for loops nested within other constructs, such as if/else statements or other loops, are not automatically parallelized and are processed sequentially.@ti.kerneldef fill():    total = 0    for i in range(10): # Parallelized        for j in range(5): # Serialized in each parallel thread            total += i * j    if total > 10:        for k in range(5):  # Not parallelized because it is not at the outermost scopeCopyYou may also serialize a for loop at the outermost scope using ti.loop_config(serialize=True). Please refer to Serialize a specified parallel for loop for additional information.WARNINGThe break statement is not supported in parallelized loops:@ti.kerneldef foo():    for i in x:        ...        break # Error!@ti.kerneldef foo():    for i in x:        for j in range(10):            ...            break # OK!CopyDisplay the resultTo render the result on screen, Taichi provides a built-in GUI System. Use the gui.set_image() method to set the content of the window and gui.show() method to show the updated image.gui = ti.GUI("Julia Set", res=(n * 2, n))# Sets the window title and the resolutioni = 0while gui.running:    paint(i * 0.03)    gui.set_image(pixels)    gui.show()    i += 1CopyTaichi's GUI system uses the standard Cartesian coordinate system to define pixel coordinates. The origin of the coordinate system is located at the lower left corner of the screen. The (0, 0) element in pixels will be mapped to the lower left corner of the window, and the (639, 319) element will be mapped to the upper right corner of the window, as shown in the following image:Key takeawaysCongratulations! By following the brief example above, you have learned the most important features of Taichi:Taichi compiles and executes Taichi functions and kernels on the designated backend.For loops located at the outermost scope in a Taichi kernel are automatically parallelized.Taichi offers a flexible data container, known as field, and you can utilize indices to iterate over a field.Taichi examplesThe Julia fractal is one of the featured demos included in Taichi. To view additional selected demos available in the Taichi Gallery:ti galleryCopyA new window will open and appear on the screen:To access the complete list of Taichi examples, run ti example. Here are some additional useful command lines:ti example -p fractal or ti example -P fractal prints the source code of the fractal example.ti example -s fractal saves the example to your current working directory.Supported systems and backendsThe table below provides an overview of the operating systems supported by Taichi and the corresponding backends that are compatible with these platforms:platformCPUCUDAOpenGLMetalVulkanWindows✔️✔️✔️N/A✔️Linux✔️✔️✔️N/A✔️macOS✔️N/AN/A✔️✔️✔️: supported;N/A: not availableYou are now prepared to begin writing your own Taichi programs. The following documents provide more information about how to utilize Taichi in some of its typical application scenarios:Accelerate Python with TaichiConduct Physical SimulationAccelerate PyTorch with Taichi.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?PrerequisitesInstallationHello, world!Import TaichiInitialize TaichiDefine a Taichi fieldKernels and functionsParallel for loopsDisplay the resultKey takeawaysTaichi examplesSupported systems and backendsCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Tutorial: Run Taichi programs in C++ applications | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTutorial: Run Taichi programs in C++ applicationsTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Deployment>>Tutorial: Run Taichi programs in C++ applicationsVersion: v1.6.0On this pageTutorial: Run Taichi programs in C++ applicationsTaichi makes it easy to write high-performance programs with efficient parallelism, but in many applications we cannot simply deploy the Python scripts. Taichi offers a runtime library (TiRT) with a C interface as well as its C++ wrapper, so your Taichi kernels can be launched in any native application. In this tutorial, we'll walk through the steps to deploy a Taichi program in a C++ application.OverviewIn Python, when you call a function decorated with @ti.kernel, Taichi immediately compiles the kernel and sends it to the device for execution. This is called just-in-time (JIT) compilation. However, in general, we don't want to compile the kernels on a mobile phone, or to expose the source code to the users. For this Taichi introduced ahead-of-time (AOT) compilation so that you can compile kernels on a development machine, and launch them on user devices via TiRT.In summary, running a Taichi program in C++ applications involves two steps:Compile Taichi kernels from Python and save the artifacts.Load the AOT modules with TiRT and launch them in your applications.Although this tutorial only demonstrates integrating Taichi in a C++ application, the C interface allows you to integrate TiRT with many other programming languages including C/C++, Swift, Rust, C# (via P/Invoke) and Java (via JNI).Quick-StartIn this section, we will write a Taichi kernel for generating images for Julia fractal and deploy it in a C++ application. The following shows the project layout. Next, we will walk through the steps to see what they do..├── cmake│   └── FindTaichi.cmake    // finds the Taichi runtime library├── CMakeLists.txt          // builds the project├── app.py                  // defines and compiles the Taichi kernel├── app.cpp                 // deploys the compiled artifact to the application└── module.tcm              // the compiled Taichi kernel artifactCopyBefore we start, it is recommended to install Taichi through taichi-nightly Python wheels using the following command. Be aware that there's no strong version compatibility enforced yet, so it's highly recommended to use the Taichi built from exactly the same commit.pip install -i https://pypi.taichi.graphics/simple/ taichi-nightlyCopy1. Compile Taichi kernel in Python scriptWe firstly write a Python script named app.py, which compiles the Taichi kernel as an artifact. Save the following code to your local machine and run the program, you will obtain an archived module.tcm in the same directory as app.py.import taichi as titi.init(arch=ti.vulkan)if ti.lang.impl.current_cfg().arch != ti.vulkan:    raise RuntimeError("Vulkan is not available.")@ti.kerneldef paint(n: ti.u32, t: ti.f32, pixels: ti.types.ndarray(dtype=ti.f32, ndim=2)):    for i, j in pixels:  # Parallelized over all pixels        c = ti.Vector([-0.8, ti.cos(t) * 0.2])        z = ti.Vector([i / n - 1, j / n - 0.5]) * 2        iterations = 0        while z.norm() < 20 and iterations < 50:            z = ti.Vector([z[0]**2 - z[1]**2, z[1] * z[0] * 2]) + c            iterations += 1        pixels[i, j] = 1 - iterations * 0.02mod = ti.aot.Module(ti.vulkan)mod.add_kernel(paint)mod.archive("module.tcm")CopyLet's dive into the code example to see what happened.We initialize Taichi specifing the backend as ti.vulkan at the beginning. Considering that Taichi will fall back to CPU device if the target architecture is unavailable, we check if the current backend meets our requirement.ti.init(arch=ti.vulkan)if ti.lang.impl.current_cfg().arch != ti.vulkan:    raise RuntimeError("Vulkan is not available.")CopyThen, we define our Taichi kernel for computing each pixel in our program. A Taichi kernel describes two aspects of a computer program: the computation itself, and the data it operates on. Because we don't know what kind of data will be fed into the kernel before execution, we have to clearly annotate the argument types for the AOT compiler.Taichi AOT module supports the following argument types: ti.i32, ti.f32, ti.Ndarray. Despite integers and floating-point numbers, we have a commonly-used data container called Ndarray. It's similar to an ndarray in NumPy, or a Tensor in PyTorch. It can be multidimensional and is laid out continuously in memory. If you have experienced the multidimensional arrays in C++, You can treat it as a nested array type like float[6][14].Our Taichi kernel accepts an integer n, a float-pointing number t and a 2-dimensional Ndarray pixels as arguments. Each element of pixels is a floating-point number ranges from 0.0 to 1.0.@ti.kerneldef paint(n: ti.i32, t: ti.f32, pixels: ti.types.ndarray(dtype=ti.f32, ndim=2)):    for i, j in pixels:  # Parallelized over all pixels        c = ti.Vector([-0.8, ti.cos(t) * 0.2])        z = ti.Vector([i / n - 1, j / n - 0.5]) * 2        iterations = 0        while z.norm() < 20 and iterations < 50:            z = ti.Vector([z[0]**2 - z[1]**2, z[1] * z[0] * 2]) + c            iterations += 1        pixels[i, j] = 1 - iterations * 0.02CopyFinally, we compile the kernel into an artifact. The following piece of code initializes the AOT module and add the kernel to the module. The compiled artifact is saved as module.tcm in the working directory.mod = ti.aot.Module(ti.vulkan)mod.add_kernel(paint)mod.archive("module.tcm")Copy2. Work with Taichi C-API in C++ programWe are now done with Python and well prepared to build our application. The compiled artifacts saved as module.tcm and the Taichi Runtime Libirary (TiRT) are all we need. TiRT provides a fundamental C interface to help achieve optimal portability, however we also provide a header-only C++ wrapper to save you from writing verbose C code. For simplicity purpose, we'll stick with the C++ wrapper in this tutorial.Firstly, we need to include the C++ wrapper header of Taichi C-API.#include <taichi/cpp/taichi.hpp>CopyNext, create a Taichi runtime with target architecture. We will further load the compiled artifacts from module.tcm and load our paint kernel from the module.ti::Runtime runtime(TI_ARCH_VULKAN);ti::AotModule aot_module = runtime.load_aot_module("module.tcm");ti::Kernel kernel_paint = aot_module.get_kernel("paint");CopyThe paint kernel accepts three arguments, and thus we need to declare corresponding variables in C++ program. We allocate memory through TiRT's allocate_ndarray interface for the pixels, the width and the height are set to 2 * n and n respectively, and the element shape is set to 1.int n = 320;float t = 0.0f;ti::NdArray<float> pixels = runtime.allocate_ndarray<float>({(uint32_t)(2 * n), (uint32_t)n}, {1}, true);CopyThen, we specify the arguments for the kernel, where the index for kernel_paint indicates the position in the kernel's argument list. Launch the kernel, and wait for the Taichi kernel process to finish.kernel_paint[0] = n;kernel_paint[1] = t;kernel_paint[2] = pixels;kernel_paint.launch();runtime.wait();CopyFinally, the pixels Ndarray holds the kernel output. Before we read the output pixel data, we must map a device memory to a user-addressable space. The image data is saved in a plain text ppm format with a utility function save_ppm. For the ppm format, please refer to Wikipedia.auto pixels_data = (const float*)pixels.map();save_ppm(pixels_data, 2 * n, n, "result.ppm");pixels.unmap();CopyThe complete C++ source code is shown below, which is saved as app.cpp in the same directory as app.py.#include <fstream>#include <taichi/cpp/taichi.hpp>void save_ppm(const float* pixels, uint32_t w, uint32_t h, const char* path) {  std::fstream f(path, std::ios::out | std::ios::trunc);  f << "P3\n" << w << ' ' << h << "\n255\n";  for (int j = h - 1; j >= 0; --j) {    for (int i = 0; i < w; ++i) {      f << static_cast<uint32_t>(255.999 * pixels[i * h + j]) << ' '        << static_cast<uint32_t>(255.999 * pixels[i * h + j]) << ' '        << static_cast<uint32_t>(255.999 * pixels[i * h + j]) << '\n';    }  }  f.flush();  f.close();}int main(int argc, const char** argv) {  ti::Runtime runtime(TI_ARCH_VULKAN);  ti::AotModule aot_module = runtime.load_aot_module("module.tcm");  ti::Kernel kernel_paint = aot_module.get_kernel("paint");  int n = 320;  float t = 0.0f;  ti::NdArray<float> pixels = runtime.allocate_ndarray<float>({(uint32_t)(2 * n), (uint32_t)n}, {1}, true);  kernel_paint[0] = n;  kernel_paint[1] = t;  kernel_paint[2] = pixels;  kernel_paint.launch();  runtime.wait();  auto pixels_data = (const float*)pixels.map();  save_ppm(pixels_data, 2 * n, n, "result.ppm");  pixels.unmap();  return 0;}Copy3. Build project with CMakeCMake is utilized to build our project, and we introduce the utility CMake module cmake/FindTaichi.cmake. It firstly find Taichi installation directory according to the environment variable TAICHI_C_API_INSTALL_DIR, without which CMake will find the Taichi library in Python wheel. Then, it will define the Taichi::Runtime target which is linked to our project.The utility module is further included in the CMakeLists.txt which looks like as below.cmake_minimum_required(VERSION 3.17)set(TAICHI_AOT_APP_NAME TaichiAot)project(${TAICHI_AOT_APP_NAME} LANGUAGES C CXX)set(CMAKE_CXX_STANDARD 17)set(CMAKE_CXX_STANDARD_REQUIRED ON)# Declare executable target.add_executable(${TAICHI_AOT_APP_NAME} app.cpp)target_include_directories(${TAICHI_AOT_APP_NAME} PUBLIC ${TAICHI_C_API_INSTALL_DIR}/include)# Find and link Taichi runtime library.set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)find_package(Taichi REQUIRED)target_link_libraries(${TAICHI_AOT_APP_NAME} Taichi::Runtime)CopyBuild the project with the commands:cmake -B buildcmake --build buildCopyRun the executable TaichiAOT demo:./build/TaichiAOTCopyAn image of Julia fractal shown below is saved as result.ppm in the project directory.FAQMap your Taichi data types from Python to C++PythonC++scalarC++ scalar typeti.vector / ti.matrixstd::vectorti.ndarrayti::Ndarrayti.Textureti::Textureti.fieldWIPDoes Taichi support device import/export?Yes! We understand that in real applications it's pretty common to hook Taichi in your existing Vulkan pipeline. As a result, you can choose to import an external device for Taichi to use, or export a device that Taichi creates to share with the external applicationWhich backends & hardware are supported?Currently ti.vulkan, ti.opengl, ti.x86 and ti.cuda are supported. ti.metal is not yet supported.How can I debug a C++ application with embedded Taichi?Check ti_get_last_error() whenever you call a Taichi C API.Enable backward-cpp in your application to locate the source of crashes. E.g. https://github.com/taichi-dev/taichi-aot-demo/pull/69Get values of ndarrays back on host using ndarray.read(), e.g. https://github.com/taichi-dev/taichi-aot-demo/pull/57/files#diff-d94bf1ff63835d9cf87e700ca3c37d1e9a3c09e5994944db2adcddf132a71d0cR32Enable printing in shaders, e.g. https://github.com/taichi-dev/taichi-aot-demo/pull/55Does Taichi support generating shaders for different deployment targets?Yes, you can specify the target device capabilities in ti.aot.Module(arch=, caps=[]). Future support for compiling to a different architecture from ti.init() is planned.Are Taichi compiled artifacts versioned?There is no official versioning yet (pre-release). For now, use Taichi and C++ runtime built from the same commit for compatibility.Can I hook Taichi into a render pipeline?Yes! If you already have a rendering pipeline, you can interop with Taichi via https://docs.taichi-lang.org/docs/taichi_vulkan.If you don't have one already, please check out our demos at https://github.com/taichi-dev/taichi-aot-demoI just want to use raw shaders generated by Taichi. Where can I find them?Yes, you can find the raw shaders generated by Taichi in the target folder of the aot save. However, it's important to note that launching Taichi shaders requires a special setup that relates to the implementation details in Taichi and may change without notice. If you have strict size limitations for your application and the provided runtime is too large to fit, you may consider writing a minimal Taichi runtime in C that consumes these raw shaders.Can I build the libtaichi_c_api.so from source?Usually, for simplicity and stability, we recommend using the official nightly taichi wheels and the c_api shipped inside the wheel. But if you want a runtime library with special build configuration:TAICHI_CMAKE_ARGS="-DTI_WITH_VULKAN:BOOL=ON -DTI_WITH_C_API:BOOL=ON" python setup.py develop# Other commonly used CMake options- TI_WITH_OPENGL- TI_WITH_CPU- TI_WITH_CUDACopyYou can find the built libtaichi_c_api.so and its headers in the _skbuild/ folder.Taichi/C API Reference Manualhttps://docs.taichi-lang.org/docs/taichi_core#api-referenceWhen do I need to recompile my artifacts?It is recommended to recompile the Taichi artifacts when changes are made to the following:Updates to the kernels and their corresponding launch logic in PythonThe need to use a newer version of either the Python Taichi or runtime libraryThe target device has a different set of capabilitiesUpdating some Python constants that are encoded as constants in the Taichi compiled artifactsPlease note that due to the nature of Ndarray handling in Taichi, the generated shaders can be used for Ndarrays with different shapes as long as their ranks match. This is a convenient feature if you need to use a single set of shaders for various scenarios, such as different screen sizes on Android phones.How can I set values for ndarrays in C++?In the C++ wrapper we provide these convenient read/write() methods on NdArray class. https://github.com/taichi-dev/taichi/blob/master/c_api/include/taichi/cpp/taichi.hpp#L192-L215In C API you can allocate your memory as host accessible and then use map/unmap. https://docs.taichi-lang.org/docs/taichi_coreEdit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?OverviewQuick-Start1. Compile Taichi kernel in Python script2. Work with Taichi C-API in C++ program3. Build project with CMakeFAQMap your Taichi data types from Python to C++Does Taichi support device import/export?Which backends & hardware are supported?How can I debug a C++ application with embedded Taichi?Does Taichi support generating shaders for different deployment targets?Are Taichi compiled artifacts versioned?Can I hook Taichi into a render pipeline?I just want to use raw shaders generated by Taichi. Where can I find them?Can I build the libtaichi_c_api.so from source?Taichi/C API Reference ManualWhen do I need to recompile my artifacts?How can I set values for ndarrays in C++?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Why a New Programming Language | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiWhy a New Programming LanguageGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Why Taichi>>Why a New Programming LanguageVersion: v1.6.0Why a New Programming LanguageImagine you'd like to write a new particle-based fluid algorithm. You started simple, didn't spend much time before finding a reference C++/CUDA work online (or derived the work from your labmate, unfortunately). cmake .. && make, you typed. Oops, cmake threw out an error due to a random incompatible third party library. Installed and rebuilt, now it passed. Then you ran it, which immediately segfaulted (without any stacktrace, of course). Then you started gazing at the code, placed the necessary asset files at the right place, fixed a few dangling pointers and reran. It... actually worked, until you plugged in your revised algorithm. Now another big fight with the GPU or CPU code. More often than not, you get lost in the language details.If all these sound too familiar to you, congratulations, you are probably looking at the right solution.Born from the MIT CSAIL lab, Taichi was designed to facilitate computer graphics researchers' everyday life, by helping them quickly implement visual computing and physics simulation algorithms that are executable on GPU. The path Taichi took was an innovative one: Taichi is embedded in Python and uses modern just-in-time (JIT) frameworks (for example LLVM, SPIR-V) to offload the Python source code to native GPU or CPU instructions, offering the performance at both development time and runtime.To be fair, a domain-specific language (DSL) with a Python frontend is not something new. In the past few years, frameworks like Halide, PyTorch, and TVM have matured into the de facto standards in areas such as image processing and deep learning (DL). What distinguishes Taichi the most from these frameworks is its imperative programming paradigm. As a DSL, Taichi is not so specialized in a particular computing pattern. This provides better flexibility. While one may argue that flexibility usually comes at the cost of not being fully optimized, we often find this not the case for a few reasons:Taichi's workload typically does not exhibit an exploitable pattern (e.g., element-wise operations), meaning that the arithmetic intensity is bounded anyway. By simply switching to the GPU backend, one can already enjoy a nice performance gain.Unlike the traditional DL frameworks, where operators are simple math expressions and have to be fused at the graph level to achieve higher arithmetic intensity, Taichi's imperative paradigm makes it quite easy to write a large amount of computation in a single kernel. We call it mega-kernel.Taichi heavily optimizes the source code using various compiler technologies: common subexpression elimination, dead code elimination, control flow graph analysis, etc. These optimizations are backend neutral, because Taichi hosts its own intermediate representation (IR) layer.JIT compilation provides additional optimization opportunities.That said, Taichi goes beyond a Python JIT transpiler. One of the initial design goals is to decouple the computation from the data structures. The mechanism that Taichi provides is a set of generic data containers, called SNode (/ˈsnoʊd/). SNodes can be used to compose hierarchical, dense or sparse, multi-dimensional fields conveniently. Switching between array-of-structures and structure-of-arrays layouts is usually a matter of ≤10 lines of code. This has sparked many use cases in numerical simulation. If you are interested to learn them, please check out Fields (advanced), Spatially Sparse Data Structures, or the original Taichi paper.The concept of decoupling is further extended to the type system. With GPU memory capacity and bandwidth becoming the major bottlenecks nowadays, it is vital to be able to pack more data per memory unit. Since 2021, Taichi has introduced customizable quantized types, allowing for the definition of fixed point or floating point numbers with arbitrary bits (still needs to be under 64). This has allowed an MPM simulation of over 400 million particles on a single GPU device. Learn more details in the QuanTaichi paper.Taichi is intuitive. If you know Python, you know Taichi. If you write Taichi, you awaken your GPU (or CPU as a fallback). Ever since its debut, this simple idea has gained so much popularity, that many were attracted to contribute new backends, including Vulkan, OpenGL and DirectX (working in progress). Without our strong and dedicated community, Taichi would never have been where it is now.Going forward, we see many new opportunities lying ahead, and would like to share some of our vision with you.Academia90% of the research code will be trashed due to the nature of research where assumptions keep being broken and ideas keep being iterated. Swiftly coding without thinking too much about performance may lead to incorrect conclusions, while pre-matured code optimization can be a waste of time and often produces a tangled mess. The high performance and productivity are, therefore, extremely helpful for research projects.Taichi will keep embracing the academia. The key features we have (or plan to have) for high-performance computing research projects include small-scale linear algebra (inside kernels), large-scale sparse systems, and efficient neighbor accessing for both structured and unstructured data.Taichi also provides an automatic differentiation module via source code transformation (at IR level), making it a sweet differentiable simulation tool for machine learning projects.Apps & game engine integrationOne huge advantange of Taichi lies in its portability, thanks to the support for a wide variety of backends. During the development process, we have also recognized the increasing demands from our industry users for multi-platform packaging and deployment. Below shows an experimental demo of integrating Taichi with Unity. By exporting Taichi kernels as SPIR-V shaders, we can easily import them into a Unity project.General-purpose computingWhile originally designed for physics simulation, Taichi has found its application in many other areas that can be boosted by GPU general-purpose computing.taichimd: Interactive, GPU-accelerated Molecular (& Macroscopic) Dynamics using the Taichi programming languageTaichiSLAM: a 3D Dense mapping backend library of SLAM based Taichi-Lang, designed for the aerial swarm.Stannum: Fusing Taichi into PyTorch.Maybe a new frontend?The benefit of adopting the compiler approach is that you can decouple the frontend from the backend. Taichi is currently embedded in Python, but who says it needs to stay that way? Stay tuned :-)Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Hello, World! | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedHello, World!Accelerate Python with TaichiConduct Physical SimulationAccelerate PyTorch with TaichiKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Get Started>>Hello, World!Version: v1.6.0On this pageHello, World!Taichi is a domain-specific language designed for high-performance, parallel computing, and is embedded in Python.When writing compute-intensive tasks, users can leverage Taichi's high performance computation by following a set of extra rules, and making use of the two decorators @ti.func and @ti.kernel. These decorators instruct Taichi to take over the computation tasks and compile the decorated functions to machine code using its just-in-time (JIT) compiler. As a result, calls to these functions are executed on multi-core CPUs or GPUs and can achieve acceleration by 50x~100x compared to native Python code.Additionally, Taichi also has an ahead-of-time (AOT) system for exporting code to binary/shader files that can be run without the Python environment. See Tutorial: Run Taichi programs in C++ application for more information.PrerequisitesPython: 3.7/3.8/3.9/3.10 (64-bit)OS: Windows, OS X, and Linux (64-bit)InstallationTaichi is available as a PyPI package:pip install taichiCopyHello, world!A basic fractal example, the Julia fractal, can be a good starting point for you to understand the fundamentals of the Taichi programming language.fractal.pyimport taichi as tiimport taichi.math as tmti.init(arch=ti.gpu)n = 320pixels = ti.field(dtype=float, shape=(n * 2, n))@ti.funcdef complex_sqr(z):  # complex square of a 2D vector    return tm.vec2(z[0] * z[0] - z[1] * z[1], 2 * z[0] * z[1])@ti.kerneldef paint(t: float):    for i, j in pixels:  # Parallelized over all pixels        c = tm.vec2(-0.8, tm.cos(t) * 0.2)        z = tm.vec2(i / n - 1, j / n - 0.5) * 2        iterations = 0        while z.norm() < 20 and iterations < 50:            z = complex_sqr(z) + c            iterations += 1        pixels[i, j] = 1 - iterations * 0.02gui = ti.GUI("Julia Set", res=(n * 2, n))i = 0while gui.running:    paint(i * 0.03)    gui.set_image(pixels)    gui.show()    i += 1CopySave the code above to your local machine and run this program, you get the following animation:noteIf you are not using an IDE for running code, you can simply run the code from your terminal by:Go to the directory that contains your .py fileType in python3 filename.py (replace filename with your script's name. Be sure to include the .py extension)Let's dive into this simple Taichi program.Import Taichiimport taichi as tiimport taichi.math as tmCopyThe first two lines import Taichi and its math module. The math module contains built-in vectors and matrices of small dimensions, such as vec2 for 2D real vectors and mat3 for 3×3 real matrices. See the Math Module for more information.Initialize Taichiti.init(arch=ti.gpu)CopyThe argument arch in ti.init() specifies the backend that will execute the compiled code. This backend can be either ti.cpu or ti.gpu. If the ti.gpu option is specified, Taichi will attempt to use the GPU backends in the following order: ti.cuda, ti.vulkan, and ti.opengl/ti.Metal. If no GPU architecture is available, the CPU will be used as the backend.Additionally, you can specify the desired GPU backend directly by setting arch=ti.cuda, for example. Taichi will raise an error if the specified architecture is not available. For further information, refer to the Global Settings section in the Taichi documentation.noteti.init(**kwargs)- Initializes Taichi environment and allows you to customize your Taichi runtime depending on the optional arguments passed into it.Define a Taichi fieldn = 320pixels = ti.field(dtype=float, shape=(n * 2, n))CopyThe function ti.field(dtype, shape) defines a Taichi field whose shape is of shape and whose elements are of type dtype.Field is a fundamental and frequently utilized data structure in Taichi. It can be considered equivalent to NumPy's ndarray or PyTorch's tensor, but with added flexibility. For instance, a Taichi field can be spatially sparse and can be easily changed between different data layouts. Further advanced features of fields will be covered in other scenario-based tutorials. For now, it is sufficient to understand that the field pixels is a dense two-dimensional array.Kernels and functions@ti.funcdef complex_sqr(z):  # complex square of a 2D vector    return tm.vec2(z[0] * z[0] - z[1] * z[1], 2 * z[0] * z[1])@ti.kerneldef paint(t: float):    for i, j in pixels:  # Parallelized over all pixels        c = tm.vec2(-0.8, tm.cos(t) * 0.2)        z = tm.vec2(i / n - 1, j / n - 0.5) * 2        iterations = 0        while z.norm() < 20 and iterations < 50:            z = complex_sqr(z) + c            iterations += 1        pixels[i, j] = 1 - iterations * 0.02CopyThe code above defines two functions, one decorated with @ti.func and the other with @ti.kernel. They are called Taichi function and kernel respectively. Taichi functions and kernels are not executed by Python's interpreter but taken over by Taichi's JIT compiler and deployed to your parallel multi-core CPU or GPU, which is determined by the arch argument in the ti.init() call.The main differences between Taichi functions and kernels are as follows:Kernels serve as the entry points for Taichi to take over the execution. They can be called anywhere in the program, whereas Taichi functions can only be invoked within kernels or other Taichi functions. For instance, in the provided code example, the Taichi function complex_sqr is called within the kernel paint.It is important to note that the arguments and return values of kernels must be type hinted, while Taichi functions do not require type hinting. In the example, the argument t in the kernel paint is type hinted, while the argument z in the Taichi function complex_sqr is not.Taichi supports the use of nested functions, but nested kernels are not supported. Additionally, Taichi does not support recursive calls within Taichi functions.tipNote that, for users familiar with CUDA programming, the ti.func in Taichi is equivalent to the __device__ in CUDA, while the ti.kernel in Taichi corresponds to the __global__ in CUDA.For those familiar with the world of OpenGL, ti.func can be compared to a typical function in GLSL, while ti.kernel can be compared to a compute shader.Parallel for loops@ti.kerneldef paint(t: float):    for i, j in pixels:  # Parallelized over all pixelsCopyThe key to achieving high performance in Taichi lies in efficient iteration. By utilizing parallelized looping, data can be processed more effectively.The code snippet above showcases a for loop at the outermost scope within a Taichi kernel, which is automatically parallelized. The for loop operates on the i and j indices simultaneously, allowing for concurrent execution of iterations.Taichi provides a convenient syntax for parallelizing tasks. Any for loop at the outermost scope within a kernel is automatically parallelized, eliminating the need for manual thread allocation, recycling, and memory management.The field pixels is treated as an iterator, with i and j being integer indices ranging from 0 to 2*n-1 and 0 to n-1, respectively. The (i, j) pairs loop over the sets (0, 0), (0, 1), ..., (0, n-1), (1, 0), (1, 1), ..., (2*n-1, n-1) in parallel.It is important to keep in mind that for loops nested within other constructs, such as if/else statements or other loops, are not automatically parallelized and are processed sequentially.@ti.kerneldef fill():    total = 0    for i in range(10): # Parallelized        for j in range(5): # Serialized in each parallel thread            total += i * j    if total > 10:        for k in range(5):  # Not parallelized because it is not at the outermost scopeCopyYou may also serialize a for loop at the outermost scope using ti.loop_config(serialize=True). Please refer to Serialize a specified parallel for loop for additional information.WARNINGThe break statement is not supported in parallelized loops:@ti.kerneldef foo():    for i in x:        ...        break # Error!@ti.kerneldef foo():    for i in x:        for j in range(10):            ...            break # OK!CopyDisplay the resultTo render the result on screen, Taichi provides a built-in GUI System. Use the gui.set_image() method to set the content of the window and gui.show() method to show the updated image.gui = ti.GUI("Julia Set", res=(n * 2, n))# Sets the window title and the resolutioni = 0while gui.running:    paint(i * 0.03)    gui.set_image(pixels)    gui.show()    i += 1CopyTaichi's GUI system uses the standard Cartesian coordinate system to define pixel coordinates. The origin of the coordinate system is located at the lower left corner of the screen. The (0, 0) element in pixels will be mapped to the lower left corner of the window, and the (639, 319) element will be mapped to the upper right corner of the window, as shown in the following image:Key takeawaysCongratulations! By following the brief example above, you have learned the most important features of Taichi:Taichi compiles and executes Taichi functions and kernels on the designated backend.For loops located at the outermost scope in a Taichi kernel are automatically parallelized.Taichi offers a flexible data container, known as field, and you can utilize indices to iterate over a field.Taichi examplesThe Julia fractal is one of the featured demos included in Taichi. To view additional selected demos available in the Taichi Gallery:ti galleryCopyA new window will open and appear on the screen:To access the complete list of Taichi examples, run ti example. Here are some additional useful command lines:ti example -p fractal or ti example -P fractal prints the source code of the fractal example.ti example -s fractal saves the example to your current working directory.Supported systems and backendsThe table below provides an overview of the operating systems supported by Taichi and the corresponding backends that are compatible with these platforms:platformCPUCUDAOpenGLMetalVulkanWindows✔️✔️✔️N/A✔️Linux✔️✔️✔️N/A✔️macOS✔️N/AN/A✔️✔️✔️: supported;N/A: not availableYou are now prepared to begin writing your own Taichi programs. The following documents provide more information about how to utilize Taichi in some of its typical application scenarios:Accelerate Python with TaichiConduct Physical SimulationAccelerate PyTorch with Taichi.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?PrerequisitesInstallationHello, world!Import TaichiInitialize TaichiDefine a Taichi fieldKernels and functionsParallel for loopsDisplay the resultKey takeawaysTaichi examplesSupported systems and backendsCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Accelerate Python with Taichi | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedHello, World!Accelerate Python with TaichiConduct Physical SimulationAccelerate PyTorch with TaichiKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Get Started>>Accelerate Python with TaichiVersion: v1.6.0On this pageAccelerate Python with TaichiTaichi is a domain-specific language embedded in Python. One of its key features is that Taichi can accelerate computation-intensive Python programs and help these programs achieve comparable performance to C/C++ or even CUDA. This makes Taichi much better positioned in the area of scientific computation.In the following sections, we provide two examples to give you a sense as to how much acceleration Taichi can bring to your Python programs.Count the primesLarge-scale or nested for loops in Python always leads to poor runtime performance. The following demo counts the primes within a specified range and involves nested for loops (see here for the complete version). Simply by importing Taichi or switching to Taichi's GPU backends, you will see a significant boost to the overall performance:"""Count the prime numbers in the range [1, n]"""# Checks if a positive integer is a prime numberdef is_prime(n: int):    result = True    # Traverses the range between 2 and sqrt(n)    # - Returns False if n can be divided by one of them;    # - otherwise, returns True    for k in range(2, int(n ** 0.5) + 1):        if n % k == 0:            result = False            break    return result# Traverses the range between 2 and n# Counts the primes according to the return of is_prime()def count_primes(n: int) -> int:    count = 0    for k in range(2, n):        if is_prime(k):           count += 1    return countprint(count_primes(1000000))CopySave the code as count_prime.py and run the following command in your terminal:time python count_primes.pyCopy The count of prime numbers along with the execution time appears on the screen. It takes 2.235s to run this program.78498real        0m2.235suser        0m2.235ssys        0m0.000sCopyNow, let's change the code a bit: import Taichi to your Python code and initialize it using the CPU backend:import taichi as titi.init(arch=ti.cpu)CopyDecorate is_prime() with @ti.func and count_primes() with @ti.kernel:Taichi's compiler compiles the Python code decorated with @ti.kernel onto different devices, such as CPU and GPU, for high-performance computation.See Kernels & Functions for a detailed explanation of Taichi's core concepts: kernels and functions.@ti.funcdef is_prime(n: int):    result = True    for k in range(2, int(n ** 0.5) + 1):        if n % k == 0:            result = False            break    return result@ti.kerneldef count_primes(n: int) -> int:    count = 0    for k in range(2, n):        if is_prime(k):            count += 1    return countCopyRerun count_primes.py：time python count_primes.pyCopy The calculation speed is six times up (2.235/0.363).78498real        0m0.363suser        0m0.546ssys        0m0.179sCopyIncrease N tenfold to 10,000,000 and rerun count_primes.py:The calculation time with Taichi is 0.8s vs. 55s with Python only. The calculation speed with Taichi is 70x up.Change Taichi's backend from CPU to GPU and give it a rerun:ti.init(arch=ti.gpu)Copy The calculation time with Taichi is 0.45s vs. 55s with Python only. The calculation speed with Taichi is taken further to 120x up.Dynamic programming: longest common subsequenceThe core philosophy behind dynamic programming is that it sacrifices some storage space for less execution time and stores intermediate results to avoid repetitive computation. In the following section, we will walk you through a complete implementation of DP, and demonstrate another area where Taichi can make a real 'acceleration'.The example below follows the philosophy of DP to work out the length of the longest common subsequence (LCS) of two given sequences. For instance, the LCS of sequences a = [0, 1, 0, 2, 4, 3, 1, 2, 1] and b = [4, 0, 1, 4, 5, 3, 1, 2],  is [0, 1, 4, 3, 1, 2], and the LCS' length is six. Let's get started:Import NumPy and Taichi to your Python program:import taichi as tiimport numpy as npCopyInitialize Taichi:ti.init(arch=ti.cpu)CopyCreate two 15,000-long NumPy arrays of random integers in the range of [0, 100] to compare:N = 15000a_numpy = np.random.randint(0, 100, N, dtype=np.int32)b_numpy = np.random.randint(0, 100, N, dtype=np.int32)CopyHere we define an N×N Taichi field f, using its [i, j]-th element to represent the length of the LCS of sequence a's first i elements and sequence b's first j elements:f = ti.field(dtype=ti.i32, shape=(N + 1, N + 1))CopyNow we turn the dynamic programming issue to the traversal of a field f, where a and b are the two sequences to compare:f[i, j] = ti.max(f[i - 1, j - 1] + (a[i - 1] == b[j - 1]),              ti.max(f[i - 1, j], f[i, j - 1]))CopyDefine a kernel function compute_lcs(), which takes in two sequences and works out the length of their LCS.@ti.kerneldef compute_lcs(a: ti.types.ndarray(), b: ti.types.ndarray()) -> ti.i32:       len_a, len_b = a.shape[0], b.shape[0]    ti.loop_config(serialize=True) # Disable auto-parallelism in Taichi    for i in range(1, len_a + 1):        for j in range(1, len_b + 1):            f[i, j] = ti.max(f[i - 1, j - 1] + (a[i - 1] == b[j - 1]),                          ti.max(f[i - 1, j], f[i, j - 1]))    return f[len_a, len_b]CopyNumPy arrays are stored as ndarray in Taichi.Ensure that you set ti.loop_config(serialize=True) to disable auto-parallelism in Taichi. The iterations here should not happen in parallelism because the computation of a loop iteration is dependent on its previous iterations.Print the result of compute_lcs(a_numpy, b_numpy).
Now you get the following program:import taichi as tiimport numpy as npti.init(arch=ti.cpu)benchmark = TrueN = 15000f = ti.field(dtype=ti.i32, shape=(N + 1, N + 1))if benchmark:    a_numpy = np.random.randint(0, 100, N, dtype=np.int32)    b_numpy = np.random.randint(0, 100, N, dtype=np.int32)else:    a_numpy = np.array([0, 1, 0, 2, 4, 3, 1, 2, 1], dtype=np.int32)    b_numpy = np.array([4, 0, 1, 4, 5, 3, 1, 2], dtype=np.int32)@ti.kerneldef compute_lcs(a: ti.types.ndarray(), b: ti.types.ndarray()) -> ti.i32:    len_a, len_b = a.shape[0], b.shape[0]    ti.loop_config(serialize=True) # Disable auto-parallelism in Taichi    for i in range(1, len_a + 1):        for j in range(1, len_b + 1):               f[i, j] = ti.max(f[i - 1, j - 1] + (a[i - 1] == b[j - 1]),                          ti.max(f[i - 1, j], f[i, j - 1]))    return f[len_a, len_b]print(compute_lcs(a_numpy, b_numpy))CopySave the above code as lcs.py and run:time python lcs.pyCopy The system prints the length of the LCS, along with the execution time.2721real        0m1.409suser        0m1.112ssys        0m0.549sCopyIn this repo, we provide our implementation of this dynamic planning algorithm in Taichi and NumPy:With Python only, it takes 476s to work out the length of the LCS of two 15,000-long random sequences.With Taichi, it only takes about 0.9s and sees an up to 500x speed up!noteThe actual execution time may vary depending on your machine, but we believe that the performance improvements you will see is comparable to ours.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Count the primesDynamic programming: longest common subsequenceCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Conduct Physical Simulation | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedHello, World!Accelerate Python with TaichiConduct Physical SimulationAccelerate PyTorch with TaichiKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Get Started>>Conduct Physical SimulationVersion: v1.6.0On this pageConduct Physical SimulationThe GIF image above nicely simulates a piece of cloth falling onto a sphere. The cloth in the image is modeled as a mass-spring system, which contains over 10,000 mass points and around 100,000 springs. To simulate a massive physical system at this scale and render it in real-time is never an easy task.With Taichi, physical simulation programs can be much more readable and intuitive, while still achieving performance comparable to that of C++ or CUDA. With Taichi, those with basic Python programming skills can write high-performance parallel programs in much fewer lines than before, focusing on the higher-level algorithms per se and leaving tasks like performance optimization to Taichi.In this document, we will walk you through the process of writing a Python program simulating and rendering a piece of cloth falling onto a sphere. Before you proceed, please take a guess of how many lines of code this program consists of.Get startedBefore using Taichi in your Python program, you need to import Taichi to your namespace and initialize Taichi:Import Taichi:import taichi as tiCopyInitialize Taichi:# Choose any of the following backend when initializing Taichi# - ti.cpu# - ti.gpu# - ti.cuda# - ti.vulkan# - ti.metal# - ti.openglti.init(arch=ti.cpu)CopyWe choose ti.cpu here despite the fact that running Taichi on a GPU backend can be much faster. This is mainly because we need to make sure that you can run our source code without any editing or additional configurations to your platform. Please note:If you choose a GPU backend, for example ti.cuda, ensure that you have installed it on your system; otherwise, Taichi will raise an error.The GGUI we use for 3D rendering only supports CUDA and Vulkan, and x86 for now. If you choose a different backend, consider switching the GGUI system we provide in the source code.ModellingThis section does the following:Generalizes and simplifies the models involved in the cloth simulation.Represents the falling cloth and the ball with the data containers provided by Taichi.Model simplificationThis section generalizes and simplifies the models involved in the cloth simulation.Cloth: a mass-spring systemIn this program, the falling cloth is modeled as a mass-spring system. More specifically, we represent the piece of cloth as an n × n grid of mass points, where adjacent points are linked by springs. The following image provided by Matthew Fisher illustrates this structure, where the red vertices are the mass points and the white edges of the grids are the springs.This mass-spring system can be represented by two arrays:An n × n array of mass points' positions,An n × n array of mass points' velocities.Further, the position and movement of this system is affected by four factors:GravityInternal forces of the springsDampingCollision with the ball in the middleOur simulation begins at time t = 0 and advances time by a small constant dt. At the end of each time step dt, the program does the following:Estimates the effects of the four factors above on the mass-spring system,Updates the position and velocity of each mass point accordingly.Representation of the ballA ball can usually be represented by its ball center and radius.Data structuresIn this program, we represent the falling cloth and the ball with the data containers provided by Taichi.Data structures for clothHaving initialized Taichi, you can declare the data structures that represent the cloth.Declare two arrays x and v for storing the mass points' positions and velocities. In Taichi, such arrays are called fields.n = 128# x is an n x n field consisting of 3D floating-point vectors# representing the mass points' positionsx = ti.Vector.field(3, dtype=float, shape=(n, n))# v is an n x n field consisting of 3D floating-point vectors# representing the mass points' velocitiesv = ti.Vector.field(3, dtype=float, shape=(n, n))CopyInitialize the defined fields x and v:# The n x n grid is normalized# The distance between two x- or z-axis adjacent points# is 1.0 / nquad_size = 1.0 / n# The @ti.kernel decorator instructs Taichi to# automatically parallelize all top-level for loops# inside initialize_mass_points()@ti.kerneldef initialize_mass_points():    # A random offset to apply to each mass point    random_offset = ti.Vector([ti.random() - 0.5, ti.random() - 0.5]) * 0.1    # Field x stores the mass points' positions    for i, j in x:        # The piece of cloth is 0.6 (y-axis) above the original point        #        # By taking away 0.5 from each mass point's [x,z] coordinate value        # you move the cloth right above the original point        x[i, j] = [            i * quad_size - 0.5 + random_offset[0], 0.6,            j * quad_size - 0.5 + random_offset[1]        ]        # The initial velocity of each mass point is set to 0        v[i, j] = [0, 0, 0]CopyData structures for ballHere, the ball center is a 1D field, whose only element is a 3-dimensional floating-point vector.ball_radius = 0.3# Use a 1D field for storing the position of the ball center# The only element in the field is a 3-dimentional floating-point vectorball_center = ti.Vector.field(3, dtype=float, shape=(1, ))# Place the ball center at the original pointball_center[0] = [0, 0, 0]CopySimulationAt the end of each time step dt, the program simulates the effects of the aforementioned four factors on the mass-spring system. In this case, a kernel function substep() is defined.# substep() works out the *accumulative* effects# of gravity, internal force, damping, and collision# on the mass-spring system@ti.kerneldef substep():CopyGravity# Gravity is a force applied in the negative direction of the y axis,# and so is set to [0, -9.8, 0]gravity = ti.Vector([0, -9.8, 0])# The @ti.kernel decorator instructs Taichi to# automatically parallelize all top-level for loops# inside substep()@ti.kerneldef substep():    # The for loop iterates over all elements of the field v    for i in ti.grouped(x):        v[i] += gravity * dtCopynotefor i in ti.grouped(x) is an important feature of Taichi. It means that this for loop automatically traverses all the elements of x as a 1D array regardless of its shape, sparing you the trouble of writing multiple levels of for loops.Either for i in ti.grouped(x) or for i in ti.grouped(v) is fine here because field x has the same shape as field v.Internal forces of the springsAs the image below shows, we make the following assumptions:A given point can be influenced by at most 12 neighboring points, and influences from other mass points are neglected.These neighboring points exert internal forces on the point through springs.noteThe internal forces here broadly refer to the internal force caused by the elastic deformation of springs and the damping caused by the relative movement of two points.The code below does the following:Traverses the mass-spring grid,Accumulates the internal forces that the neighboring points exert on each mass point,Translates the internal forces to velocities.quad_size = 1.0 / n# Elastic coefficient of springsspring_Y = 3e4# Damping coefficient caused by# the relative movement of the two mass points# The assumption here is:# A mass point can have at most 12 'influential` pointsdashpot_damping = 1e4# The cloth is modeled as a mass-spring grid. Assume that:# a mass point, whose relative index is [0, 0],# can be affected by at most 12 surrounding points## spring_offsets is such a list storing# the relative indices of these 'influential' pointsspring_offsets = []for i in range(-1, 2):    for j in range(-1, 2):        if (i, j) != (0, 0):            spring_offsets.append(ti.Vector([i, j]))@ti.kerneldef substep():    # Traverses the field x as a 1D array    #    # The `i` here refers to the *absolute* index    # of an element in the field x    #    # Note that `i` is a 2-dimentional vector here    for i in ti.grouped(x):        # Initial force exerted to a specific mass point        force = ti.Vector([0.0, 0.0, 0.0])        # Traverse the surrounding mass points        for spring_offset in ti.static(spring_offsets):            # j is the *absolute* index of an 'influential' point            # Note that j is a 2-dimensional vector here            j = i + spring_offset            # If the 'influential` point is in the n x n grid,            # then work out the internal force that it exerts            # on the current mass point            if 0 <= j[0] < n and 0 <= j[1] < n:                # The relative displacement of the two points                # The internal force is related to it                x_ij = x[i] - x[j]                # The relative movement of the two points                v_ij = v[i] - v[j]                # d is a normalized vector (its norm is 1)                d = x_ij.normalized()                current_dist = x_ij.norm()                original_dist = quad_size * float(i - j).norm()                # Internal force of the spring                force += -spring_Y * d * (current_dist / original_dist - 1)                # Continues to apply the damping force                # from the relative movement                # of the two points                force += -v_ij.dot(d) * d * dashpot_damping * quad_size        # Continues to add the velocity caused by the internal forces        # to the current velocity        v[i] += force * dtCopyDampingIn the real world, when springs oscillate, the energy stored in the springs dissipates into the surrounding environment as the oscillations die away. To capture this effect, we slightly reduce the magnitude of the velocity of each point in the grid at each time step:# Damping coefficient of springsdrag_damping = 1@ti.kerneldef substep():    # Traverse the elements in field v    for i in ti.grouped(x):        v[i] *= ti.exp(-drag_damping * dt)CopyCollision with the ballWe assume that the collision with the ball is an elastic collision: When a mass point collides with the ball, its velocity component on the normal vector of that collision point changes.Note that the position of each mass point is updated using the velocity at the end of each time step.# Damping coefficient of springsdrag_damping = 1@ti.kerneldef substep():    # Traverse the elements in field v    for i in ti.grouped(x):        offset_to_center = x[i] - ball_center[0]        if offset_to_center.norm() <= ball_radius:            # Velocity projection            normal = offset_to_center.normalized()            v[i] -= min(v[i].dot(normal), 0) * normal        # After working out the accumulative v[i],        # work out the positions of each mass point        x[i] += dt * v[i]CopyAnd that's it! This is all the code required to perform a parallel simulation of a mass-spring grid system.RenderingWe use Taichi's GPU-based GUI system (also known as GGUI) for 3D rendering. GGUI supports rendering two types of 3D objects: triangle meshes and particles. In this case, we can render the cloth as a triangle mesh and the ball as a particle.GGUI represents a triangle mesh with two Taichi fields: vertices and indices. The vertices fields is a 1-dimensional field where each element is a 3D vector representing the position of a vertex, possibly shared by multiple triangles. In our application, every point mass is a triangle vertex, so we can simply copy data from x to vertices:vertices = ti.Vector.field(3, dtype=float, shape=n * n)@ti.kerneldef update_vertices():    for i, j in ti.ndrange(n, n):        vertices[i * n + j] = x[i, j]CopyNote that update_vertices needs to be called every frame, because the vertex positions are constantly being updated by the simulation.Our cloth is represented by an n by n grid of mass points, which can also be seen as an n-1 by n-1 grid of small squares. Each of these squares will be rendered as two triangles. Thus, there are a total of (n - 1) * (n - 1) * 2 triangles. Each of these triangles will be represented as three integers in the vertices field, which records the indices of the vertices of the triangle in the vertices field. The following code snippet captures this structure:@ti.kerneldef initialize_mesh_indices():    for i, j in ti.ndrange(n - 1, n - 1):        quad_id = (i * (n - 1)) + j        # First triangle of the square        indices[quad_id * 6 + 0] = i * n + j        indices[quad_id * 6 + 1] = (i + 1) * n + j        indices[quad_id * 6 + 2] = i * n + (j + 1)        # Second triangle of the square        indices[quad_id * 6 + 3] = (i + 1) * n + j + 1        indices[quad_id * 6 + 4] = i * n + (j + 1)        indices[quad_id * 6 + 5] = (i + 1) * n + j    for i, j in ti.ndrange(n, n):        if (i // 4 + j // 4) % 2 == 0:            colors[i * n + j] = (0.22, 0.72, 0.52)        else:            colors[i * n + j] = (1, 0.334, 0.52)CopyNote that, unlike update_vertices(), initialize_mesh_indices() only needs to be called once. This is because the indices of the triangle vertices do not actually change -- it is only the positions that are changing.As for rendering the ball, the ball_center and ball_radius variable previously defined are sufficient.Source codeimport taichi as titi.init(arch=ti.vulkan)  # Alternatively, ti.init(arch=ti.cpu)n = 128quad_size = 1.0 / ndt = 4e-2 / nsubsteps = int(1 / 60 // dt)gravity = ti.Vector([0, -9.8, 0])spring_Y = 3e4dashpot_damping = 1e4drag_damping = 1ball_radius = 0.3ball_center = ti.Vector.field(3, dtype=float, shape=(1, ))ball_center[0] = [0, 0, 0]x = ti.Vector.field(3, dtype=float, shape=(n, n))v = ti.Vector.field(3, dtype=float, shape=(n, n))num_triangles = (n - 1) * (n - 1) * 2indices = ti.field(int, shape=num_triangles * 3)vertices = ti.Vector.field(3, dtype=float, shape=n * n)colors = ti.Vector.field(3, dtype=float, shape=n * n)bending_springs = False@ti.kerneldef initialize_mass_points():    random_offset = ti.Vector([ti.random() - 0.5, ti.random() - 0.5]) * 0.1    for i, j in x:        x[i, j] = [            i * quad_size - 0.5 + random_offset[0], 0.6,            j * quad_size - 0.5 + random_offset[1]        ]        v[i, j] = [0, 0, 0]@ti.kerneldef initialize_mesh_indices():    for i, j in ti.ndrange(n - 1, n - 1):        quad_id = (i * (n - 1)) + j        # 1st triangle of the square        indices[quad_id * 6 + 0] = i * n + j        indices[quad_id * 6 + 1] = (i + 1) * n + j        indices[quad_id * 6 + 2] = i * n + (j + 1)        # 2nd triangle of the square        indices[quad_id * 6 + 3] = (i + 1) * n + j + 1        indices[quad_id * 6 + 4] = i * n + (j + 1)        indices[quad_id * 6 + 5] = (i + 1) * n + j    for i, j in ti.ndrange(n, n):        if (i // 4 + j // 4) % 2 == 0:            colors[i * n + j] = (0.22, 0.72, 0.52)        else:            colors[i * n + j] = (1, 0.334, 0.52)initialize_mesh_indices()spring_offsets = []if bending_springs:    for i in range(-1, 2):        for j in range(-1, 2):            if (i, j) != (0, 0):                spring_offsets.append(ti.Vector([i, j]))else:    for i in range(-2, 3):        for j in range(-2, 3):            if (i, j) != (0, 0) and abs(i) + abs(j) <= 2:                spring_offsets.append(ti.Vector([i, j]))@ti.kerneldef substep():    for i in ti.grouped(x):        v[i] += gravity * dt    for i in ti.grouped(x):        force = ti.Vector([0.0, 0.0, 0.0])        for spring_offset in ti.static(spring_offsets):            j = i + spring_offset            if 0 <= j[0] < n and 0 <= j[1] < n:                x_ij = x[i] - x[j]                v_ij = v[i] - v[j]                d = x_ij.normalized()                current_dist = x_ij.norm()                original_dist = quad_size * float(i - j).norm()                # Spring force                force += -spring_Y * d * (current_dist / original_dist - 1)                # Dashpot damping                force += -v_ij.dot(d) * d * dashpot_damping * quad_size        v[i] += force * dt    for i in ti.grouped(x):        v[i] *= ti.exp(-drag_damping * dt)        offset_to_center = x[i] - ball_center[0]        if offset_to_center.norm() <= ball_radius:            # Velocity projection            normal = offset_to_center.normalized()            v[i] -= min(v[i].dot(normal), 0) * normal        x[i] += dt * v[i]@ti.kerneldef update_vertices():    for i, j in ti.ndrange(n, n):        vertices[i * n + j] = x[i, j]window = ti.ui.Window("Taichi Cloth Simulation on GGUI", (1024, 1024),                      vsync=True)canvas = window.get_canvas()canvas.set_background_color((1, 1, 1))scene = ti.ui.Scene()camera = ti.ui.Camera()current_t = 0.0initialize_mass_points()while window.running:    if current_t > 1.5:        # Reset        initialize_mass_points()        current_t = 0    for i in range(substeps):        substep()        current_t += dt    update_vertices()    camera.position(0.0, 0.0, 3)    camera.lookat(0.0, 0.0, 0)    scene.set_camera(camera)    scene.point_light(pos=(0, 1, 2), color=(1, 1, 1))    scene.ambient_light((0.5, 0.5, 0.5))    scene.mesh(vertices,               indices=indices,               per_vertex_color=colors,               two_sided=True)    # Draw a smaller ball to avoid visual penetration    scene.particles(ball_center, radius=ball_radius * 0.95, color=(0.5, 0.42, 0.8))    canvas.scene(scene)    window.show()CopyTotal number of lines: 145.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Get startedModellingModel simplificationData structuresSimulationGravityInternal forces of the springsDampingCollision with the ballRenderingSource codeCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Accelerate PyTorch with Taichi | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedHello, World!Accelerate Python with TaichiConduct Physical SimulationAccelerate PyTorch with TaichiKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Get Started>>Accelerate PyTorch with TaichiVersion: v1.6.0On this pageAccelerate PyTorch with TaichiTaichi and Torch serve different application scenarios but can complement each other.Taichi provides finer control over parallelization and enables more 'granular' (element-level) operations, giving its users much more flexibilities.Torch abstracts such details into Tensor-level operations like LEGO bricks, enabling its users to focus on building ML (Machine Learning) models.This document uses two examples to explain how to use Taichi kernel to implement data preprocessing operators and custom high-performance ML operators.Data preprocessingThis section uses padding as an example to show you how Taichi can complement PyTorch in data preprocessing.Padding is a commonly-used data preprocessing technique in machine learning. For example, padding can prevent convolution operations from changing the size of the input image. However, no PyTorch operators are designed specifically for padding in a specific customized pattern. Previously, you have two options to work around this:Using Python or PyTorch to iterate over matrix elements.Writing a C++/CUDA operator and connecting it to PyTorch via Python's custom operator extension.The former has very poor efficiency and could become a drain of the neural network training performance; the latter requires large amount of domain-specific knowledge about the underlying hardware architectures and it could take a long while to get started.Now, you can use Taichi to pad a brick wall of a specific customized pattern in a much more efficient way.The following sections compare PyTorch's implementation of this workflow with Taichi's implementation:Create a 'brick' and fill it with changing colors.Repeat the bricks horizontally with a fixed offset to form a staggered layout.Padding with PyTorchThe following code implements a PyTorch kernel torch_pad() for padding. To improve efficiency, the kernel turns the padding process into a series of native PyTorch matrix operations. But such matrix operations are usually unintuitive and require so many intermediate results to be stored in the GPU memory that old GPUs with less RAM cannot even afford them.def torch_pad(arr, tile, y):    # image_pixel_to_coord    arr[:, :, 0] = image_height - 1 + ph - arr[:, :, 0]    arr[:, :, 1] -= pw    arr1 = torch.flip(arr, (2, ))    # map_coord    v = torch.floor(arr1[:, :, 1] / tile_height).to(torch.int)    u = torch.floor((arr1[:, :, 0] - v * shift_y[0]) / tile_width).to(torch.int)    uu = torch.stack((u, u), axis=2)    vv = torch.stack((v, v), axis=2)    arr2 = arr1 - uu * shift_x - vv * shift_y    # coord_to_tile_pixel    arr2[:, :, 1] = tile_height - 1 - arr2[:, :, 1]    table = torch.flip(arr2, (2, ))    table = table.view(-1, 2).to(torch.float)    inds = table.mv(y)    gathered = torch.index_select(tile.view(-1), 0, inds.to(torch.long))    return gatheredwith Timer():    gathered = torch_pad(coords, tile, y)    torch.cuda.synchronize(device=device)CopyPadding with TaichiThe following code implements a Taichi kernel ti_pad() for padding. The kernel iterates over the pixels in the output image, works out each pixel's corresponding position in the input 'brick', and fills the pixel with the RGB color in that position.Taichi automatically runs the top-level for-loops in parallel, and matrix operations written in Taichi are much more readable. Moreover, as you can tell from the following code, ti_pad() takes in the PyTorch tensors directly so that it can reuse the memory allocated by PyTorch and would not cause extra overhead from the data transfer between the two frameworks.@ti.kerneldef ti_pad(image_pixels: ti.types.ndarray(), tile: ti.types.ndarray()):    for row, col in ti.ndrange(image_height, image_width):        # image_pixel_to_coord        x1, y1 = ti.math.ivec2(col - pw, image_height - 1 - row + ph)        # map_coord        v: ti.i32 = ti.floor(y1 / tile_height)        u: ti.i32 = ti.floor((x1 - v * shift_y[0]) / tile_width)        x2, y2 = ti.math.ivec2(x1 - u * shift_x[0] - v * shift_y[0],                 y1 - u * shift_x[1] - v * shift_y[1])        # coord_to_tile_pixel        x, y = ti.math.ivec2(tile_height - 1 - y2, x2)        image_pixels[row, col] = tile[x, y]with Timer():    ti_pad(image_pixels, tile）    ti.sync()CopyPerformance comparisonAs the following table shows, the PyTorch kernel takes 30.392 ms[1] to complete padding; the Taichi kernel takes 0.267 ms only. Taichi outruns PyTorch by more than 100x (30.392/0.267).torch_pad() launches 58 CUDA kernels, whilst Taichi compiles all computation into one CUDA kernel. The fewer the CUDA kernels, the less GPU launch overhead is incurred. Moreover, the Taichi kernel manages to save a lot more redundant memory operations than the PyTorch kernel. The GPU launch overhead and the redundant memory operations are the potential source for optimization and acceleration.Kernel functionAverage time (ms)CUDA kernels launched (number)torch_pad()30.39258ti_pad()0.2671GPU: RTX3090PyTorch version: v1.12.1; Taichi version: v1.1.0The actual acceleration rate may vary depending on your implementation and GPU setup.Customize ML operatorsResearchers in machine learning usually spend a lot of time designing model architectures. Because they cannot find decent support for their newly-designed or customized operators from PyTorch, they have to spend time studying CUDA for fine tuning and to improve efficiency. But writing in CUDA is hard, tuning CUDA code is even harder, and accelerating model iteration with CUDA is difficult.This repo introduces an example of customizing an ML operator in CUDA. The author developed an RWKV language model using sort of a one-dimensional depthwise convolution custom operator. The model does not involve much computation but still runs slow because PyTorch does not have native support for it.  So, the author customized the operator in CUDA using a set of optimization techniques, such as loop fusion and Shared Memory, and achieved a performance 20x better than he did with PyTorch.Referring to the CUDA code3, we customized a Taichi depthwise convolution operator4 in the RWKV model using the same optimization techniques.The function of the depth wise convolution operator:Iterates over two input Tensors w and k,Adds up the product of the respective elements in w and k into s,Saves s to an output Tensor out.The following subsections take the Baseline implementations as an example to show you how to implement a depthwise convolution operator with Python, PyTorch, CUDA, and Taichi, and how they compare to each other. With Taichi, you can accelerate your ML model development with ease and get rid of the tedious low-level parallel programming.ImplementationReadabilityPerformancePythonExcellentThe slowestPyTorchPoorSlowCUDAPoorFastTaichiExcellentComparable to that of CUDA or even betterImplement a depthwise convolution operator with PythonThe Python reference code is straightforward and easy to understand, but it runs so slow that the result can hardly make itself into the diagram below.def run_formula_very_slow(w, k, B, C, T, eps):    out = torch.empty((B, C, T), device='cpu')    for b in range(B):        for c in range(C):            for t in range(T):                s = eps                for u in range(t-T+1, t+1):                    s += w[c][0][(T-1)-(t-u)] * k[b][c][u+T-1]                out[b][c][t] = s    return outCopyImplement a depthwise convolution operator with PyTorchIt is very challenging to translate the Python reference code above to this code line. To come up with this, you have to know very well the underlying logic of these PyTorch operators.out = eps + F.conv1d(nn.ZeroPad2d((T-1, 0, 0, 0))(k), w.unsqueeze(1), groups=C)CopyImplement a depthwise convolution operator with CUDAThe CUDA reference code has much poorer readability: The outmost loop is implicitly defined by thread parallelism. The index calculation is complicated, and each element's position in the matrix is not clear at a glance. Besides, it could be rather error-prone to implement more sophisticated algorithms with CUDA.__global__ void kernel_forward(const float* w, const float* k, float* x,                               const float eps, const int B, const int C, const int T){    const int i = blockIdx.y;    const int t = threadIdx.x;    float s = eps;    const float* www = w + (i % C) * T + (T - 1) - t;    const float* kk = k + i * T;    for (int u = 0; u <= t; u++){        s += www[u] * kk[u];    }    x[i * T + t] = s;}CopyFurther, you need a proper compile environment to run your CUDA code! If you have precompiled your CUDA code into a dynamic link library, then you also need to spend time working hard on trivial matters such as environment settings and Python API encapsulation.Implement a depthwise convolution operator with TaichiThe Taichi reference code is almost identical to its Python counterpart. And a good advantage that Taichi has over CUDA is that, without worrying about low-level details like parallelization and pointer offsets, one can easily use Taichi to achieve comparable performance.@ti.kerneldef taichi_forward_v0(        out: ti.types.ndarray(ndim=3),        w: ti.types.ndarray(ndim=3),        k: ti.types.ndarray(ndim=3),        eps: ti.f32):    for b, c, t in out:        s = eps        for u in range(t-T+1, t+1):            s += w[c, 0, (T-1)-(t-u)] * k[b, c, u+T-1]        out[b, c, t] = sCopyPerformance comparisonThe following diagram shows that Taichi always shows a performance that is comparable to its CUDA counterpart or even better under certain circumstances.The RWKV compute time in the diagram is in milliseconds. The less the compute time, the better the performance is.'Baseline': The reference code is a faithful implementation of the algorithm without any modification.v1 to v3: The three different optimized implementations.RecapPyTorch is efficient in handling a large proportion of computation tasks in machine learning. Still, there are niches and needs that it falls short of addressing, such as native support for many operators and unsatisfactory runtime performance.As a high-performance programming language embedded in Python, Taichi features:Eeasy readability,Optimized memory consumption,Runtime performance comparable to that of CUDA,Good portability that encourages reproducible code sharing among the community.All these features set Taichi apart as a convenient tool for ML operator customization.The two examples provided in this document give you a glimpse of how Taichi and PyTorch can complement each other to solve real-world high-performance programming issues.Reference1 Pure PyTorch padding
2 Padding PyTorch tensor in Taichi kernel
3 RWKV-CUDA
4 RWKV-Taichi Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Data preprocessingPadding with PyTorchPadding with TaichiPerformance comparisonCustomize ML operatorsImplement a depthwise convolution operator with PythonImplement a depthwise convolution operator with PyTorchImplement a depthwise convolution operator with CUDAImplement a depthwise convolution operator with TaichiPerformance comparisonRecapReferenceCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Kernels and Functions | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsKernels and FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Kernels & Functions>>Kernels and FunctionsVersion: v1.6.0On this pageKernels and FunctionsTaichi and Python share a similar syntax, but they are not identical. To distinguish Taichi code from native Python code, we utilize two decorators, @ti.kernel and @ti.func:Functions decorated with @ti.kernel are known as Taichi kernels or simply kernels. These functions are the entry points where Taichi's runtime takes over the tasks, and they must be directly invoked by Python code. You can use native Python to prepare tasks, such as reading data from disk and pre-processing, before calling the kernel to offload computation-intensive tasks to Taichi.Functions decorated with @ti.func are known as Taichi functions. These functions are building blocks of kernels and can only be invoked by another Taichi function or a kernel. Like normal Python functions, you can divide your tasks into multiple Taichi functions to enhance readability and reuse them across different kernels.In the following example, inv_square() is decorated with @ti.func and is a Taichi function. partial_sum() is decorated with @ti.kernel and is a kernel. The former (inv_square()) is called by the latter (partial_sum()). The arguments and return value in partial_sum() are type hinted, while those in the Taichi function inv_square() are not.import taichi as titi.init(arch=ti.cpu)@ti.funcdef inv_square(x):  # A Taichi function    return 1.0 / (x * x)@ti.kerneldef partial_sum(n: int) -> float:  # A kernel    total = 0.0    for i in range(1, n + 1):        total += inv_square(n)    return totalpartial_sum(1000)CopyHere comes a significant difference between Python and Taichi - type hinting:Type hinting in Python is recommended, but not compulsory.Taichi mandates that the arguments and return value of a kernel are type hinted, unless it has neither an argument nor a return statement.WARNINGCalling a Taichi function from within the native Python code (the Python scope) results in a syntax error raised by Taichi. For example:import taichi as titi.init(arch=ti.cpu)@ti.funcdef inv_square(x):    return 1.0 / (x * x)print(inv_square(1.0))  # Syntax errorCopyYou must call Taichi functions from within the Taichi scope, a concept as opposed to the Python scope.Let's introduce two important concepts: Taichi scope and Python scope.The code inside a kernel or a Taichi function is part of the Taichi scope. Taichi's runtime compiles and executes this code in parallel on multi-core CPU or GPU devices for high-performance computation. The Taichi scope corresponds to the device side in CUDA.Code outside of the Taichi scope belongs to the Python scope. This code is written in native Python and executed by Python's virtual machine, not by Taichi's runtime. The Python scope corresponds to the host side in CUDA.It is important to distinguish between kernels and Taichi functions as they have slightly different syntax. The following sections explain their respective usages.KernelA kernel is the basic unit of execution in Taichi, and serves as the entry point for Taichi's runtime, which takes over from Python's virtual machine. Kernels are called in the same way as Python functions, and allow for switching between Taichi's runtime and Python's virtual machine.For instance, the partial_sum() kernel can be called from within a Python function:@ti.kerneldef partial_sum(n: int) -> float:    ...def main():    print(partial_sum(100))    print(partial_sum(1000))main()CopyMultiple kernels can be defined in a single Taichi program. These kernels are independent of each other, and are compiled and executed in the same order in which they are first called. The compiled kernels are cached to reduce the launch overhead for subsequent calls.WARNINGKernels in Taichi can be called either directly or from inside a native Python function. However, calling a kernel from inside another kernel or from inside a Taichi function is not allowed. In other words, kernels can only be called from the Python scope.ArgumentsA kernel can accept multiple arguments. However, it's important to note that you can't pass arbitrary Python objects to a kernel. This is because Python objects can be dynamic and may contain data that the Taichi compiler cannot recognize.The kernel can accept various argument types, including scalars, ti.types.matrix(), ti.types.vector(), ti.types.struct(), ti.types.ndarray(), and ti.template(). These argument types make it easy to pass data from the Python scope to the Taichi scope. You can find the supported types in the ti.types module. For more information on this, see the Type System.Scalars, ti.types.matrix(), ti.types.vector(), and ti.types.struct() are passed by value, which means that the kernel receives a copy of the argument. However, ti.types.ndarray() and ti.template() are passed by reference, which means that any changes made to the argument inside the kernel will affect the original value as well.Note that we won't cover ti.template() here as it is a more advanced topic and is discussed in Metaprogramming.Here is an example of passing arguments x and y to my_kernel() by value:@ti.kerneldef my_kernel(x: int, y: float):    print(x + y)my_kernel(1, 1.0)  # Prints 2.0CopyHere is another example of passing a nested struct argument with a matrix to a kernel by value, in which we created a struct type transform_type that contains two members: a rotation matrix R and a translation vector T. We then created another struct type pos_type that has transform_type as its member and passed an instance of pos_type to a kernel.transform_type = ti.types.struct(R=ti.math.mat3, T=ti.math.vec3)pos_type = ti.types.struct(x=ti.math.vec3, trans=transform_type)@ti.kerneldef kernel_with_nested_struct_arg(p: pos_type) -> ti.math.vec3:    return p.trans.R @ p.x + p.trans.Ttrans = transform_type(ti.math.mat3(1), [1, 1, 1])p = pos_type(x=[1, 1, 1], trans=trans)print(kernel_with_nested_struct_arg(p))  # [4., 4., 4.]CopyYou can use ti.types.ndarray() as a type hint to pass a ndarray from NumPy or a tensor from PyTorch to a kernel. Taichi recognizes the shape and data type of these data structures, which allows you to access their attributes in a kernel.In the example below, x is updated after my_kernel() is called since it is passed by reference:import numpy as npimport taichi as titi.init(arch=ti.cpu)x = np.array([1, 2, 3])y = np.array([4, 5, 6])@ti.kerneldef my_kernel(x: ti.types.ndarray(), y: ti.types.ndarray()):    # Taichi recognizes the shape of the array x and allows you to access it in a kernel    for i in range(x.shape[0]):        x[i] += y[i]my_kernel(x, y)print(x)  # Prints [5, 7, 9]CopyReturn valueIn Taichi, a kernel is allowed to have a maximum of one return value, which could either be a scalar, ti.types.matrix(), or ti.types.vector().
Moreover, in the LLVM-based backends (CPU and CUDA backends), a return value could also be a ti.types.struct().Here is an example of a kernel that returns a ti.Struct:s0 = ti.types.struct(a=ti.math.vec3, b=ti.i16)s1 = ti.types.struct(a=ti.f32, b=s0)@ti.kerneldef foo() -> s1:    return s1(a=1, b=s0(a=ti.math.vec3(100, 0.2, 3), b=1))print(foo())  # {'a': 1.0, 'b': {'a': [100.0, 0.2, 3.0], 'b': 1}}CopyWhen defining the return value of a kernel in Taichi, it is important to follow these rules:Use type hint to specify the return value of a kernel.Make sure that you have at most one return value in a kernel.Make sure that you have at most one return statement in a kernel.If the return value is a vector or matrix, please ensure that it contains no more than 32 elements. In case it contains more than 32 elements, the kernel will still compile, but a warning will be raised.At most one return valueIn this code snippet, the test() kernel cannot have more than one return value:vec2 = ti.math.vec2@ti.kerneldef test(x: float, y: float) -> vec2: # Return value must be type hinted    # Return x, y  # Compilation error: Only one return value is allowed    return vec2(x, y)  # FineCopyAutomatic type castIn the following code snippet, the return value is automatically cast into the hinted type:@ti.kerneldef my_kernel() -> ti.i32:  # int32    return 128.32# The return value is cast into the hinted type ti.i32print(my_kernel())  # 128CopyAt most one return statementIn this code snippet, Taichi raises an error because the kernel test_sign() has more than one return statement:@ti.kerneldef test_sign(x: float) -> float:    if x >= 0:        return 1.0    else:        return -1.0    # Error: multiple return statementsCopyAs a workaround, you can save the result in a local variable and return it at the end:@ti.kerneldef test_sign(x: float) -> float:    sign = 1.0    if x < 0:        sign = -1.0    return sign    # One return statement works fineCopyGlobal variables are compile-time constantsIn Taichi, a kernel treats global variables as compile-time constants. This means that it takes in the current values of the global variables at the time it is compiled and does not track changes to them afterwards. Consider the following example:import taichi as titi.init()a = 1@ti.kerneldef kernel_1():    print(a)@ti.kerneldef kernel_2():    print(a)kernel_1()  # Prints 1a = 2kernel_1()  # Prints 1kernel_2()  # Prints 2CopyHere, kernel_1 and kernel_2 both access the global variable a. The first call to kernel_1 prints 1, which is the value of a at the time the kernel was compiled. When a is updated to 2, the second call to kernel_1 still prints 1 because the kernel does not track changes to a after it is compiled.On the other hand, kernel_2 is compiled after a is updated, so it takes in the current value of a and prints 2.Taichi functionTaichi functions are fundamental units of a kernel and can only be called from within a kernel or another Taichi function.In the code snippet below, Taichi will raise an error because the function foo_1() is called from the Python scope, not the Taichi scope:# A normal Python functiondef foo_py():    print("This is a Python function.")@ti.funcdef foo_1():    print("This is a Taichi function to be called by another Taichi function, foo_2().")@ti.funcdef foo_2():    print("This is a Taichi function to be called by a kernel.")    foo_1()@ti.kerneldef foo_kernel():    print("This is a kernel calling a Taichi function, foo_2().")    foo_2()foo_py()# foo_1() # You cannot call a Taichi function from the Python scopefoo_kernel()CopyWARNINGAll Taichi functions are force-inlined. This means that if you call a Taichi function from another Taichi function, the calling function is fully expanded, or inlined, into the called function at compile time. This process continues until there are no more function calls to inline, resulting in a single, large function. This means that runtime recursion is not allowed in Taichi, because it would cause an infinite expansion of the function call stack at compile time.ArgumentsA Taichi function can accept multiple arguments, which may include scalar, ti.types.matrix(), ti.types.vector(), ti.types.struct(), ti.types.ndarray(), ti.field(), and ti.template() types. Note that some of the restrictions on kernel arguments do not apply to Taichi functions:It is not strictly required to type hint the function arguments (but it is still recommended).You can pass an unlimited number of elements in the function arguments.Return valuesReturn values of a Taichi function can be scalars, ti.types.matrix(), ti.types.vector(), ti.types.struct(), or other types. Note the following:Unlike a kernel, a Taichi function can have multiple return values.It is not required (but recommended) to type hint the return values of a Taichi function.A Taichi function cannot have more than one return statement.A recap: Taichi kernel vs. Taichi functionKernelTaichi FunctionCall scopePython scopeTaichi scopeType hint argumentsMandatoryRecommendedType hint return valuesMandatoryRecommendedReturn typeScalarti.types.matrix()ti.types.vector()ti.types.struct()(Only on LLVM-based backends)Scalarti.types.matrix()ti.types.vector()ti.types.struct()...Maximum number of elements in arguments32 (OpenGL)64 (otherwise)UnlimitedMaximum number of return values in a return statement1UnlimitedKey termsBackendIn the computer world, the term backend may have different meanings based on the context, and generally refers to any part of a software program that users do not directly engage with. In the context of Taichi, backend is the place where your code is being executed, for example cpu, opengl, cuda, and vulkan.Compile-time recursionCompile-time recursion is a technique of meta-programming. The recursion is handled by Taichi's compiler and expanded and compiled into a serial function without recursion. The recursion conditions must be constant during compile time, and the depth of the recursion must be a constant.Force inlineForce inline means that the users cannot choose whether to inline a function or not. The function will always be expanded into the caller by the compiler.MetaprogrammingMetaprogramming generally refers to the manipulation of programs with programs. In the context of Taichi, it means generating actual-running programs with compile-time computations. In many cases, this allows developers to minimize the number of code lines to express a solution.Runtime recursionRuntime recursion is the kind of recursion that happens at runtime. The compiler does not expand the recursion, and it is compiled into a function that calls itself recursively. The recursion conditions are evaluated at runtime, and the depth does not need to be a constant number.Type hintType hinting is a formal solution to statically indicate the type of value within your code.FAQCan I call a kernel from within a Taichi function?No. Keep in mind that a kernel is the smallest unit for Taichi's runtime execution. You cannot call a kernel from within a Taichi function (in the Taichi scope). You can only call a kernel from the Python scope.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?KernelArgumentsReturn valueGlobal variables are compile-time constantsTaichi functionArgumentsReturn valuesA recap: Taichi kernel vs. Taichi functionKey termsFAQCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Type System | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Type System>>Type SystemVersion: v1.6.0On this pageType SystemTaichi is a statically typed programming language, meaning that the type of a variable in the Taichi scope is determined at compile time. This means that once a variable has been declared, it cannot be assigned a value of a different type.Let's see a quick example:@ti.kerneldef test():    x = 1  # x is the integer 1    x = 3.14  # x is an integer, so the value 3.14 is cast to 3 and x takes the value 3    x = ti.Vector([1, 1])  # Error!CopyLine 3: x is an integer since it is assigned an integer value when it is declared for the first time.Line 4: x is reassigned a floating-point number 3.14. However, the value of x is 3 instead of 3.14. This is because 3.14 is automatically cast to an integer 3 to match the type of x.Line 5: The system throws an error, because ti.Vector cannot be cast into an integer.The ti.types module in Taichi defines all of the supported data types. These data types are categorized into two groups: primitive and compound.Primitive types encompass commonly utilized numerical data types, such as ti.i32 (int32), ti.u8 (uint8), and ti.f64 (float64).Compound types, on the other hand, encompass array-like or struct-like data types, including ti.types.matrix, ti.types.ndarray, and ti.types.struct. These types are composed of multiple members, which can be primitive or other compound types.Primitive typesThe primitive data types in Taichi are scalars, which are the smallest units that make up compound data types. These types are denoted with a letter indicating their category, followed by a number indicating their precision in bits. The category letter can be i for signed integers, u for unsigned integers, or f for floating-point numbers. The precision bits can be 8, 16, 32, or 64. The two most commonly used primitive types are:i32: 32-bit signed integerf32 : 32-bit floating-point number.The support of Taichi's primitive types by various backends may vary. Consult the following table for detailed information, and note that some backends may require extensions for complete support of a specific primitive type.Backendi8i16i32i64u8u16u32u64f16f32f64CPU✔️✔️✔️✔️✔️✔️✔️✔️✔️✔️✔️CUDA✔️✔️✔️✔️✔️✔️✔️✔️✔️✔️✔️OpenGL❌❌✔️⭕❌❌❌❌❌✔️✔️Metal✔️✔️✔️❌✔️✔️✔️❌❌✔️❌Vulkan⭕⭕✔️⭕⭕⭕✔️⭕✔️✔️⭕⭕: Requiring extensions for the backend.Customize default primitive typesWhen initializing the Taichi runtime, Taichi automatically uses the following data types as the default primitive data types:ti.i32: the default integer type.ti.f32: the default floating-point type.Taichi allows you to specify the default primitive data type(s) when calling init():ti.init(default_ip=ti.i64)  # Sets the default integer type to ti.i64ti.init(default_fp=ti.f64)  # Sets the default floating-point type to ti.f64CopynoteThe numeric literals in Taichi's scope have default integer or floating-point types. For example, if the default floating-point type is ti.f32, a numeric literal 3.14159265358979 will be cast to a 32-bit floating-point number with a precision of approximately seven decimal digits. To ensure high precision in applications such as engineering simulations, it is recommended to set default_fp to ti.f64.Data type AliasesIn Taichi scope, the two names int and float serve as aliases for the default integer and floating-point types, respectively. These default types can be changed using the configuration option default_ip and default_fp. For instance, setting the default_ip to i64 and default_fp to f64 would allow you to use int as an alias for i64 and float as an alias for f64 in your code.ti.init(default_ip=ti.i64, default_fp=ti.f64)@ti.kerneldef example_cast() -> int:  # the returned type is ti.i64    x = 3.14    # x is of ti.f64 type    y = int(x)  # equivalent to ti.i64(x)    return yCopyFurthermore, in the Python scope, when declaring Taichi's data containers using ti.field, ti.Vector, ti.Matrix, ti.ndarray, these two names also serve as aliases for the default integer and floating-point types. For example:x = ti.field(float, 5)# Is equivalent to:x = ti.field(ti.f64, 5)CopyHowever, when using int and float outside of Taichi's data containers in regular Python code, they refer to their standard meaning as built-in functions and not aliases for Taichi's default_ip and default_fp. Therefore, in Python scope and outside of Taichi's data containers, int and float have their standard meaning as built-in functions.x = numpy.array([1, 2, 3, 4], dtype=int)  # NumPy's int64 typey = int(3.14)  # Python's built-in int typeCopyExplicit type castingAs mentioned at the beginning of this document, the type of a variable in the Taichi scope is determined at compile time, meaning that it is statically typed. The Taichi compiler performs type checking at compile time and therefore, once a variable is declared, you cannot assign to it a value of a different type. However, in certain situations, you may need to switch to a different data type due to the unavailability of the original type for an assignment or calculation. In these cases, you must perform an explicit type casting.The ti.cast() function allows you to convert a given value to a specific target type. For instance, you can use ti.cast(x, float) to transform a variable x into a floating-point type.@ti.kerneldef foo():    a = 3.14    b = ti.cast(a, ti.i32)  # 3    c = ti.cast(b, ti.f32)  # 3.0CopyAs of Taichi v1.1.0, the capability to perform type casting on scalar variables has been introduced using primitive types such as ti.f32 and ti.i64. This allows you to convert scalar variables to different scalar types with ease.@ti.kerneldef foo():    a = 3.14    x = int(a)    # 3    y = float(a)  # 3.14    z = ti.i32(a)  # 3    w = ti.f64(a)  # 3.14CopyImplicit type castingImplicit type casting occurs when a value is placed or assigned where a different data type is expected.WARNINGAs a general principle, implicit type casting can be a significant source of bugs. As such, Taichi strongly discourages the use of this mechanism and recommends that you explicitly specify the desired data types for all variables and operations.Implicit type casting in binary operationsIn Taichi, implicit type casting can occur during binary operations or assignments. The casting rules are implemented specifically for Taichi and are slightly different from those for the C programming language. These rules are prioritized as follows:Integer + floating point -> floating pointi32 + f32 -> f32i16 + f16 -> f16Low-precision bits + high-precision bits -> high-precision bitsi16 + i32 -> i32f16 + f32 -> f32u8 + u16 -> u16Signed integer + unsigned integer -> unsigned integeru32 + i32 -> u32u8 + i8 -> u8When it comes to rule conflicts, the rule of the highest priority applies:u8 + i16 -> i16 (when rule #2 conflicts with rule #3, rule #2 applies.)f16 + i32 -> f16 (when rule #1 conflicts with rule #2, rule #1 applies.)A few exceptions:bit-shift operations return lhs' (left hand side's) data type:u8 << i32 -> u8i16 << i8 -> i16Logical operations return i32.Comparison operations return i32.Implicit type casting in assignmentsIn Taichi, implicit type casting is performed when assigning a value to a variable with a different data type. In cases where the value has a higher precision than the target variable, a warning indicating potential precision loss will be displayed.Example 1: The variable a is initialized with the data type float, and then immediately reassigned the value 1. This reassignment implicitly converts the data type of 1 from int to float without generating a warning.@ti.kerneldef foo():    a = 3.14    a = 1    print(a)  # 1.0CopyExample 2: The variable a is initialized with the data type int, and is immediately reassigned the value 3.14. This reassignment implicitly converts the data type of 3.14 from float to int, which results in a loss of precision due to int having a lower precision than float. As a result, a warning is generated.@ti.kerneldef foo():    a = 1    a = 3.14    print(a)  # 3CopyCompound typesCompound types are user-defined data types, which comprise multiple elements. Supported compound types include vectors, matrices, ndarrays, and structs.Taichi allows you to use all types supplied in the ti.types module as scaffolds to customize higher-level compound types.noteThe ndarray type is discussed in another document interacting with External Arrays.Matrices and vectorsYou can use the two functions ti.types.matrix() and ti.types.vector() to create your own matrix and vector types:vec4d = ti.types.vector(4, ti.f64)  # a 64-bit floating-point 4D vector typemat4x3i = ti.types.matrix(4, 3, int)  # a 4x3 integer matrix typeCopyYou can utilize the customized compound types to instantiate vectors and matrices, as well as annotate the data types of function arguments and struct members. For instance:v = vec4d(1, 2, 3, 4)  # Create a vector instance, here v = [1.0 2.0 3.0 4.0]@ti.funcdef length(w: vec4d):  # vec4d as type hint    return w.norm()@ti.kerneldef test():    print(length(v))CopyStruct types and dataclassYou can use the function ti.types.struct() to create a struct type, which can be utilized to represent a sphere in 3D space, abstracted by its center and radius. To achieve this, you can call ti.types.vector() and ti.types.struct() to create two higher-level compound types: vec3 and sphere_type, respectively. These types can then be used as templates to initialize two local variables, sphere1 and sphere2, to represent two instances of spheres.# Define a compound type vec3 to represent a sphere's centervec3 = ti.types.vector(3, float)# Define a compound type sphere_type to represent a spheresphere_type = ti.types.struct(center=vec3, radius=float)# Initialize sphere1, whose center is at [0,0,0] and whose radius is 1.0sphere1 = sphere_type(center=vec3([0, 0, 0]), radius=1.0)# Initialize sphere2, whose center is at [1,1,1] and whose radius is 1.0sphere2 = sphere_type(center=vec3([1, 1, 1]), radius=1.0)CopyWhen defining a struct with numerous members, the use of ti.types.struct can lead to cluttered and unorganized code. Taichi provides a more elegant solution with the @ti.dataclass decorator, which acts as a lightweight wrapper around the struct type.@ti.dataclassclass Sphere:    center: vec3    radius: floatCopyThe code above accomplishes the same task as the following line, however it offers improved comprehensibility:Sphere = ti.types.struct(center=vec3, radius=float)CopyAnother benefit of utilizing the @ti.dataclass over the ti.types.struct is the ability to define member functions within a dataclass, enabling object-oriented programming (OOP) capabilities. For more information on the topic of objective data-oriented programming, refer to the objective data-oriented programming documentation.InitializationIn Taichi, creating instances of vector, matrix, or struct compound types can be achieved by directly calling the type, similar to how it is done with any other data type.As of Taichi v1.1.0, multiple options are available for initializing instances of structs or dataclasses. The conventional method of calling a compound type directly still holds true. In addition, the following alternatives are also supported:Pass positional arguments to the struct, in the order in which the members are defined.Utilize keyword arguments to set the specific struct members.Members that are not specified will be automatically set to zero.For example:@ti.dataclassclass Ray:    ro: vec3    rd: vec3    t: float# The definition above is equivalent to#Ray = ti.types.struct(ro=vec3, rd=vec3, t=float)# Use positional arguments to set struct members in orderray = Ray(vec3(0), vec3(1, 0, 0), 1.0)# ro is set to vec3(0) and t will be set to 0ray = Ray(vec3(0), rd=vec3(1, 0, 0))# both ro and rd are set to vec3(0)ray = Ray(t=1.0)# ro is set to vec3(1), rd=vec3(0) and t=0.0ray = Ray(1)# All members are set to 0ray = Ray()CopynoteYou can create vectors, matrices, and structs using GLSL-like broadcast syntax because their shapes are already known.Type castingFor now, the only compound data types that support type casting in Taichi are vectors and matrices. When casting the type of a vector or matrix, it is performed element-wise, resulting in the creation of new vectors and matrices.@ti.kerneldef foo():    u = ti.Vector([2.3, 4.7])    v = int(u)              # ti.Vector([2, 4])    # If you are using ti.i32 as default_ip, this is equivalent to:    v = ti.cast(u, ti.i32)  # ti.Vector([2, 4])CopyEdit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Primitive typesCustomize default primitive typesData type AliasesExplicit type castingImplicit type castingCompound typesMatrices and vectorsStruct types and dataclassInitializationType castingCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Fields | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersFieldsFields (advanced)Taichi NdarraySpatially Sparse Data StructuresCoordinate OffsetsInteracting with External ArraysDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Data Containers>>FieldsVersion: v1.6.0On this pageFieldsThe term field is borrowed from mathematics and physics. If you already know scalar field (for example heat field), or vector field (for example gravitational field), then it is easy for you to understand fields in Taichi.Fields in Taichi are the global data containers, which can be accessed from both the Python scope and the Taichi scope. Just like an ndarray in NumPy or a tensor in PyTorch, a field in Taichi is defined as a multi-dimensional array of elements, and elements in a field can be a Scalar, a Vector, a Matrix, or a Struct.Scalar fieldsScalar fields refer to the fields that store scalars and are the most basic fields.A 0D scalar field is a single scalar.A 1D scalar field is a 1D array of scalars.A 2D scalar field is a 2D array of scalars, and so on.DeclarationThe simplest way to declare a scalar field is to call ti.field(dtype, shape), where dtype is a primitive data type as explained in the Type System and shape is a tuple of integers.When declaring a 0D scalar field, you need to set its shape to the empty tuple ():# Declares a 0D scalar field whose data type is f32f_0d = ti.field(ti.f32, shape=())  # 0D fieldCopyAn illustration of f_0d:    ┌─────┐    │     │    └─────┘    └─────┘ f_0d.shape=()CopyWhen declaring a 1D scalar field of length n, set its shape to n or (n,):f_1d = ti.field(ti.i32, shape=9)  # A 1D field of length 9CopyAn illustration of f_1d:┌───┬───┬───┬───┬───┬───┬───┬───┬───┐│   │   │   │   │   │   │   │   │   │└───┴───┴───┴───┴───┴───┴───┴───┴───┘└───────────────────────────────────┘        f_1d.shape = (9,)CopyThere is little difference between a 0D field and a 1D field of length 1 except for their indexing rules. You must use None as the index to access a 0D field and 0 as the index to access a 1D field of length 1:f1 = ti.field(int, shape=())f2 = ti.field(int, shape=1)f1[None] = 1  # Use None to access a 0D fieldf2[0] = 1  # Use 0 to access a 1D field of length 1CopyWhen declaring a 2D scalar field, you need to set its two dimensions (numbers of rows and columns) respectively. For example, the following code snippet defines a 2D scalar field with the shape (3, 6) (three rows and six columns):f_2d = ti.field(int, shape=(3, 6))  # A 2D field in the shape (3, 6)CopyHere is an illustration of f_2d:                       f_2d.shape[1]                           (=6)                 ┌───────────────────────┐              ┌  ┌───┬───┬───┬───┬───┬───┐  ┐              │  │   │   │   │   │   │   │  │              │  ├───┼───┼───┼───┼───┼───┤  │f_2d.shape[0] │  │   │   │   │   │   │   │  │    (=3)      │  ├───┼───┼───┼───┼───┼───┤  │              │  │   │   │   │   │   │   │  │              └  └───┴───┴───┴───┴───┴───┘  ┘CopyScalar fields of higher dimensions can be similarly defined.WARNINGTaichi only supports fields of dimensions ≤ 8.Accessing elements in a scalar fieldOnce a field is declared, Taichi automatically initializes its elements to zero.To access an element in a scalar field, you need to explicitly specify the element's index.noteWhen accessing a 0D field x, use x[None] = 0, not x = 0.To access the element in a 0D field, use the index None even though the field has only one element:f_0d = ti.field(ti.f32, shape=())f_0d[None] = 10.0CopyThe layout of f_0d:    ┌──────┐    │ 10.0 │    └──────┘    └──────┘  f_0d.shape=()CopyTo access an element in a 1D field, use index i to get the i-th element of our defined field.f_1d = ti.field(ti.f32, shape=(9,))@ti.kerneldef loop_over_1d():  for i in range(9):      f_1d[i] = iloop_over_1d()CopyThe layout of f_1d:┌───┬───┬───┬───┬───┬───┬───┬───┬───┐│ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │ 6 │ 7 │ 8 │└───┴───┴───┴───┴───┴───┴───┴───┴───┘CopyTo access an element in a 2D field, use index (i, j), which is an integer pair to get the i-th, j-th element of our defined field.f_2d = ti.field(ti.f32, shape=(16, 16))@ti.kerneldef loop_over_2d():  for i, j in f_2d:      f_2d[i, j] = iloop_over_2d()CopyThe layout of f_2d:┌───┬───┬───┬───┬───┬───┐│ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │├───┼───┼───┼───┼───┼───┤│ 1 │ 1 │ 1 │ 1 │ 1 │ 1 │├───┼───┼───┼───┼───┼───┤│ 2 │ 2 │ 2 │ 2 │ 2 │ 2 │└───┴───┴───┴───┴───┴───┘CopyTo access an element in an n-dimensional field, use index (i, j, k, ...), which is an n-tuple of integers.You can use a 2D scalar field to represent a 2D grid of values. The following code snippet creates and displays a 640×480 gray scale image of randomly-generated values:import taichi as titi.init(arch=ti.cpu)width, height = 640,480# Creates a 640x480 scalar field, each of its elements representing a pixel value (f32)gray_scale_image = ti.field(dtype=ti.f32, shape=(width, height))@ti.kerneldef fill_image():    # Fills the image with random gray    for i,j in gray_scale_image:        gray_scale_image[i,j] = ti.random()fill_image()# Creates a GUI of the size of the gray-scale imagegui = ti.GUI('gray-scale image of random values', (width, height))while gui.running:    gui.set_image(gray_scale_image)    gui.show()CopyWARNINGTaichi fields do not support slicing. Neither of the following usages are correct:for x in f_2d[0]:  # Error! You tried to access its first row，but it is not supported    ...Copyf_2d[0][3:] = [4, 5, 6]  # Error! You tried to access a slice of the first row, but it is not supportedCopyEither way, the system throws an error message 'Slicing is not supported on ti.field'.Fill a scalar field with a given valueTo set all elements in a scalar field to a given value, call field.fill():x = ti.field(int, shape=(5, 5))x.fill(1)  # Sets all elements in x to 1@ti.kerneldef test():    x.fill(-1)  # Sets all elements in x to -1CopyMetadataMetadata provides the basic information of a scalar field. You can retrieve the data type and shape of a scalar field via its shape and dtype properties:f_1d.shape  # (9,)f_3d.dtype  # f32CopyVector fieldsAs the name suggests, vector fields are the fields whose elements are vectors. What a vector represents depends on the scenario of your program. For example, a vector may stand for the (R, G, B) triple of a pixel, the position of a particle, or the gravitational field in space.DeclarationDeclaring a vector field where each element is an N-dimensional vector is similar to declaring a scalar field, except that you need to call ti.Vector.field instead of ti.field and specify N as the first positional argument.For example, the following code snippet declares a 2D field of 2D vectors:# Declares a 3x3 vector field comprising 2D vectorsf = ti.Vector.field(n=2, dtype=float, shape=(3, 3))CopyThe layout of f:                     f.shape[1]                       (=3)               ┌────────────────────┐            ┌  ┌──────┬──────┬──────┐  ┐            │  │[*, *]│[*, *]│[*, *]│  │            │  ├──────┼──────┼──────┤  │ f.shape[0] │  │[*, *]│[*, *]│[*, *]│  │     [*,  *]    (=3)    │  ├──────┼──────┼──────┤  │     └─────┘            │  │[*, *]│[*, *]│[*, *]│  │       n=2            └  └──────┴──────┴──────┘  ┘CopyThe following code snippet declares a 300x300x300 vector field volumetric_field, whose vector dimension is 4:box_size = (300, 300, 300)  # A 300x300x300 grid in a 3D space# Declares a 300x300x300 vector field, whose vector dimension is n=4volumetric_field = ti.Vector.field(n=4, dtype=ti.f32, shape=box_size)CopyAccessing elements in a vector fieldAccessing a vector field is similar to accessing a multi-dimensional array: You use an index operator [] to access an element in the field. The only difference is that, to access a specific component of an element (vector in this case), you need an extra index operator []:To access the velocity vector at a specific position of the volumetric field above:volumetric_field[i, j, k]To access the l-th component of the velocity vector:volumetric_field[i, j, k][l]noteAlternatively, you can use swizzling with the indices xyzw or rgba to access the components of a vector, provided that the dimension of the vector is no more than four:volumetric_field[i, j, k].x = 1  # Equivalent to volumetric_field[i, j, k][0] = 1volumetric_field[i, j, k].y = 2  # Equivalent to volumetric_field[i, j, k][1] = 2volumetric_field[i, j, k].z = 3  # Equivalent to volumetric_field[i, j, k][2] = 3volumetric_field[i, j, k].w = 4  # Equivalent to volumetric_field[i, j, k][3] = 4volumetric_field[i, j, k].xyz = 1, 2, 3  # Assigns 1, 2, 3 to the first three componentsvolumetric_field[i, j, k].rgb = 1, 2, 3  # Equivalent to the aboveCopyThe following code snippet generates and prints a random vector field:# n: vector dimension; w: width; h: heightn, w, h = 3, 128, 64vec_field = ti.Vector.field(n, dtype=float, shape=(w,h))@ti.kerneldef fill_vector():    for i,j in vec_field:        for k in ti.static(range(n)):            #ti.static unrolls the inner loops            vec_field[i,j][k] = ti.random()fill_vector()print(vec_field[w-1,h-1][n-1])CopynoteTo access the p-th component of the 0D vector field x = ti.Vector.field(n=3, dtype=ti.f32, shape=()):x[None][p] (0 ≤ p < n).Matrix fieldsAs the name suggests, matrix fields are the fields whose elements are matrices. In continuum mechanics, at each infinitesimal point in a 3D material exists a strain and stress tensor, which is a 3 x 2 matrix. We can use a matrix field to represent this tensor.DeclarationThe following code snippet declares a tensor field:# Declares a 300x400x500 matrix field, each of its elements being a 3x2 matrixtensor_field = ti.Matrix.field(n=3, m=2, dtype=ti.f32, shape=(300, 400, 500))CopyAccessing elements in a matrix fieldAccessing a matrix field is similar to accessing a vector field: You use an index operator [] for field indexing and a second index operator [] for matrix indexing.To access the i-th, j-th element of the matrix field tensor_field:mat = tensor_field[i, j]To access the member on the first row and second column of the element mat:mat[0, 1] or tensor_field[i, j][0, 1]noteTo access the 0D matrix field x = ti.Matrix.field(n=3, m=4, dtype=ti.f32, shape=()):x[None][p, q] (0 ≤ p < n, 0 ≤ q < m)Considerations: Matrix sizeMatrix operations are unrolled at compile time. Take a look at the following example:import taichi as titi.init()a = ti.Matrix.field(n=2, m=3, dtype=ti.f32, shape=(2, 2))@ti.kerneldef test():    for i in ti.grouped(a):        # a[i] is a 2x3 matrix        a[i] = [[1, 1, 1], [1, 1, 1]]        # The assignment is unrolled to the following at compile time:        # a[i][0, 0] = 1        # a[i][0, 1] = 1        # a[i][0, 2] = 1        # a[i][1, 0] = 1        # a[i][1, 1] = 1        # a[i][1, 2] = 1CopyOperating on larger matrices (for example 32x128) can lead to longer compilation time and poor performance. For performance reasons, it is recommended that you keep your matrices small:2x1, 3x3, and 4x4 matrices work fine.32x6 is a bit too large.Workaround:When declaring a matrix field, leave large dimensions to the fields, rather than to the matrices. If you have a 3x2 field of 64x32 matrices:Not recommended:
ti.Matrix.field(64, 32, dtype=ti.f32, shape=(3, 2))Recommended:
ti.Matrix.field(3, 2, dtype=ti.f32, shape=(64, 32))Struct fieldsStruct fields are fields that store user-defined structs. Members of a struct element can be:ScalarsVectorsMatricesOther struct fields.DeclarationThe following code snippet declares a 1D field of particle information (position, velocity, acceleration, and mass) using ti.Struct.field(). Note that:Member variables pos, vel, acc, and mass are provided in the dictionary format.You can use compound types, such as ti.types.vector, ti.types.matrix, and ti.types.struct, to declare vectors, matrices, or structs as struct members.# Declares a 1D struct field using the ti.Struct.field() methodn = 10particle_field = ti.Struct.field({    "pos": ti.math.vec3,    "vel": ti.math.vec3,    "acc": ti.math.vec3,    "mass": float,  }, shape=(n,))CopyBesides directly using ti.Struct.field(), you can first declare a compound type particle and then create a field of it:# vec3 is a built-in vector type suppied in the `taichi.math` modulevec3 = ti.math.vec3n = 10# Declares a struct comprising three vectors and one floating-point numberparticle = ti.types.struct(  pos=vec3, vel=vec3, acc=vec3, mass=float,)# Declares a 1D field of the struct particle by calling field()particle_field = particle.field(shape=(n,))CopyAccessing elements in a struct fieldYou can access a member of an element in a struct field in either of the following ways: index-first or name-first.The index-first approach locates a certain element with its index before specifying the name of the target member:# Sets the position of the first particle in the field to [0.0, 0.0, 0.0]particle_field[0].pos = vec3(0) # particle_field is a 1D struct field, pos is a 3D vectorCopyThe name-first approach first creates a sub-field, which gathers all the mass members in the struct field, and then uses the index operator [] to access a specific member:particle_field.mass[0] = 1.0  # Sets the mass of the first particle in the field to 1.0CopyConsidering that particle_field.mass is a field consisting of all the mass members of the structs in particle_field, you can also call fill() to set the members to a specific value all at once:particle_field.mass.fill(1.0)  # Sets all mass of the particles in the struct field to 1.0CopyEdit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Scalar fieldsDeclarationAccessing elements in a scalar fieldFill a scalar field with a given valueMetadataVector fieldsDeclarationAccessing elements in a vector fieldMatrix fieldsDeclarationAccessing elements in a matrix fieldConsiderations: Matrix sizeStruct fieldsDeclarationAccessing elements in a struct fieldCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Fields (advanced) | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersFieldsFields (advanced)Taichi NdarraySpatially Sparse Data StructuresCoordinate OffsetsInteracting with External ArraysDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Data Containers>>Fields (advanced)Version: v1.6.0On this pageFields (advanced)Modern processor cores compute orders of magnitude faster than their equipped memory systems. To shrink this  performance gap, multi-level cache systems and high-bandwidth multi-channel memories are built into the computer architectures.After familiarizing yourself with the basics of Taichi Fields, this article helps you one step further by explaining the underlying memory layout that is essential to write high-performance Taichi programs. In particular, we present how to organize an efficient data layout and how to manage memory occupancy.Organizing an efficient data layoutIn this section, we introduce how to organize data layouts in Taichi fields. The central principle of efficient data layout is locality. Generally speaking, a program with desirable locality has at least one of the following features:Dense data structuresSmall-range data looping (within 32KB is good for most processors)Sequentially loading and storing datanoteBe aware that data is traditionally fetched from memory in blocks (pages). In addition, the hardware itself has little knowledge about how a specific data element is used in the block. The processor blindly fetches the entire block according to the requested memory address. Thus, the memory bandwidth is wasted when data are not fully utilized.For sparse fields, see the Sparse computation.Layout 101: from shape to ti.root.XIn basic usages, we use the shape descriptor to construct a field. Taichi provides flexible statements to describe more advanced data organizations, the ti.root.X.
Here are some examples:Declare a 0-D field:x = ti.field(ti.f32)ti.root.place(x)# is equivalent to:x = ti.field(ti.f32, shape=())CopyDeclare a 1-D field of shape 3:x = ti.field(ti.f32)ti.root.dense(ti.i, 3).place(x)# is equivalent to:x = ti.field(ti.f32, shape=3)CopyDeclare a 2-D field of shape (3, 4):x = ti.field(ti.f32)ti.root.dense(ti.ij, (3, 4)).place(x)# is equivalent to:x = ti.field(ti.f32, shape=(3, 4))CopyYou can also nest two 1D dense statements to describe a 2D array of the same shape.x = ti.field(ti.f32)ti.root.dense(ti.i, 3).dense(ti.j, 4).place(x)# has the same shape withx = ti.field(ti.f32, shape=(3,4))CopynoteThe above 2D array built with nested dense statements is not equivalent to the 2D array built with ti.field.
Although both statements result in a 2D array of the same shape, they have
different layers of SNodeTree, as shown below:The following snippet has two SNodeTree layers below the root:x = ti.field(ti.f32)ti.root.dense(ti.i, 3).dense(ti.j, 4).place(x)CopyThe following snippet only has one SNodeTree below the root:x = ti.field(ti.f32)ti.root.dense(ti.ij, (3, 4)).place(x)# or equivalentlyx = ti.field(ti.f32, shape=(3,4))CopySee the sketch below for a visualization that illustrates the difference between these layers:The difference here is subtle as both arrays are row-major, but it may have slight performance impact
because the overhead of calculating the SNodeTree index is different for the two.In a nutshell, the ti.root.X statement progressively binds a shape to the corresponding axis.
By nesting multiple statements, we can construct a field with higher dimensions.In order to traverse the nested statements, you can use a for loop:for i, j in A:    A[i, j] += 1CopyThe Taichi compiler is capable of automatically deducing the underlying data layout and applying a proper data access order. This is an advantage over most general-purpose programming languages where the access order has to be optimized manually.Row-major versus column-majorOne important thing to note about memory addresses is that their space is linear. Without loss of generality, we omit the differences in data types and assume each data element has size 1. Moreover, we denote the starting memory address of a field as base, and the indexing formula for 1D Taichi fields is base + i for the i-th element.For multi-dimensional fields, we can flatten the high-dimension index into the linear memory address space in two ways: Taking a 2D field of shape (M, N) as an instance, we can either store M rows with N-length 1D buffers, say the row-major way, or store N columns, say the column-major way. The index flatten formula for the (i, j)-th element is base + i * N + j for row-major and base + j * M + i for column-major, respectively.Trivially, elements in the same row are close in memory for row-major fields. The selection of the optimal layout is based on how the elements are accessed, namely, the access patterns. Patterns such as frequently accessing elements of the same row in a column-major field typically lead to performance degradation.The default Taichi field layout is row-major. With the ti.root statements, fields can be defined as follows:x = ti.field(ti.f32)y = ti.field(ti.f32)ti.root.dense(ti.i, M).dense(ti.j, N).place(x)   # row-majorti.root.dense(ti.j, N).dense(ti.i, M).place(y)   # column-majorCopyIn the code above, the axis denotation in the rightmost dense statement indicates the continuous axis. For the x field, elements in the same row (with same i and different j) are close in memory, hence it's row-major; For the y field, elements in the same column (same j and different i) are close, hence it's column-major. With an example of (2, 3), we visualize the memory layouts of x and y as follows:# address:  low ........................................... high#       x:  x[0, 0]  x[0, 1]  x[1, 0]  x[1, 1]  x[2, 0]  x[2, 1]#       y:  y[0, 0]  y[1, 0]  y[2, 0]  y[0, 1]  y[1, 1]  y[2, 1]CopyIt is worth noting that the accessor is unified for Taichi fields: the (i, j)-th element in the field is accessed with the identical 2D index x[i, j] and y[i, j]. Taichi handles the layout variants and applies proper indexing equations internally. Thanks to this feature, users can specify their desired layout at definition, and use the fields without concerning about the underlying memory organizations. To change the layout, we can simply swap the order of dense statements, and leave rest of the code intact.noteFor readers who are familiar with C/C++, below is an example C code snippet that demonstrates data access in 2D arrays:int x[3][2];  // row-majorint y[2][3];  // column-majorfor (int i = 0; i < 3; i++) {    for (int j = 0; j < 2; j++) {        do_something(x[i][j]);        do_something(y[j][i]);    }}CopyThe accessors of x and y are in reverse order between row-major arrays and column-major arrays, respectively. Compared with Taichi fields, there is much more code to revise when you change the memory layout.AoS versus SoAAoS means array of structures and SoA means structure of arrays. Consider an RGB image with four pixels and three color channels: an AoS layout stores RGBRGBRGBRGB while an SoA layout stores RRRRGGGGBBBB. The selection of an AoS or SoA layout largely depends on the access pattern to the field. Let's discuss a scenario to process large RGB images. The two layouts have the following arrangements in memory:# address: low ...................... high# AoS:     RGBRGBRGBRGBRGBRGB.............# SoA:     RRRRR...RGGGGGGG...GBBBBBBB...BCopyTo calculate grey scale of each pixel, you need all color channels but do not require the value of other pixels. In this case, the AoS layout has a better memory access pattern: Since color channels are stored continuously, and adjacent channels can be fetched instantly. The SoA layout is not a good option because the color channels of a pixel are stored far apart in the memory space.We describe how to construct AoS and SoA fields with our ti.root.X statements. The SoA fields are trivial:x = ti.field(ti.f32)y = ti.field(ti.f32)ti.root.dense(ti.i, M).place(x)ti.root.dense(ti.i, M).place(y)Copywhere M is the length of x and y.
The data elements in x and y are continuous in memory:#  address: low ................................. high#           x[0]  x[1]  x[2] ... y[0]  y[1]  y[2] ...CopyFor AoS fields, we construct the field withx = ti.field(ti.f32)y = ti.field(ti.f32)ti.root.dense(ti.i, M).place(x, y)CopyThe memory layout then becomes#  address: low .............................. high#           x[0]  y[0]  x[1]  y[1]  x[2]  y[2] ...CopyHere, place interleaves the elements of Taichi fields x and y.As previously introduced, the access methods to x and y remain the same for both  AoS and SoA. Therefore, the data layout can be changed flexibly without revising the application logic.For better illustration, let's see an example of an 1D wave equation solver:N = 200000pos = ti.field(ti.f32)vel = ti.field(ti.f32)# SoA placementti.root.dense(ti.i, N).place(pos)ti.root.dense(ti.i, N).place(vel)@ti.kerneldef step():    pos[i] += vel[i] * dt    vel[i] += -k * pos[i] * dtCopyThe above code snippet defines SoA fields and a step kernel that sequentially accesses each element.
The kernel fetches an element from pos and vel for every iteration, respectively.
For SoA fields, the closest distance of any two elements in memory is N, which is unlikely to be efficient for large N.We hereby switch the layout to AoS as follows:N = 200000pos = ti.field(ti.f32)vel = ti.field(ti.f32)# AoS placementti.root.dense(ti.i, N).place(pos, vel)@ti.kerneldef step():    pos[i] += vel[i] * dt    vel[i] += -k * pos[i] * dtCopyMerely revising the place statement is sufficient to change the layout. With this optimization, the instant elements pos[i] and vel[i] are now adjacent in memory, which is more efficient.AoS extension: hierarchical fieldsSometimes we want to access memory in a complex but fixed pattern, like traversing an image in 8x8 blocks. The apparent best practice is to flatten each 8x8 block and concatenate them together. However, the field is no longer a flat buffer as it now has a hierarchy with two levels: The image level and the block level. Equivalently, the field is an array of implicit 8x8 block structures.We demonstrate the statements as follows:# Flat fieldval = ti.field(ti.f32)ti.root.dense(ti.ij, (M, N)).place(val)Copy# Hierarchical fieldval = ti.field(ti.f32)ti.root.dense(ti.ij, (M // 8, N // 8)).dense(ti.ij, (8, 8)).place(val)Copywhere M and N are multiples of 8. We encourage you to try this out! The performance difference can be significant!noteWe highly recommend that you use power-of-two block size so that accelerated indexing with bitwise arithmetic and better memory address alignment can be enabled.Manage memory occupancyManual field allocation and destructionGenerally, Taichi manages memory allocation and destruction without disturbing the users. However, there are times that users want explicit control over their memory allocations.In this scenario, Taichi provides the FieldsBuilder for manual field memory allocation and destruction. FieldsBuilder features identical declaration APIs as ti.root. The extra step is to invoke finalize() at the end of all declarations. The finalize() returns an SNodeTree object to handle subsequent destructions.Let's see a simple example:import taichi as titi.init()@ti.kerneldef func(v: ti.template()):    for I in ti.grouped(v):        v[I] += 1fb1 = ti.FieldsBuilder()x = ti.field(dtype=ti.f32)fb1.dense(ti.ij, (5, 5)).place(x)fb1_snode_tree = fb1.finalize()  # Finalizes the FieldsBuilder and returns a SNodeTreefunc(x)fb1_snode_tree.destroy()  # Destructionfb2 = ti.FieldsBuilder()y = ti.field(dtype=ti.f32)fb2.dense(ti.i, 5).place(y)fb2_snode_tree = fb2.finalize()  # Finalizes the FieldsBuilder and returns a SNodeTreefunc(y)fb2_snode_tree.destroy()  # DestructionCopyThe above demonstrated ti.root statements are implemented with FieldsBuilder, despite that ti.root has the capability to automatically manage memory allocations and recycling.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Organizing an efficient data layoutLayout 101: from shape to ti.root.XRow-major versus column-majorAoS versus SoAAoS extension: hierarchical fieldsManage memory occupancyManual field allocation and destructionCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Taichi Ndarray | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersFieldsFields (advanced)Taichi NdarraySpatially Sparse Data StructuresCoordinate OffsetsInteracting with External ArraysDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Data Containers>>Taichi NdarrayVersion: v1.6.0On this pageTaichi NdarrayThe Taichi ndarray is an array object that holds contiguous multi-dimensional data. Generally speaking, it plays a similar role to its counterpart numpy.ndarray in NumPy, but its underlying memory is allocated on the user-specified Taichi arch and managed by Taichi runtime.When to use ndarrayYou can use fields as data containers in most cases. However, fields might have very complicated tree-structured layouts with even sparsity in them. It is hard for external libraries to interpret or use computed results stored in ti.field directly. An ndarray, however, always allocates a contiguous memory block to allow straightforward data exchange with external libraries.In a nutshell, fields are mainly used for maximizing performance with complex data layouts. If you process dense data only or need external interop, just move forward with ndarrays!Python scope usagesThe statement below instantiates an instance of a Taichi ndarray. dtype refers to the data type, which can either be a scalar data type like ti.f32/ti.i32 or a vector/matrix data type like ti.math.vec2/mat2. shape denotes the array size with respect to the data type. Like fields, ndarray can only be constructed in the Python scope rather than in the Taichi scope. That is to say, an ndarray cannot be constructed inside Taichi kernels or functions.arr = ti.ndarray(dtype=ti.math.vec3, shape=(4, 4))CopySimilar to ti.field, ndarrays are allocated on the arch as specified in ti.init and are initialized to 0 by default.Apart from the constructor, Taichi provides some basic operations to interact with ndarray data from the Python scope.Fill in the ndarray with a scalar valuearr.fill(1.0)CopyRead/write ndarray elements from the Python scope# Returns a ti.Vector, which is a copy of the elementprint(arr[0, 0]) # [1.0, 1.0, 1.0]# Writes to an elementarr[0, 0] = [1.0, 2.0, 3.0] # arr[0, 0] is now [1.0, 2.0, 3.0]# Writes to a scalar inside vector elementarr[0, 0][1] = 2.2  # arr[0, 0] is now [1.0, 2.2, 3.0]CopynoteAccessing elements of an ndarray from the Python scope can be convenient, but it can also result in the creation and launch of multiple small Taichi kernels. This is not the most efficient approach from a performance standpoint. It is recommended that computationally intensive tasks be performed within a single Taichi kernel rather than operating on array elements individually from the Python scope.Data copy of ndarrays  Both shallow copy and deep copy from one ndarray to another are supported.# Copies from another ndarray with the same sizeb = ti.ndarray(dtype=ti.math.vec3, shape=(4, 4))b.copy_from(arr)  # Copies all data from arr to bimport copy# Deep copyc = copy.deepcopy(b)  # c is a new ndarray that has a copy of b's data.# Shallow copyd = copy.copy(b)  # d is a shallow copy of b; they share the underlying memoryd[0, 0][0] = 1.2  # This mutates b as well, so b[0, 0][0] is now 1.2CopyBidirectional data exchange with NumPy ndarrays# to_numpy returns a NumPy array with the same shape as d and a copy of d's valuee = d.to_numpy()# from_numpy copies the data in the NumPy array e to the Taichi ndarray de.fill(10.0)  # Fills in the NumPy array with value 10.0d.from_numpy(e)  # Now d is filled in with 10.0CopyUsing Taichi ndarrays in ti.kernelTo use an ndarray in a Taichi kernel, you need to properly annotate its type in the kernel definition and pass the Ndarray object to the Taichi kernel at runtime. Ndarrays are passed by reference. Therefore, you can mutate their content inside Taichi kernels. The following example shows how to specify the type annotation for an ndarray:@ti.kerneldef foo(A: ti.types.ndarray(dtype=ti.f32, ndim=2)):    do_something()CopyIt is important to note that the dtype and ndim arguments are optional when an ndarray is instantiated. If left unspecified, the data type and the number of dimensions are inferred from the passed-in array at runtime. However, if the arguments are specified, Taichi validates that the specified data type and dimensions match those of the passed-in array. If a mismatch is detected, an error is thrown.In certain scenarios, it may be necessary to process arrays with vector or matrix elements, such as an RGB pixel map (vec3). Taichi provides support for these types of arrays through the use of vector and matrix data types. An example of this would be creating an ndarray for a pixel map with vec3 elements, as demonstrated in the following code snippet:import taichi as titi.init(arch=ti.cuda)arr_ty = ti.types.ndarray(dtype=ti.math.vec3, ndim=2)@ti.kerneldef proc(rgb_map : arr_ty):    for I in ti.grouped(rgb_map):        rgb_map[I] = [0.1, 0.2, 0.3]    # do somethingrgb = ti.ndarray(dtype=ti.types.vector(3, ti.f32), shape=(8,8))proc(rgb)CopyIt does not matter whether you use the range-for or struct-for to iterate over the ndarrays.tipIn the above code, we use arr_ty as a type alias for the 2D ndarray type of vec3 elements. The type alias makes type annotations shorter and easier to read.Using external data containers in ti.kernelThe introduction of Taichi ndarrays achieves the seemless interop with external data containers. With the argument annotation ti.types.ndarray, a kernel accepts not only Taichi ndarrays but also external arrays. Currently, the external arrays supported by Taichi are NumPy ndarrays and PyTorch tensors.The code snippet below defines a Taichi kernel that adds 1.0 to every element in the ndarray arr.ti.init(arch=ti.cuda)@ti.kerneldef add_one(arr : ti.types.ndarray(dtype=ti.f32, ndim=2)):    for i in ti.grouped(arr):        arr[i] = arr[i] + 1.0CopyThe external arrays can be fed into the Taichi kernel without further type conversions.To feed a NumPy ndarray:arr_np = np.ones((3, 3), dtype=np.float32)add_one(arr_np) # arr_np is updated by taichi kernelCopyTo feed a PyTorch tensor:arr_torch = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], device='cuda:0')add_one(arr_torch) # arr_torch is updated by taichi kernelCopynoteEvery element in the external array (arr_np and arr_torch) is added by 1.0 when the Taichi kernel add_one finishes.When the external data container and Taichi are utilizing the same device, passing arguments incurs no additional overhead. This can be seen in the PyTorch example above, where the tensor is allocated on the CUDA device, which is also the device utilized by Taichi. As a result, the kernel function can access and manipulate the data in the original CUDA buffer allocated by PyTorch without incurring any extra costs.On the other hand, if the devices being used are different, as is the case in the first example where Numpy utilizes CPUs and Taichi utilizes CUDA, Taichi automatically manages the transfer of data between devices, eliminating the need for manual intervention on the part of the user.tipNumPy's default data precision is 64-bit, which is still inefficient for most desktop GPUs. It is recommended to explicitly specify 32-bit data types.tipOnly contiguous NumPy arrays and PyTorch tensors are supported.# Transposing the tensor returns a view of the tensor, which is not contiguousp = arr_torch.T# add_one(p) # Error!z = p.clone()add_one(z) # Correctk = p.contiguous()addd_one(k) # CorrectCopyWhen a NumPy ndarray or a PyTorch tensor of scalar type is passed as the argument to a Taichi kernel, it can be interpreted as an array of scalar type, or an array of vector type, or an array of matrix type. This is controlled by the dtype and ndim options in the type hint ti.types.ndarray().When the array is interpreted as a vector/matrix array, you should set dtype to the correct vector/matrix type. For example, you can safely pass a NumPy ndarray in shape (2, 2, 3, 3) as an argument into the add_one kernel, as shown below:@ti.kerneldef add_one(arr : ti.types.ndarray(dtype=ti.math.mat3, ndim=2)):    for i in ti.grouped(arr):        arr[i] = arr[i] + 1.0CopyKernel compilation with ndarray templateIn the examples above, dtype and ndim were specified explicitly in the kernel type hints, but Taichi also allows you to skip such details and just annotate the argument as ti.types.ndarray(). When one ti.kernel definition works with different (dtype, ndim) inputs, you do not need to duplicate the definition each time.For example:@ti.kerneldef test(arr: ti.types.ndarray()):    for I in ti.grouped(arr):        arr[I] += 2CopyConsider ti.types.ndarray() as a template type on parameters dtype and ndim. Equipped with a just-in-time (JIT) compiler, Taichi goes through the following two steps when a kernel with templated ndarray parameters is invoked:First, Taichi checks whether a kernel with the same dtype and ndim inputs has been compiled. If yes, it loads and launches the compiled kernel directly with the input arguments.If your templated kernel needs to be compiled - either because it has never been compiled before or because it is invoked with an input of different (dtype, ndim), kernel compilation is automatically triggered and executed. Note that the compiled kernel is also cached for future use.The code snippet below provides more examples to demonstrate the behavior:a = ti.ndarray(dtype=ti.math.vec3, shape=(4, 4))b = ti.ndarray(dtype=ti.math.vec3, shape=(5, 5))c = ti.ndarray(dtype=ti.f32, shape=(4, 4))d = ti.ndarray(dtype=ti.f32, shape=(8, 6))e = ti.ndarray(dtype=ti.math.vec3, shape=(4, 4, 4))test(a) # New kernel compilationtest(b) # Reuse kernel compiled for atest(c) # New kernel compilationtest(d) # Reuse kernel compiled for ctest(e) # New kernel compilationCopyThe compilation rule also applies to external arrays from NumPy or PyTorch. Changing the shape values does not trigger compilation, but changing the data type or the number of array dimensions does.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?When to use ndarrayPython scope usagesUsing Taichi ndarrays in ti.kernelUsing external data containers in ti.kernelKernel compilation with ndarray templateCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Spatially Sparse Data Structures | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersFieldsFields (advanced)Taichi NdarraySpatially Sparse Data StructuresCoordinate OffsetsInteracting with External ArraysDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Data Containers>>Spatially Sparse Data StructuresVersion: v1.6.0On this pageSpatially Sparse Data StructuresnotePrerequisite: please read the Fields, Fields (advanced), and SNodes first.
Figure: A 3D fluid simulation that uses both particles and grids. Left to right: particles, 1x1x1 voxels, 4x4x4 blocks, 16x16x16 blocks.MotivationIn large-scale spatial computing, such as physical modelling, graphics, and 3D reconstruction, high-resolution 2D/3D grids are frequently required. However, if we employ dense data structures, these grids tend to consume a significant amount of memory space and processing (see field and field advanced). While a programmer may allocate large dense grids to store spatial data (particularly physical qualities such as a density or velocity field), they may only be interested in a tiny percentage of this dense grid because the remainder may be empty space (vacuum or air).To illustrate this idea, the regions of interest in sparse grids shown below may only occupy a small fraction of the whole bounding box.
If we can leverage such "spatial sparsity" and focus computation on the regions we care about,
we will significantly save storage and computing power.noteThe key to leveraging spatial sparsity is replacing dense grids with sparse grids.Sparse data structures are traditionally based on Quadtrees (2D) and
Octrees (3D). Given that dereferencing pointers is relatively costly on modern computer architectures, Quadtrees and Octrees are less performance friendly than shallower trees with larger branching factors, such as
VDB and SPGrid.
In Taichi, you can compose data structures similar to VDB and SPGrid with SNodes. The advantages of Taichi spatially sparse data structures include:Access with indices, which just like accessing a dense data structure.Automatic parallelization when iterating.Automatic memory access optimization.noteBackend compatibility: The LLVM-based backends (CPU/CUDA) offer the full functionality for performing computations on spatially sparse data structures.
Using sparse data structures on the Metal backend is now deprecated. The support for Dynamic SNode has been removed in v1.3.0,
and the support for Pointer/Bitmasked SNode will be removed in v1.4.0.noteSparse matrices are usually not implemented in Taichi via spatially sparse data structures. See sparse matrix instead.Spatially sparse data structures in TaichiSpatially sparse data structures in Taichi are composed of pointer, bitmasked, dynamic, and dense SNodes. A SNode tree merely composed of dense SNodes is not a spatially sparse data structure.On a spatially sparse data structure, we consider a pixel, a voxel, or a grid node to be active if it is allocated and involved in the computation.
The rest of the grid simply becomes inactive.
In SNode terms, the activity of a leaf or intermediate cell is represented as a Boolean value. The activity value of a cell is True if and only if the cell is active. When writing to an inactive cell, Taichi automatically activates it. Taichi also provides manual manipulation of the activity of a cell: See Explicitly manipulating and querying sparsity.noteReading an inactive pixel returns zero.Pointer SNodeThe code snippet below creates an 8x8 sparse grid, with the top-level being a 4x4 pointer array (line 2 of pointer.py),
and each pointer pointing to a 2x2 dense block.
Just as you do with a dense field, you can use indices to write and read the sparse field. The following figure shows the active blocks and pixels in green.pointer.pyx = ti.field(ti.f32)block = ti.root.pointer(ti.ij, (4,4))pixel = block.dense(ti.ij, (2,2))pixel.place(x)@ti.kerneldef activate():    x[2,3] = 1.0    x[2,4] = 2.0@ti.kerneldef print_active():    for i, j in block:        print("Active block", i, j)    # output: Active block 1 1    #         Active block 1 2    for i, j in x:        print('field x[{}, {}] = {}'.format(i, j, x[i, j]))    # output: field x[2, 2] = 0.000000    #         field x[2, 3] = 1.000000    #         field x[3, 2] = 0.000000    #         field x[3, 3] = 0.000000    #         field x[2, 4] = 2.000000    #         field x[2, 5] = 0.000000    #         field x[3, 4] = 0.000000    #         field x[3, 5] = 0.000000CopyExecuting the activate() function automatically activates block[1,1], which includes x[2,3], and block[1,2], which includes x[2,4]. Other pixels of block[1,1] (x[2,2], x[3,2], x[3,3]) and block[1,2] (x[2,5], x[3,4], x[3,5]) are also implicitly activated because all pixels in the dense block share the same activity value.In fact, the sparse field is an SNode tree shown in the following figure. You can use a for loop to loop over the different levels of the SNode tree like the print_active() function in the previous example. A parallelized loop over a block for i, j in block would loop over all active pointer SNodes. A parallelized loop over a pixel for i, j in pixel would loop over all active dense SNodes.Bitmasked SNodeWhile a null pointer can effectively represent an empty sub-tree, using 64 bits to represent the activity
of a single pixel at the leaf level can consume too much space.For example, if each pixel contains a single f32 value (4 bytes),
the 64-bit pointer pointing to the value would take 8 bytes.
The fact that storage costs of pointers are higher than the space to store the value themselves
goes against our goal to use spatially sparse data structures to save space.To amortize the storage cost of pointers, you could organize pixels in a blocked manner
and let the pointers directly point to the blocks like the data structure defined in pointer.py.One caveat of this design is that pixels in the same dense block can no longer change their activity flexibly.
Instead, they share a single activity flag. To address this issue,
the bitmasked SNode additionally allocates 1-bit per pixel data to represent the pixel activity.The code snippet below illustrates this idea using a 8x8 grid. The only difference between bitmasked.py and pointer.py is that the bitmasked SNode replaces the dense SNode (line 3).bitmasked.pyx = ti.field(ti.f32)block = ti.root.pointer(ti.ij, (4,4))pixel = block.bitmasked(ti.ij, (2,2))pixel.place(x)@ti.kerneldef activate():    x[2,3] = 1.0    x[2,4] = 2.0@ti.kerneldef print_active():    for i, j in block:        print("Active block", i, j)    for i, j in x:        print('field x[{}, {}] = {}'.format(i, j, x[i, j]))CopyFurthermore, the active blocks are the same as pointer.py as shown below. However, the bitmasked pixels in the block are not all activated, because each of them has an activity value.The bitmasked SNodes are like dense SNodes with auxiliary activity values.Dynamic SNodeTaichi officially supports dynamic data structure Dynamic SNode since version v1.4.0. You can think of a dynamic SNode as a List that can only store data of a fixed type. The element types it supports include scalars, vectors/matrices, and structs. It also supports the following three APIs:append: Dynamically adds an element, equivalent to the append method of a Python list.deactivate: Clears all stored elements, equivalent to the clear method of a Python list.length: Gets the actual number of elements currently stored, equivalent to the __len__ method of a Python list.All three methods must be called inside the Taichi scope.Unfortunately, Dynamic SNode does not support dynamically deleting elements like pop and remove. This is because it is difficult to implement these operations with high performance in parallel computing.noteHere are a few rules you must obey when using Dynamic Snode:Dynamic SNode can only be used in the CPU and CUDA backends.A dynamic SNode must have one axis only, and the axis must be the last axis.No other SNodes can be placed under a dynamic SNode. In other words, a dynamic SNode must be directly placed with a field.Along the path from a dynamic SNode to the root of the SNode tree, other SNodes must not have the same axis as the dynamic SNode.For example, to declare a one-dimensional dynamic list x that stores integers, we can write:S = ti.root.dynamic(ti.i, 1024, chunk_size=32)x = ti.field(int)S.place(x)CopyLet's explain the meaning of these three lines of code:In the first line of code, ti.root.dynamic means that the direct parent node of S is ti.root. Generally, calling S = P.dynamic() for an SNode P means the direct parent node of S in the Snode tree is P. Hence this operation specifies the position of S in the SNode tree system.The first parameter of the dynamic function is the axis on which S is located. This axis must be one-dimensional and cannot have been used by any parent node of S. Here we use the axis ti.i (equivalent to axis=0 in NumPy).The second parameter of the dynamic function is the maximum length of S. Since Dynamic SNode dynamically allocates memory as needed, it does not occupy space when there is no data. However, this maximum length also has an upper limit (the maximum value of 32-bit int type), so it is not possible to assign an astronomical number to it at will. It is also possible to add elements beyond this maximum length, and elements outside the range can also be accessed normally using subscripts, but we recommend keeping the size of the list within the maximum length range.The third parameter chunk_size will be explained later in this article.After obtaining the Dynamic SNode S in this way, we declare an integer field variable x = ti.field(int), and then call S.place(x) to convert x into a data structure described by S. Before calling place, x cannot be used to store data; after calling place, x can be used as a mutable list of type int.For example, we can use the append method to add data to x, and call the length function to get the actual length of x. Both functions must be called inside the kernel:@ti.kerneldef add_data():    for i in range(1000):        x.append(i)        print(x.length())add_data()CopyWe can also call the deactivate method of x to clear the entire list, which is equivalent to restoring x to its uninitialized state:@ti.kerneldef clear_data():    x.deactivate()    print(x.length())  # will print 0CopyReturning to the explanation of the chunk_size parameter: the implementation of Dynamic SNode internally uses linked lists, where multiple elements are densely packed into a node (or "chunk") of the linked list, with each chunk containing chunk_size elements. Element allocation and deallocation are performed in units of chunks. The following diagram illustrates how x is laid out in memory (with k = 32):Thus, the actual number of chunks allocated is ceil(x.length() / chunk_size).We can also define more complex variable-length lists. For example, the following code defines an array x of length n = 10, where each element of x is a one-dimensional variable-length list:S = ti.root.dense(ti.i, 10).dynamic(ti.j, 1024, chunk_size=32)x = ti.field(int)S.place(x)CopyHere, ti.root.dense(ti.i, 10) is a Dense SNode that represents a dense array of length 10 along the ti.i axis. S = ti.root.dense(ti.i, 10).dynamic(ti.j, ...) represents a child node of this Dense SNode, occupying the ti.j axis (which is different from the parent node!). The layout of x in memory is illustrated in the following diagram:As with the one-dimensional case, you can dynamically add elements to the i-th list using x[i].append(), get the current length of the i-th list using x[i].length(), and clear the ith list using x[i].deactivate().@ti.kerneldef add_data():    for i in range(10):        for j in range(i):            x[i].append(j)        print(x[i].length())  # will print i    for i in range(10):        x[i].deactivate()        print(x[i].length())  # will print 0CopyAll of the above discussion applies to using Dynamic SNode with other numeric types. For vector/matrix and struct types, the steps are identical. For example, consider the following code using struct types:S = ti.root.dynamic(ti.i, 1024, chunk_size=32)SphereType = ti.types.struct(center=ti.math.vec3, radius=float)x = SphereType.field()S.place(x)CopyHere, x is a one-dimensional variable-length list that can store values of type SphereType.Computation on spatially sparse data structuresSparse struct-forsEfficiently looping over sparse grid cells that distribute irregularly can be challenging, especially on parallel devices such as GPUs.
In Taichi, for loops natively support spatially sparse data structures and only loop over currently active pixels with automatic efficient parallelization.Explicitly manipulating and querying sparsityTaichi also provides APIs that explicitly manipulate data structure sparsity. You can manually check the activity of a SNode, activate a SNode, or deactivate a SNode. We now illustrate these functions based on the field defined below.x = ti.field(dtype=ti.i32)block1 = ti.root.pointer(ti.ij, (3, 3))block2 = block1.pointer(ti.ij, (2, 2))pixel = block2.bitmasked(ti.ij, (2, 2))pixel.place(x)Copy1. Activity checkingYou can use ti.is_active(snode, [i, j, ...]) to explicitly query if snode[i, j, ...] is active or not.@ti.kerneldef activity_checking(snode: ti.template(), i: ti.i32, j: ti.i32):    print(ti.is_active(snode, [i, j]))for i in range(3):    for j in range(3):        activity_checking(block1, i, j)for i in range(6):    for j in range(6):        activity_checking(block2, i, j)for i in range(12):    for j in range(12):        activity_checking(pixel, i, j)Copy2. ActivationYou can use ti.activate(snode, [i, j, ...]) to explicitly activate a cell of snode[i, j, ...].@ti.kerneldef activate_snodes():    ti.activate(block1, [1, 0])    ti.activate(block2, [3, 1])    ti.activate(pixel, [7, 3])activity_checking(block1, 1, 0) # output: 1activity_checking(block2, 3, 1) # output: 1activity_checking(pixel, 7, 3)  # output: 1Copy3. DeactivationUse ti.deactivate(snode, [i, j, ...]) to explicitly deactivate a cell of snode[i, j, ...].Use snode.deactivate_all() to deactivate all cells of SNode snode. This operation also recursively deactivates all its children.Use ti.deactivate_all_snodes() to deactivate all cells of all SNodes with sparsity.When deactivation happens, the Taichi runtime automatically recycles and zero-fills memory of the deactivated containers.noteFor performance reasons, ti.activate(snode, index) only activates snode[index].
The programmer must ensure that all ancestor containers of snode[index] is already active.
Otherwise, this operation results in undefined behavior.Similarly, ti.deactivate ...does not recursively deactivate all the descendants of a cell.does not trigger deactivation of its parent container, even if all the children of the parent container are deactivated.4. Ancestor index queryYou can use ti.rescale_index(descendant_snode/field, ancestor_snode, index) to compute the ancestor index given a descendant index.print(ti.rescale_index(x, block1, ti.Vector([7, 3]))) # output: [1, 0]print(ti.rescale_index(x, block2, [7, 3]))            # output: [3, 1]print(ti.rescale_index(x, pixel,  [7, 3]))            # output: [7, 3]print(ti.rescale_index(block2, block1, [3, 1]))       # output: [1, 0]CopyRegarding line 1, you can also compute the block1 index given pixel index [7, 3] as [7//2//2, 3//2//2]. However, doing so couples computation code with the internal configuration of data structures (in this case, the size of block1 containers). By using ti.rescale_index(), you can avoid hard-coding internal information of data structures.Further readingPlease read the SIGGRAPH Asia 2019 paper or watch the associated
introduction video with slides
for more details on computation of spatially sparse data structures.Taichi elements implement a high-performance MLS-MPM solver on Taichi sparse grids.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?MotivationSpatially sparse data structures in TaichiPointer SNodeBitmasked SNodeDynamic SNodeComputation on spatially sparse data structuresSparse struct-forsExplicitly manipulating and querying sparsityFurther readingCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Coordinate Offsets | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersFieldsFields (advanced)Taichi NdarraySpatially Sparse Data StructuresCoordinate OffsetsInteracting with External ArraysDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Data Containers>>Coordinate OffsetsVersion: v1.6.0Coordinate OffsetsA Taichi field can be defined with coordinate offsets. The
offsets will move field bounds so that field origins are no longer
zero vectors. A typical use case is to support voxels with negative
coordinates in physical simulations.For example, a matrix of 32x64 elements with coordinate offset
(-16, 8) can be defined as the following:a = ti.Matrix.field(2, 2, dtype=ti.f32, shape=(32, 64), offset=(-16, 8))CopyIn this way, the field's indices are from (-16, 8) to (16, 72) (exclusive).a[-16, 8]  # lower left cornera[16, 8]   # lower right cornera[-16, 72]  # upper left cornera[16, 72]   # upper right cornerCopynoteThe dimensionality of field shapes should be consistent with that of
the offset. Otherwise, a AssertionError will be raised.a = ti.Matrix.field(2, 3, dtype=ti.f32, shape=(32,), offset=(-16, ))          # Works!b = ti.Vector.field(3, dtype=ti.f32, shape=(16, 32, 64), offset=(7, 3, -4))   # Works!# c = ti.Matrix.field(2, 1, dtype=ti.f32, shape=None, offset=(32,))           # AssertionError# d = ti.Matrix.field(3, 2, dtype=ti.f32, shape=(32, 32), offset=(-16, ))     # AssertionErrore = ti.field(dtype=ti.i32, shape=16, offset=-16)                              # Works!# f = ti.field(dtype=ti.i32, shape=None, offset=-16)                          # AssertionError# g = ti.field(dtype=ti.i32, shape=(16, 32), offset=-16)                      # AssertionErrorCopyEdit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Differentiable Programming | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Differentiable Programming>>Differentiable ProgrammingVersion: v1.6.0On this pageDifferentiable ProgrammingIntroductionDifferentiable programming proves to be useful in a wide variety of areas
such as scientific computing and artificial intelligence. For instance,
a controller optimization system equipped with differentiable simulators converges one to
four orders of magnitude faster than those using model-free reinforcement learning algorithms.12Suppose you have the following kernel:x = ti.field(float, ())y = ti.field(float, ())@ti.kerneldef compute_y():    y[None] = ti.sin(x[None])CopyNow if you want to get the derivative of y with respect to x:
dy/dx, it is straightforward to write out the gradient kernel manually:x = ti.field(dtype=ti.f32, shape=())y = ti.field(dtype=ti.f32, shape=())dy_dx = ti.field(dtype=ti.f32, shape=())@ti.kerneldef compute_dy_dx():    dy_dx[None] = ti.cos(x[None])CopyHowever, as you make a change to compute_y, you have to rework the gradient formula
by hand and update compute_dy_dx accordingly. Apparently, when
the kernel becomes larger and gets frequently updated, this manual workflow is
really error-prone and hard to maintain.If you run into this situation, Taichi's handy automatic differentiation (autodiff)
system comes to your rescue! Taichi supports gradient evaluation through
either ti.ad.Tape() or the more flexible kernel.grad() syntax.Using ti.ad.Tape()Let's still take the compute_y kernel above for an explanation.
Using ti.ad.Tape() is the easiest way to obtain a kernel that computes dy/dx:Enable needs_grad=True option when declaring fields involved in
the derivative chain.Use context manager with ti.ad.Tape(y): to capture the kernel invocations which you want to automatically differentiate.Now dy/dx value at current x is available at x.grad[None].The following code snippet explains the steps above:x = ti.field(dtype=ti.f32, shape=(), needs_grad=True)y = ti.field(dtype=ti.f32, shape=(), needs_grad=True)@ti.kerneldef compute_y():    y[None] = ti.sin(x[None])with ti.ad.Tape(y):    compute_y()print('dy/dx =', x.grad[None], ' at x =', x[None])CopyCase study: gravity simulationA common problem in physical simulation is that it is usually easy to compute
energy but hard to compute force on every particle,
for example Bond bending (and torsion) in molecular dynamics
and FEM with hyperelastic energy functions.
Recall that we can differentiate
(negative) potential energy to get forces: F_i = -dU / dx_i. So once you have coded
a kernel that computes the potential energy, you may use Taichi's autodiff
system to obtain the derivatives and then F_i on each particle.Taking
examples/simulation/ad_gravity.py
as an example:import taichi as titi.init()N = 8dt = 1e-5x = ti.Vector.field(2, dtype=ti.f32, shape=N, needs_grad=True)  # particle positionsv = ti.Vector.field(2, dtype=ti.f32, shape=N)  # particle velocitiesU = ti.field(dtype=ti.f32, shape=(), needs_grad=True)  # potential energy@ti.kerneldef compute_U():    for i, j in ti.ndrange(N, N):        r = x[i] - x[j]        # r.norm(1e-3) is equivalent to ti.sqrt(r.norm()**2 + 1e-3)        # This is to prevent 1/0 error which can cause wrong derivative        U[None] += -1 / r.norm(1e-3)  # U += -1 / |r|@ti.kerneldef advance():    for i in x:        v[i] += dt * -x.grad[i]  # dv/dt = -dU/dx    for i in x:        x[i] += dt * v[i]  # dx/dt = vdef substep():    with ti.ad.Tape(loss=U):        # Kernel invocations in this scope will later contribute to partial derivatives of        # U with respect to input variables such as x.        compute_U(        )  # The tape will automatically compute dU/dx and save the results in x.grad    advance()@ti.kerneldef init():    for i in x:        x[i] = [ti.random(), ti.random()]init()gui = ti.GUI('Autodiff gravity')while gui.running:    for i in range(50):        substep()    gui.circles(x.to_numpy(), radius=3)    gui.show()CopynoteThe argument U to ti.ad.Tape(U) must be a 0D field.To use autodiff with multiple output variables, see the
kernel.grad() usage below.noteti.ad.Tape(U) automatically sets U[None] to 0 on
start up.tipSee
examples/simulation/mpm_lagrangian_forces.py
and
examples/simulation/fem99.py
for examples on using autodiff-based force evaluation MPM and FEM.Using kernel.grad()As mentioned above, ti.ad.Tape() can only track a 0D field as the output variable.
If there are multiple output variables that you want to back-propagate
gradients to inputs, call kernel.grad() instead of ti.ad.Tape().
Different from using ti.ad.Tape(), you need to set the grad of the output variables themselves to 1 manually
before calling kernel.grad(). The reason is that the grad of the output variables themselves
will always be multiplied to the grad with respect to the inputs at the end of the back-propagation.
By calling ti.ad.Tape(), you have the program do this under the hood.import taichi as titi.init()N = 16x = ti.field(dtype=ti.f32, shape=N, needs_grad=True)loss = ti.field(dtype=ti.f32, shape=(), needs_grad=True)loss2 = ti.field(dtype=ti.f32, shape=(), needs_grad=True)@ti.kerneldef func():    for i in x:       loss[None] += x[i] ** 2       loss2[None] += x[i]for i in range(N):    x[i] = i# Set the `grad` of the output variables to `1` before calling `func.grad()`.loss.grad[None] = 1loss2.grad[None] = 1func()func.grad()for i in range(N):    assert x.grad[i] == i * 2 + 1CopytipIt may be tedius to write out need_grad=True for every input in a complicated use case.
Alternatively, Taichi provides an API ti.root.lazy_grad() that automatically places the
gradient fields following the layout of their primal fields.cautionWhen using kernel.grad(), it is recommended that you always run forward kernel before backward, for example kernel(); kernel.grad(). If global fields used in the derivative calculation get mutated in the forward run, skipping
kernel() breaks global data access rule #1 below and may produce incorrect gradients.Limitations of Taichi autodiff systemUnlike tools such as TensorFlow where immutable output buffers are
generated, the imperative programming paradigm adopted by Taichi
allows programmers to freely modify global fields.To make automatic differentiation well-defined under this setting, the following
rules are enforced when writing differentiable programs in Taichi:Global Data Access RulesCurrently Taichi's autodiff implementation does not save intermediate results of global fields which might be used in the backward pass. Therefore mutation is forbidden once you've read from a global field.Global Data Access Rule #1Once you read an element in a field, the element cannot be mutated anymore.import taichi as titi.init()N = 16x = ti.field(dtype=ti.f32, shape=N, needs_grad=True)loss = ti.field(dtype=ti.f32, shape=(), needs_grad=True)b = ti.field(dtype=ti.f32, shape=(), needs_grad=True)@ti.kerneldef func_broke_rule_1():    # BAD: broke global data access rule #1, reading global field and before mutation is done.    loss[None] = x[1] * b[None]    b[None] += 100@ti.kerneldef func_equivalent():    loss[None] = x[1] * 10for i in range(N):    x[i] = ib[None] = 10loss.grad[None] = 1with ti.ad.Tape(loss):    func_broke_rule_1()# Call func_equivalent to see the correct result# with ti.ad.Tape(loss):    # func_equivalent()assert x.grad[1] == 10.0CopyGlobal Data Access Rule #2If a global field element is written more than once, then starting from the second write, the write must come in the form of an atomic add ("accumulation", using ti.atomic_add or simply +=). Although += violates rule #1 above since it reads the old value before computing the sum, it is the only special case of "read before mutation" that Taichi allows in the autodiff system.import taichi as titi.init()N = 16x = ti.field(dtype=ti.f32, shape=N, needs_grad=True)loss = ti.field(dtype=ti.f32, shape=(), needs_grad=True)@ti.kerneldef func_break_rule_2():    loss[None] += x[1] ** 2    # Bad: broke global data access rule #2, it's not an atomic_add.    loss[None] *= x[2]@ti.kerneldef func_equivalent():    loss[None] = (2 + x[1] ** 2) * x[2]for i in range(N):    x[i] = iloss.grad[None] = 1loss[None] = 2func_break_rule_2()func_break_rule_2.grad()# Call func_equivalent to see the correct result# func_equivalent()# func_equivalent.grad()assert x.grad[1] == 4.0assert x.grad[2] == 3.0CopyGlobal data access rule violation checkerA checker is provided for detecting potential violations of global data access rules.The checker only works in debug mode. To enable it, set debug=True when calling ti.init().Set validation=True when using ti.ad.Tape() to validate the kernels captured by ti.ad.Tape().
The checker pinpoints the line of code breaking the rules, if a violation occurs.For example:import taichi as titi.init(debug=True)N = 5x = ti.field(dtype=ti.f32, shape=N, needs_grad=True)loss = ti.field(dtype=ti.f32, shape=(), needs_grad=True)b = ti.field(dtype=ti.f32, shape=(), needs_grad=True)@ti.kerneldef func_1():    for i in range(N):        loss[None] += x[i] * b[None]@ti.kerneldef func_2():    b[None] += 100b[None] = 10with ti.ad.Tape(loss, validation=True):    func_1()    func_2()"""taichi.lang.exception.TaichiAssertionError:(kernel=func_2_c78_0) Breaks the global data access rule. Snode S10 is overwritten unexpectedly.File "across_kernel.py", line 16, in func_2:    b[None] += 100    ^^^^^^^^^^^^^^"""CopyAvoid mixed usage of parallel for-loop and non-for statementsMixed usage of parallel for-loops and non-for statements are not supported in the autodiff system.
Please split the two kinds of statements into different kernels.noteKernel body must only consist of either multiple for-loops or non-for statements.Example:@ti.kerneldef differentiable_task():    # Bad: mixed usage of a parallel for-loop and a statement without looping. Please split them into two kernels.    loss[None] += x[0]    for i in range(10):        ...CopyViolation of this rule results in an error.DANGERViolation of rules above might result in incorrect gradient result without a proper error.
We're actively working on improving the error reporting mechanism for it. Please feel free
to open a github issue
if you see any silent wrong results.Write differentiable code inside a Taichi kernelTaichi's compiler only captures the code in the Taichi scope when performing the source code transformation for autodiff. Therefore, only the code written in Taichi scope is auto-differentiated. Although you can modify the grad of a field in python scope manually, the code is not auto-differentiated.Example:import taichi as titi.init()x = ti.field(dtype=float, shape=(), needs_grad=True)loss = ti.field(dtype=float, shape=(), needs_grad=True)@ti.kerneldef differentiable_task():    for l in range(3):        loss[None] += ti.sin(x[None]) + 1.0@ti.kerneldef manipulation_in_kernel():    loss[None] += ti.sin(x[None]) + 1.0x[None] = 0.0with ti.ad.Tape(loss=loss):    # The line below in python scope only contribute to the forward pass    # but not the backward pass i.e., not auto-differentiated.    loss[None] += ti.sin(x[None]) + 1.0    # Code in Taichi scope i.e. inside Taichi kernels, is auto-differentiated.    manipulation_in_kernel()    differentiable_task()# The outputs are 5.0 and 4.0print(loss[None], x.grad[None])# You can modify the grad of a field manually in python scope, e.g., clear the grad.x.grad[None] = 0.0# The output is 0.0print(x.grad[None])CopyExtending Taichi Autodiff systemSometimes user may want to override the gradients provided by the Taichi autodiff system. For example, when differentiating a 3D singular value decomposition (SVD) used in an iterative
solver, it is preferred to use a manually engineered SVD derivative subroutine for better numerical stability.
Taichi provides two decorators ti.ad.grad_replaced and ti.ad.grad_for to overwrite the default
automatic differentiation behavior.The following is a simple example to use customized gradient function in autodiff:import taichi as titi.init()x = ti.field(ti.f32)total = ti.field(ti.f32)n = 128ti.root.dense(ti.i, n).place(x)ti.root.place(total)ti.root.lazy_grad()@ti.kerneldef func(mul: ti.f32):    for i in range(n):        ti.atomic_add(total[None], x[i] * mul)@ti.ad.grad_replaceddef forward(mul):    func(mul)    func(mul)@ti.ad.grad_for(forward)def backward(mul):    func.grad(mul)with ti.ad.Tape(loss=total):    forward(4)assert x.grad[0] == 4CopyCustomized gradient function works with both ti.ad.Tape() and kernel.grad(). More examples can be found at test_customized_grad.py.CheckpointingAnother use case of customized gradient function is checkpointing. We can use recomputation to save memory space through
a user-defined gradient function.
diffmpm.py
demonstrates that by defining a customized gradient function that recomputes the grid states during backward,
we can reuse the grid states and allocate only one copy compared to O(n) copies in a native implementation
without customized gradient function.DiffTaichiThe DiffTaichi repo
contains 10 differentiable physical simulators built with Taichi
differentiable programming. A few examples with neural network
controllers optimized using differentiable simulators and brute-force
gradient descent:tipCheck out the DiffTaichi paper
and video to learn more
about Taichi differentiable programming.Forward-Mode AutodiffAutomatic differentiation (Autodiff) has two modes, reverse mode and forward mode.Reverse mode computes Vector-Jacobian Product (VJP), which means computing one row of the Jacobian matrix at a time. Therefore, reverse mode is more efficient for functions, which have more inputs than outputs. ti.ad.Tape() and kernel.grad() are for reverse-mode autodiff.Forward mode computes Jacobian-Vector Product (JVP), which means computing one column of the Jacobian matrix at a time. Therefore, forward mode is more efficient for functions, which have more outputs than inputs. As of v1.1.0, Taichi supports forward-mode autodiff. ti.ad.FwdMode() and ti.root.lazy_dual() are for forward-mode autodiff.Using ti.ad.FwdMode()The usage of ti.ad.FwdMode() is similar to that of ti.ad.Tape(). Here we reuse the example for reverse mode above for ti.ad.FwdMode().Set needs_dual=True when declaring fields involved in a derivative chain.The dual here indicates dual number in math. This is because forward-mode autodiff is equivalent to evaluating a function with dual numbers.Use context manager with ti.ad.FwdMode(loss=y, param=x) to capture the kernel invocations to automatically differentiate.Now dy/dx value at the current x is available at function output y.dual[None].The following code snippet explains the steps above:import taichi as titi.init()x = ti.field(dtype=ti.f32, shape=(), needs_dual=True)y = ti.field(dtype=ti.f32, shape=(), needs_dual=True)@ti.kerneldef compute_y():    y[None] = ti.sin(x[None])# `loss`: The function's output# `param`: The input of the functionwith ti.ad.FwdMode(loss=y, param=x):    compute_y()print('dy/dx =', y.dual[None], ' at x =', x[None])Copynoteti.ad.FwdMode() automatically clears the dual field of loss.ti.ad.FwdMode() supports multiple inputs and outputs:param can be an N-D field.loss can be an individual N-D field or a list of N-D fields.seed is the 'vector' in Jacobian-vector product, which controls the parameter that is computed derivative with respect to. seed is required if param is not a scalar field.The following code snippet shows another two cases with multiple inputs and outputs: With seed=[1.0, 0.0] or seed=[0.0, 1.0] , we can compute derivatives solely with respect to x_0 or x_1.import taichi as titi.init()N_param = 2N_loss = 5x = ti.field(dtype=ti.f32, shape=N_param, needs_dual=True)y = ti.field(dtype=ti.f32, shape=N_loss, needs_dual=True)@ti.kerneldef compute_y():    for i in range(N_loss):        for j in range(N_param):            y[i] += i * ti.sin(x[j])# Compute derivatives with respect to x_0# `seed` is required if `param` is not a scalar fieldwith ti.ad.FwdMode(loss=y, param=x, seed=[1.0, 0.0]):    compute_y()print('dy/dx_0 =', y.dual, ' at x_0 =', x[0])# Compute derivatives with respect to x_1# `seed` is required if `param` is not a scalar fieldwith ti.ad.FwdMode(loss=y, param=x, seed=[0.0, 1.0]):    compute_y()print('dy/dx_1 =', y.dual, ' at x_1 =', x[1])CopytipJust as reverse-mode autodiff, Taichi's forward-mode autodiff provides ti.root.lazy_dual(), which automatically places the dual fields following the layout of their primal fields.End-to-End Differentiable Physics for Learning and Control
↩ChainQueen: A Real-Time Differentiable Physical Simulator for Soft Robotics↩Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?IntroductionUsing ti.ad.Tape()Case study: gravity simulationUsing kernel.grad()Limitations of Taichi autodiff systemGlobal Data Access RulesGlobal data access rule violation checkerAvoid mixed usage of parallel for-loop and non-for statementsWrite differentiable code inside a Taichi kernelExtending Taichi Autodiff systemCheckpointingDiffTaichiForward-Mode AutodiffUsing ti.ad.FwdMode()Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Metaprogramming | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingMetaprogrammingObjective Data-Oriented ProgrammingData-Oriented ClassTaichi DataclassUse quantized data typesVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Advanced Programming>>MetaprogrammingVersion: v1.6.0On this pageMetaprogrammingTaichi provides metaprogramming infrastructures. There are many benefits of metaprogramming in Taichi:Enabling the development of dimensionality-independent code, e.g., code which is
adaptive for both 2D/3D physical simulations.Improving runtime performance by moving computations from runtime to compile time.Simplifying the development of Taichi standard library.noteTaichi kernels are lazily instantiated and large amounts of computation can be executed at compile-time.
Every kernel in Taichi is a template kernel, even if it has no template arguments.Template metaprogrammingBy using ti.template() as an argument type hint, a Taichi field or a python object can be passed into a kernel. Template programming also enables the code to be reused for fields with different shapes:@ti.kerneldef copy_1D(x: ti.template(), y: ti.template()):    for i in x:        y[i] = x[i]a = ti.field(ti.f32, 4)b = ti.field(ti.f32, 4)c = ti.field(ti.f32, 12)d = ti.field(ti.f32, 12)# Pass field a and b as arguments of the kernel `copy_1D`:copy_1D(a, b)# Reuse the kernel for field c and d:copy_1D(c, d)CopynoteIf a template parameter is not a Taichi object, it cannot be reassigned inside Taichi kernel.noteThe template parameters are inlined into the generated kernel after compilation.Dimensionality-independent programming using grouped indicesTaichi provides ti.grouped syntax which supports grouping loop indices into a ti.Vector.
It enables dimensionality-independent programming, i.e., code are adaptive to scenarios of
different dimensionalities automatically:@ti.kerneldef copy_1D(x: ti.template(), y: ti.template()):    for i in x:        y[i] = x[i]@ti.kerneldef copy_2d(x: ti.template(), y: ti.template()):    for i, j in x:        y[i, j] = x[i, j]@ti.kerneldef copy_3d(x: ti.template(), y: ti.template()):    for i, j, k in x:        y[i, j, k] = x[i, j, k]# Kernels listed above can be unified into one kernel using `ti.grouped`:@ti.kerneldef copy(x: ti.template(), y: ti.template()):    for I in ti.grouped(y):        # I is a vector with dimensionality same to y        # If y is 0D, then I = ti.Vector([]), which is equivalent to `None` used in x[I]        # If y is 1D, then I = ti.Vector([i])        # If y is 2D, then I = ti.Vector([i, j])        # If y is 3D, then I = ti.Vector([i, j, k])        # ...        x[I] = y[I]CopyField metadataThe two attributes data type and shape of fields can be accessed by field.dtype and  field.shape, in both Taichi-scope and Python-scope:x = ti.field(dtype=ti.f32, shape=(3, 3))# Print field metadata in Python-scopeprint("Field dimensionality is ", x.shape)print("Field data type is ", x.dtype)# Print field metadata in Taichi-scope@ti.kerneldef print_field_metadata(x: ti.template()):    print("Field dimensionality is ", len(x.shape))    for i in ti.static(range(len(x.shape))):        print("Size along dimension ", i, "is", x.shape[i])    ti.static_print("Field data type is ", x.dtype)CopynoteFor sparse fields, the full domain shape will be returned.Matrix & vector metadataFor matrices, matrix.m and matrix.n returns the number of columns and rows, respectively.
For vectors, they are treated as matrices with one column in Taichi, where vector.n is the number of elements of the vector.@ti.kerneldef foo():    matrix = ti.Matrix([[1, 2], [3, 4], [5, 6]])    print(matrix.n)  # number of row: 3    print(matrix.m)  # number of column: 2    vector = ti.Vector([7, 8, 9])    print(vector.n)  # number of elements: 3    print(vector.m)  # always equals to 1 for a vectorCopyCompile-time evaluationsUsing compile-time evaluation allows for some computation to be executed when kernels are instantiated. This helps the compiler to conduct optimization and reduce
computational overhead at runtime:Static Scopeti.static is a function which receives one argument. It is a hint for the compiler to evaluate the argument at compile time.
The scope of the argument of ti.static is called static-scope.Compile-time branchingUse ti.static for compile-time branching (for those who are familiar with
C++17, this is similar to if
constexpr.):enable_projection = True@ti.kerneldef static():  if ti.static(enable_projection): # No runtime overhead    x[0] = 1CopynoteOne of the two branches of the static if will be discarded after compilation.Loop unrollingUse ti.static for forced loop unrolling:@ti.kerneldef func():  for i in ti.static(range(4)):      print(i)  # The code snippet above is equivalent to:  print(0)  print(1)  print(2)  print(3)CopynoteBefore v1.4.0, indices for accessing Taichi matrices/vectors must be compile-time constants.
Therefore, if the indices come from a loop, the loop must be unrolled:# Here we declare a field containing 3 vectors. Each vector contains 8 elements.x = ti.Vector.field(8, ti.f32, shape=3)@ti.kerneldef reset():    for i in x:        for j in ti.static(range(x.n)):            # The inner loop must be unrolled since j is an index for accessing a vector.            x[i][j] = 0CopyStarting from v1.4.0, indices for accessing Taichi matrices/vectors can be runtime variables.
Therefore, the loop above is no longer required to be unrolled.
That said, unrolling it will still help you reduce runtime overhead.Compile-time recursion of ti.funcA compile-time recursive function is a function with recursion that can be recursively inlined at compile time. The condition which determines whether to recurse is evaluated at compile time.You can combine compile-time branching and template to write compile-time recursive functions.For example, sum_from_one_to is a compile-time recursive function that calculates the sum of numbers from 1 to n.@ti.funcdef sum_from_one_to(n: ti.template()) -> ti.i32:    ret = 0    if ti.static(n > 0):        ret = n + sum_from_one_to(n - 1)    return ret@ti.kerneldef sum_from_one_to_ten():    print(sum_from_one_to(10))  # prints 55CopyWARNINGWhen the recursion is too deep, it is not recommended to use compile-time recursion because deeper compile-time recursion expands to longer code during compilation, resulting in increased compilation time.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Template metaprogrammingDimensionality-independent programming using grouped indicesField metadataMatrix & vector metadataCompile-time evaluationsStatic ScopeCompile-time branchingLoop unrollingCompile-time recursion of ti.funcCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Objective Data-Oriented Programming | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingMetaprogrammingObjective Data-Oriented ProgrammingData-Oriented ClassTaichi DataclassUse quantized data typesVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Advanced Programming>>Objective Data-Oriented ProgrammingVersion: v1.6.0Objective Data-Oriented ProgrammingTaichi is a Data-Oriented Programming (DOP) language. However, one-size-fits-all DOP makes modularization hard. To allow modularized code, Taichi borrows some concepts from Object-Oriented Programming (OOP). For convenience, let's call the hybrid scheme Objective Data-Oriented Programming (ODOP).noteDOP approaches coding in a unique way. While you may be familiar with OOP, the Data-Oriented design indicates that everything is data that can be acted on. This differentiates functionality from data. They are no longer linked by a set of rules. Your DOP routines are general-purpose and deal with enormous volumes of data. To guarantee that the function takes as little effort as possible, you should organize the data as close to the output data as possible.The ODOP scheme allows you to organize data and methods in a class and call the methods to manipulate the data in the Taichi scope. Taichi offers two different types of classes that serve this purpose, and they are distinguished by the two decorators @ti.data_oriented and @ti.dataclass respectively:Decorated with @ti.data_oriented, a Data-Oriented class is used when your data is actively updated in the Python scope (such as current time and user input events) and tracked in Taichi kernels. This type of class can have native Python objects as members and must be instantiated in the Python scope. Data-Oriented Class describes this type of class.Decorated with @ti.dataclass, a dataclass is a wrapper of ti.types.struct. A dataclass provides more flexibilities. You can define Taichi functions as its methods and call these methods in the Taichi scope. Taichi Dataclass describes this type of class.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Data-Oriented Class | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingMetaprogrammingObjective Data-Oriented ProgrammingData-Oriented ClassTaichi DataclassUse quantized data typesVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Advanced Programming>>Data-Oriented ClassVersion: v1.6.0On this pageData-Oriented ClassTo define a Taichi kernel as a Python class member function:Decorate the class with a @ti.data_oriented decorator.Define ti.kernels and ti.funcs in your Data-Oriented Python class.noteThe first argument of the function should be the class instance ("self"), unless you are defining a @staticmethod.A brief example. Notice the use of @ti.data_oriented and @ti.kernel in lines 1 and 6, respectively:@ti.data_orientedclass TiArray:    def __init__(self, n):        self.x = ti.field(dtype=ti.i32, shape=n)    @ti.kernel    def inc(self):        for i in self.x:            self.x[i] += 1a = TiArray(32)a.inc()CopyDefinitions of Taichi fields can be made not only in init functions, but also at any place of a Python-scope function in a Data-Oriented class.import taichi as titi.init()@ti.data_orientedclass MyClass:    @ti.kernel    def inc(self, temp: ti.template()):        #increment all elements in array by 1        for I in ti.grouped(temp):            temp[I] += 1    def call_inc(self):        self.inc(self.temp)    def allocate_temp(self, n):        self.temp = ti.field(dtype = ti.i32, shape=n)a = MyClass() # creating an instance of Data-Oriented Class# a.call_inc() cannot be called, because a.temp has not been allocated at this pointa.allocate_temp(4) # [0 0 0 0]a.call_inc() # [1 1 1 1]a.call_inc() # [2 2 2 2]print(a.temp)  # will print [2 2 2 2]a.allocate_temp(8) # [0 0 0 0 0 0 0 0 0]a.call_inc() # [1 1 1 1 1 1 1 1]print(a.temp)  # will print [1 1 1 1 1 1 1 1]CopyAnother memory recycling example:import taichi as titi.init()@ti.data_orientedclass Calc:    def __init__(self):        self.x = ti.field(dtype=ti.f32, shape=16)        self.y = ti.field(dtype=ti.f32, shape=4)    @ti.kernel    def func(self, temp: ti.template()):        for i in range(8):            temp[i] = self.x[i * 2] + self.x[i * 2 + 1]        for i in range(4):            self.y[i] = ti.max(temp[i * 2], temp[i * 2 + 1])    def call_func(self):        fb = ti.FieldsBuilder()        temp = ti.field(dtype=ti.f32)        fb.dense(ti.i, 8).place(temp)        tree = fb.finalize()        self.func(temp)        tree.destroy()a = Calc()for i in range(16):    a.x[i] = ia.call_func()print(a.y)  # [ 5. 13. 21. 29.]CopyTo know more about FieldsBuilder, please refer to FieldsBuilder.Inheritance of Data-Oriented classesThe Data-Oriented property is automatically carried along with the Python class inheritence. This implies that you can call a Taichi Kernel if any of its ancestor classes is decorated with @ti.data_oriented, which is shown in the example below:An example:import taichi as titi.init(arch=ti.cuda)class BaseClass:    def __init__(self):        self.n = 10        self.num = ti.field(dtype=ti.i32, shape=(self.n, ))    @ti.kernel    def sum(self) -> ti.i32:        ret = 0        for i in range(self.n):            ret += self.num[i]        return ret    @ti.kernel    def add(self, d: ti.i32):        for i in range(self.n):            self.num[i] += d@ti.data_orientedclass DataOrientedClass(BaseClass):    passclass DeviatedClass(DataOrientedClass):    @ti.kernel    def sub(self, d: ti.i32):        for i in range(self.n):            self.num[i] -= da = DeviatedClass()a.add(1)a.sub(1)print(a.sum())  # 0b = DataOrientedClass()b.add(2)print(b.sum())  # 20c = BaseClass()# c.add(3)# print(c.sum())# The two lines above trigger a kernel define error, because class c is not decorated with @ti.data_orientedCopyPython built-in decoratorsCommon decorators that are pre-built in Python, @staticmethod1 and @classmethod2, can decorate a Taichi kernel in Data-Oriented classes.staticmethod example:import taichi as titi.init()@ti.data_orientedclass Array2D:    def __init__(self, n):        self.arr = ti.Vector([0.] * n)    @staticmethod    @ti.func    def clamp(x):  # Clamp to [0, 1)        return max(0, min(1, x))Copyclassmethod example:import taichi as titi.init(arch=ti.cuda)@ti.data_orientedclass Counter:    num_ = ti.field(dtype=ti.i32, shape=(32, ))    def __init__(self, data_range):        self.range = data_range        self.add(data_range[0], data_range[1], 1)    @classmethod    @ti.kernel    def add(cls, l: ti.i32, r: ti.i32, d: ti.i32):        for i in range(l, r):            cls.num_[i] += d    @ti.kernel    def num(self) -> ti.i32:        ret = 0        for i in range(self.range[0], self.range[1]):            ret += self.num_[i]        return reta = Counter((0, 5))print(a.num())  # 5b = Counter((4, 10))print(a.num())  # 6print(b.num())  # 7CopyPython built-in functions - staticmethod↩Python built-in functions - classmethod↩Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Inheritance of Data-Oriented classesPython built-in decoratorsCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Taichi Dataclass | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingMetaprogrammingObjective Data-Oriented ProgrammingData-Oriented ClassTaichi DataclassUse quantized data typesVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Advanced Programming>>Taichi DataclassVersion: v1.6.0On this pageTaichi DataclassTaichi provides custom struct types for developers to assemble pieces of data together. However, it would be more convenient to have:A Python representation of the struct type which is more Object-Oriented.Functions associated with a struct type.To achieve the ends, Taichi enabled the @ti.dataclass decorator on a Python class. This is inspired by Python's dataclass feature, which uses class fields with annotations to create data types.Create a struct from a Python classThe following is an example of defining a Taichi struct type under a Python class:vec3 = ti.math.vec3@ti.dataclassclass Sphere:    center: vec3    radius: ti.f32CopyThis is the same equivalent as using ti.types.struct():Sphere = ti.types.struct(center=vec3, radius=ti.f32)CopyThe @ti.dataclass decorator converts the annotated members in the Python class to members in the resulting struct type. In both of the above examples, you end up with the same struct field.Associate Functions with the struct typeBoth Python classes and Taichi struct types can have functions attached to them. Building from the above example, one can embed functions in the struct as follows:vec3 = ti.math.vec3@ti.dataclassclass Sphere:    center: vec3    radius: ti.f32    @ti.func    def area(self):        # a function to run in taichi scope        return 4 * math.pi * self.radius * self.radius    def is_zero_sized(self):        # a python scope function        return self.radius == 0.0CopyFunctions associated with structs follow the same scope rules as other functions. In other words, they can be placed in either the Taichi scope or the Python scope. Each instance of the Sphere struct type now have the above functions attached to them. The functions can be called in the following way:a_python_struct = Sphere(center=ti.math.vec3(0.0), radius=1.0)# calls a python scope function from pythona_python_struct.is_zero_sized() # False@ti.kerneldef get_area() -> ti.f32:    a_taichi_struct = Sphere(center=ti.math.vec3(0.0), radius=4.0)    # return the area of the sphere, a taichi scope function    return a_taichi_struct.area()get_area() # 201.062...CopyNotesInheritance of Taichi dataclasses is not supported.While it is convenient and recommended to associate functions with a struct defined via @ti.dataclass, ti.types.struct can serve the same purpose with the help of the __struct_methods argument. As mentioned above, the two methods of defining a struct type produce identical output.@ti.funcdef area(self):    # a function to run in taichi scope    return 4 * math.pi * self.radius * self.radiusSphere = ti.types.struct(center=ti.math.vec3, radius=ti.f32,                         __struct_methods={'area': area})CopyEdit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Create a struct from a Python classAssociate Functions with the struct typeNotesCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Use quantized data types | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingMetaprogrammingObjective Data-Oriented ProgrammingData-Oriented ClassTaichi DataclassUse quantized data typesVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Advanced Programming>>Use quantized data typesVersion: v1.6.0On this pageUse quantized data typesHigh-resolution simulations can deliver great visual quality, but are often limited by the capacity of the onboard memory, GPU memory in particular.To help reduce the memory footprint of your programs, Taichi provides quantized data types, aka low-precision data types. It allows you to define your own integers, fixed-point numbers, or floating-point numbers with arbitrary number of bits that work best with your limited memory capacity. At the same time, Taichi provides a suite of tailored optimizations to ensure that the runtime performance with quantized data types is comparable to the performance with full-precision data types.noteFor now, quantized data types are supported only on the CPU and CUDA backends.Quantized data typesTaichi supports the following quantized data types:Quantized integersQuantized fixed-point numbersQuantized floating-point numbersQuantized integersQuantized integers in Taichi are represented in the two's complement format but can contain arbitrary number of bits.To define a 10-bit signed integer type:i10 = ti.types.quant.int(bits=10)  # `signed` is set to `True` by defaultCopyTo define a 5-bit unsigned integer type:u5 = ti.types.quant.int(bits=5, signed=False)CopyQuantized fixed-point numbersThe core idea of fixed-point numbers is that, if a specific range is evenly divided into multiple scale units, then a real number within that range can be approximated and represented by multiplying the value of each scale unit by an integer number. Here's an example explaining what the scale unit here is: If you wish to represent a real number within [0, 100] in 10 binary bits, then each scale unit equals 100/210 ≈ 0.098.Taichi allows you to define quantized fixed-point types of less than 64 bits and with an arbitrary scale unit.To define a 10-bit signed fixed-point type within the range [-20.0, 20.0]:fixed_type_a = ti.types.quant.fixed(bits=10, max_value=20.0)  # `signed` is set to `True` by defaultCopyTo define a 5-bit unsigned fixed-point type within the range [0.0, 100.0]:fixed_type_b = ti.types.quant.fixed(bits=5, signed=False, max_value=100.0)CopyTo define a 6-bit unsigned fixed-point type within [0.0, 64.0]:fixed_type_c = ti.types.quant.fixed(bits=6, signed=False, scale=1.0)  # `scale` is a predefined scaling factorCopySet either scale or max_value, and Taichi works out the other based on your setting. Do not set both.
max_value is a more commonly used parameter, because you may already know the range of the number to represent.Quantized floating-point numbersA floating-point number comprises exponent bits, fraction bits, and a sign bit. There are various floating-point formats:Taichi allows you to define a quantized floating-point number with an arbitrary combination of exponent bits and fraction bits (the sign bit is made part of the fraction bits).To define a 15-bit signed floating-point type with five exponent bits:float_type_a = ti.types.quant.float(exp=5, frac=10)  # `signed` is set to `True` by defaultCopyTo define a 15-bit unsigned floating-point type with six exponent bits:float_type_b = ti.types.quant.float(exp=6, frac=9, signed=False)CopyCompute typesAll the above-mentioned parameters specify how a quantized data type is stored in your computer. However, most quantized data types have no native support on hardware, so an actual value of that quantized data type needs to be converted to a primitive type ("compute type") during computation.The default compute type for quantized integers is ti.i32,The default compute type for quantized fixed-point numbers is ti.f32,The default compute type for quantized floating-point numbers is ti.f32.To change the compute type of a quantized data type,  set the compute parameter when defining the quantized data type:i21 = ti.types.quant.int(bits=21, compute=ti.i64)bfloat16 = ti.types.quant.float(exp=8, frac=8, compute=ti.f32)CopyData containers for quantized data typesQuantized data types are not primitive types and hence require the following constructs to work with Taichi's data containers.Bitpacked fieldsQuant arraysBitpacked fieldsti.BitpackedFields packs a group of fields whose dtypes are
quantized data types together so that they are stored with one primitive type.
You can then place a ti.BitpackedFields instance under any SNode as if each member field
is placed individually.a = ti.field(float_type_a)  # 15 bitsb = ti.field(fixed_type_b)  # 5 bitsc = ti.field(fixed_type_c)  # 6 bitsd = ti.field(u5)  # 5 bitsbitpack = ti.BitpackedFields(max_num_bits=32)bitpack.place(a, b, c, d)  # 31 out of 32 bits occupiedti.root.dense(ti.i, 10).place(bitpack)CopyShared exponentWhen multiple fields with quantized floating-point types are packed together,
there is chance that they can share a common exponent. For example, in a 3D
velocity vector, if you know the x-component has a much larger absolute value
compared to y- and z-components, then you probably do not care about the exact
value of the y- and z-components. In this case, using a shared exponent can
leave more bits for components with larger absolute values. You can use
place(x, y, z, shared_exponent=True) to make fields x, y, z share a common
exponent.Your first programYou probably cannot wait to write your first Taichi program with quantized data
types. The easiest way is to modify the data definitions of an existing example.
Assume you want to save memory for
examples/simulation/euler.py.
Because most data definitions in the example are similar, here only field Q is
used for illustration:N = 16Q = ti.Vector.field(4, dtype=ti.f32, shape=(N, N))CopyAn element of Q now occupies 4 x 32 = 128 bits. If you can fit it in
64 bits, then the memory usage is halved. A direct and first attempt is to
use quantized floating-point numbers with a shared exponent:float_type_c = ti.types.quant.float(exp=8, frac=14)Q_old = ti.Vector.field(4, dtype=float_type_c)bitpack = ti.BitpackedFields(max_num_bits=64)bitpack.place(Q_old, shared_exponent=True)ti.root.dense(ti.ij, (N, N)).place(bitpack)CopySurprisingly, you find that there is no obvious difference in visual effects
after the change, and you now successfully finish a Taichi program with
quantized data types! More attempts are left to you.More complicated quantization schemesHere comes a more complicated scenario. In a 3D Eulerian fluid simulation, a
voxel may need to store a 3D vector for velocity, and an integer value for cell
category with three possible values: "source", "Dirichlet boundary", and
"Neumann boundar". You can actually store all information with a single 32-bit
ti.BitpackedFields:velocity_component_type = ti.types.quant.float(exp=6, frac=8, compute=ti.f32)velocity = ti.Vector.field(3, dtype=velocity_component_type)# Since there are only three cell categories, 2 bits are enough.cell_category_type = ti.types.quant.int(bits=2, signed=False, compute=ti.i32)cell_category = ti.field(dtype=cell_category_type)voxel = ti.BitpackedFields(max_num_bits=32)# Place three components of velocity into the voxel, and let them share the exponent.voxel.place(velocity, shared_exponent=True)# Place the 2-bit cell category.voxel.place(cell_category)# Create 512 x 512 x 256 voxels.ti.root.dense(ti.ijk, (512, 512, 256)).place(voxel)CopyThe compression scheme above allows you to store 13 bytes (4B x 3 + 1B) into
just 4 bytes. Note that you can still use velocity and cell_category in the
computation code, as if they are ti.f32 and ti.u8.Quant arraysBitpacked fields are actually laid in an array of structure (AOS) order.
However, there are also cases where a single quantized type is required to get
laid in an array. For example, you may want to store 8 x u4 values in a single
u32 type, to represent bin values of a histogram:Quant array is exactly what you need. A quant_array is a SNode which
can reinterpret a primitive type into an array of a quantized type:bin_value_type = ti.types.quant.int(bits=4, signed=False)# The quant array for 512 x 512 bin valuesarray = ti.root.dense(ti.ij, (512, 64)).quant_array(ti.j, 8, max_num_bits=32)# Place the unsigned 4-bit bin value into the arrayarray.place(bin_value_type)CopynoteOnly one field can be placed under a quant_array.Only quantized integer types and quantized fixed-point types are supported as
the dtype of the field under a quant_array.The size of the dtype of the field times the shape of the quant_array
must be less than or equal to the max_num_bits of the quant_array.Bit vectorizationFor quant arrays of 1-bit quantized integer types ("boolean"), Taichi provides
an additional optimization - bit vectorization. It aims at vectorizing
operations on such quant arrays under struct fors:u1 = ti.types.quant.int(1, False)N = 512M = 32x = ti.field(dtype=u1)y = ti.field(dtype=u1)ti.root.dense(ti.i, N // M).quant_array(ti.i, M, max_num_bits=M).place(x)ti.root.dense(ti.i, N // M).quant_array(ti.i, M, max_num_bits=M).place(y)@ti.kerneldef assign_vectorized():    ti.loop_config(bit_vectorize=True)    for i, j in x:        y[i, j] = x[i, j]  # 32 bits are handled at a timeassign_vectorized()CopyReference examplesThe following examples are from the
QuanTaichi paper,
so you can dig into details there.Game of LifeEulerian FluidMLS-MPMEdit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Quantized data typesQuantized integersQuantized fixed-point numbersQuantized floating-point numbersCompute typesData containers for quantized data typesBitpacked fieldsQuant arraysReference examplesGame of LifeEulerian FluidMLS-MPMCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
GUI System | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationGUI SystemA New UI system: GGUIExport Your ResultsPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Visualization>>GUI SystemVersion: v1.6.0On this pageGUI SystemTaichi has a built-in GUI system for visualizing simulation data in data containers like Taichi fields or NumPy ndarrays. It also has limited support for drawing primitive geometries.Create and display a windowThe following code creates a 640x360 window with a "Hello World!" title:gui = ti.GUI('Hello World!', (640, 360))CopyDisplays it by calling gui.show():while gui.running:    gui.show()CopynoteCall gui.show() inside a while loop. Otherwise, the window would flash once and disappear.Close the windowYou can set gui.running=False in the while loop to close the GUI:gui = ti.GUI('Window Title', (640, 360))some_events_happend = lambda: random.random() < 0.8while gui.running:    if some_events_happend():        gui.running = False    gui.show()CopyCoordinate systemEach window is built on a coordinate system: the origin is located in the lower-left corner, with the +x direction stretching to the right and the +y direction stretching upward.Display a field or ndarrayTo display a Taichi field or a NumPy ndarray, call gui.set_image(). The method accepts both types as input.gui = ti.GUI('Set Image', (640, 480))image = ti.Vector.field(3, ti.f32, shape=(640, 480))while gui.running:    gui.set_image(image)    gui.show()CopyBecause Taichi field is a global data container, if the vector field image is updated between the while loops, the GUI window refreshes to display the latest image.IMPORTANTEnsure that the shape of the input matches the resolution of the GUI window.Zero-copying frame bufferIn each loop of the gui.set_image() method call, the GUI system converts the image data to a displayable format and copies the result to the window buffer. This causes huge overload when the window size is large, making it hard to achieve high FPS (frames per second).If you only need to call the set_image() method without using any drawing command, you can enable fast_gui mode for better performance. This mode allows Taichi GUI to write the image data directly to the frame buffer without additional copying, and significantly increases FPS.gui = ti.GUI('Fast GUI', res=(400, 400), fast_gui=True)CopyFor this mode to work, ensure that the data passed into gui.set_image() is in a display-compatible format. In other words, If it is a Taichi field, ensure that it is one of the following:a vector field ti.field(3, dtype, shape) compatible with RGB format.a vector field ti.field(4, dtype, shape)  compatible with RGBA format.Note that dtype must be ti.f32, ti.f64, or ti.u8.Draw on a windowTaichi's GUI system supports drawing simple geometries, such as lines, circles, triangles, rectangles, arrows, and texts.Single geometryIn Taichi, drawing basic geometric shapes on the GUI is very intuitive. In most cases, all we need to do is specify information such as the position and size of the geometry and call the corresponding APIs.LineYou can draw a single line on a GUI canvas by specifying its begin and end points:import numpy as npgui = ti.GUI('Single Line', res=(400, 400))begin = [0.1, 0.1]end = [0.9, 0.9]while gui.running:    gui.line(begin, end, radius=1, color=0x068587)    gui.show()CopynoteCoordinates such as begin and end for single geometry can be Python lists, Numpy arrays or ti.Vector, as long as it's subscriptable and its dimension is (2, ).CircleYou can draw a single circle on a GUI canvas by specifying its center poistion and its radius:import numpy as npgui = ti.GUI('Single Circle', res=(400, 400))center = [0.5, 0.5]while gui.running:    gui.circle(pos=center, radius=30, color=0xED553B)    gui.show()CopyTriangleYou can draw a single triangle on a GUI canvas by specifying its three end points:import numpy as npgui = ti.GUI('Single Triangle', res=(400, 400))p1 = [0.5, 0.5]p2 = [0.6, 0.5]p3 = [0.5, 0.6]while gui.running:    gui.triangle(a=p1, b=p2, c=p3, color=0xEEEEF0)    gui.show()CopyRectangleYou can draw a single rectangle on a GUI canvas by specifying its topleft and bottomright points:import numpy as npgui = ti.GUI('Single Rectangle', res=(400, 400))p1 = [0.3, 0.4]p2 = [0.7, 0.6]while gui.running:    gui.rect(topleft=p1, bottomright=p2, color=0xFFFFFF)    gui.show()CopyArrowYou can draw a single arrow on a GUI canvas by specifying its start point and direction:import numpy as npgui = ti.GUI('Single Arrow', res=(400, 400))begin = [0.3, 0.3]increment = [0.5, 0.5]while gui.running:    gui.arrow(orig=begin, direction=increment, color=0xFFFFFF)    gui.show()CopyTextYou can draw a single line of text on a GUI canvas by specifying its position and contents:gui = ti.GUI('Text', res=(400, 400))position = [0.3, 0.5]while gui.running:    gui.text(content='Hello Taichi', pos=position, font_size=34, color=0xFFFFFF)    gui.show()CopyMultiple geometriesIt's also possible to draw multiple geometries at once by providing a collection of their positions to the GUI. The pos parameter of every drawing method accepts Taichi fields or NumPy arrays, not Python primitive lists. Each element of the array is a pair of floats ranging from 0.0 to 1.0, which represent the relative positions of the geometries. For example:(0.0, 0.0): the lower-left corner of the window.(1.0, 1.0): the upper-right corner of the window.LinesThe following code draws five blue line segments whose width is 2, with X and Y representing the five starting points and the five ending points.import numpy as npX = np.random.random((5, 2))Y = np.random.random((5, 2))gui = ti.GUI("lines", res=(400, 400))while gui.running:    gui.lines(begin=X, end=Y, radius=2, color=0x068587)    gui.show()CopyCirclesThe following code draws 50 circles with a radius of 5 and in three different colors randomly assigned by indices, an integer array of the same size as pos.import numpy as nppos = np.random.random((50, 2))# Create an array of 50 integer elements whose values are randomly 0, 1, 2# 0 corresponds to 0x068587# 1 corresponds to 0xED553B# 2 corresponds to 0xEEEEF0indices = np.random.randint(0, 2, size=(50,))gui = ti.GUI("circles", res=(400, 400))while gui.running:    gui.circles(pos, radius=5, palette=[0x068587, 0xED553B, 0xEEEEF0], palette_indices=indices)    gui.show()CopyTrianglesThe following code draws two orange triangles orange, with X, Y, and Z representing the three points of the triangles.import numpy as npX = np.random.random((2, 2))Y = np.random.random((2, 2))Z = np.random.random((2, 2))gui = ti.GUI("triangles", res=(400, 400))while gui.running:    gui.triangles(a=X, b=Y, c=Z, color=0xED553B)    gui.show()CopyArrowsThe following code generates 100 random sized arrows, with begins and direction represents their begin points and incrementals:import numpy as npbegins = np.random.random((100, 2))directions = np.random.uniform(low=-0.05, high=0.05, size=(100, 2))gui = ti.GUI('arrows', res=(400, 400))while gui.running:    gui.arrows(orig=begins, direction=directions, radius=1)    gui.show()CopyNotice that we used low and high in the call to np.random.uniform() to limit the range of generated random numbers.Event handlingTaichi's GUI system also provides a set of methods for mouse and keyboard control. Input events are classified into three types:ti.GUI.RELEASE  # key up or mouse button upti.GUI.PRESS    # key down or mouse button downti.GUI.MOTION   # mouse motion or mouse wheelCopyEvent key is the key that you press from your keyboard or mouse. It can be one of:# for ti.GUI.PRESS and ti.GUI.RELEASE event:ti.GUI.ESCAPE  # Escti.GUI.SHIFT   # Shiftti.GUI.LEFT    # Left Arrow'a'            # we use lowercase for alphabet'b'...ti.GUI.LMB     # Left Mouse Buttonti.GUI.RMB     # Right Mouse Button# for ti.GUI.MOTION event:ti.GUI.MOVE    # Mouse Movedti.GUI.WHEEL   # Mouse Wheel ScrollingCopyAn event filter is a combined list of key, type, and (type, key) tuple. For example:# if ESC pressed or released:gui.get_event(ti.GUI.ESCAPE)# if any key is pressed:gui.get_event(ti.GUI.PRESS)# if ESC is pressed or SPACE is released:gui.get_event((ti.GUI.PRESS, ti.GUI.ESCAPE), (ti.GUI.RELEASE, ti.GUI.SPACE))Copygui.get_event() pops an event from the queue and saves it to gui.event. For example:if gui.get_event():    print('Got event, key =', gui.event.key)CopyThe following code defines that the while loop goes on until ESC is pressed:while gui.running:    if gui.get_event(ti.GUI.ESCAPE):        break    gui.show()Copygui.is_pressed() detects the pressed keys. As the following code snippet shows, you must use it together with gui.get_event(). Otherwise, it is not updated.For example:while gui.running:    gui.get_event()  # must be called before is_pressed    if gui.is_pressed('a', ti.GUI.LEFT):        print('Go left!')    elif gui.is_pressed('d', ti.GUI.RIGHT):        print('Go right!')    gui.show()CopycautionCall gui.get_event() before calling gui.is_pressed(). Otherwise, gui.is_pressed() does not take effect.Retrieve cursor positiongui.get_cursor_pos() returns the cursor's current position in the window. The return value is a pair of floats in the range [0.0, 1.0]. For example:mouse_x, mouse_y = gui.get_cursor_pos()CopyGUI WidgetsTaichi's GUI system also provides widgets, including slider(), label(), and button(), for you to customize your control interface. Take a look at the following code snippet:import taichi as tigui = ti.GUI('GUI widgets')radius = gui.slider('Radius', 1, 50, step=1)xcoor = gui.label('X-coordinate')okay = gui.button('OK')xcoor.value = 0.5radius.value = 10while gui.running:    for e in gui.get_events(gui.PRESS):        if e.key == gui.ESCAPE:            gui.running = False        elif e.key == 'a':            xcoor.value -= 0.05        elif e.key == 'd':            xcoor.value += 0.05        elif e.key == 's':            radius.value -= 1        elif e.key == 'w':            radius.value += 1        elif e.key == okay:            print('OK clicked')    gui.circle((xcoor.value, 0.5), radius=radius.value)    gui.show()CopyEdit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Create and display a windowClose the windowCoordinate systemDisplay a field or ndarrayZero-copying frame bufferDraw on a windowSingle geometryMultiple geometriesEvent handlingGUI WidgetsCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
A New UI system: GGUI | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationGUI SystemA New UI system: GGUIExport Your ResultsPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Visualization>>A New UI system: GGUIVersion: v1.6.0On this pageA New UI system: GGUICategoryPrerequisitesOSWindows / Linux / Mac OS XBackendx64 / CUDA / VulkanStarting from v0.8.0, Taichi adds a new UI system GGUI. The new system uses GPU for rendering, making it much faster to render 3D scenes. That is why this new system gets its name as GGUI. This document describes the APIs that it provides.IMPORTANTIf you choose Vulkan as backend, ensure that you install the Vulkan environment.noteIt is recommended that you familiarize yourself with GGUI through the examples in examples/ggui_examples.noteThe variables referenced in code snippets below are define like this:vertices         = ti.Vector.field(2, ti.f32, shape=200)vertices_3d      = ti.Vector.field(3, ti.f32, shape=200)indices          = ti.field(ti.i32, shape=200 * 3)normals          = ti.Vector.field(3, ti.f32, shape=200)per_vertex_color = ti.Vector.field(3, ti.f32, shape=200)color  = (0.5, 0.5, 0.5)CopyCreate a windowti.ui.Window(name, res) creates a window.window = ti.ui.Window(name='Window Title', res = (640, 360), fps_limit=200, pos = (150, 150))CopyThe name parameter sets the title of the window.The res parameter specifies the resolution (width and height) of the window.The fps_limit parameter sets the maximum frames per second (FPS) for the window.The pos parameter specifies the position of the window with respect to the top-left corner of the main screen.A ti.ui.Window can display three types of objects:2D Canvas, which is used to draw simple 2D geometries like circles and triangles.+ 3D Scene, which is used to render 3D meshes and particles, and provides configurable camera and light sources.Immediate mode GUI components, such as buttons and textboxes.2D CanvasCreate a canvasThe following code retrieves a Canvas object that covers the entire window.canvas = window.get_canvas()CopyDraw on the canvascanvas.set_background_color(color)canvas.triangles(vertices, color, indices, per_vertex_color)radius = 5canvas.circles(vertices, radius, color, per_vertex_color)width = 2canvas.lines(vertices, width, indices, color, per_vertex_color)canvas.set_image(window.get_image_buffer_as_numpy())CopyThe arguments vertices, indices, per_vertex_color, and image must be Taichi fields. If per_vertex_color is provided, color is ignored.The positions/centers of geometries are represented as floats between 0.0 and 1.0, which indicate the relative positions of the geometries on the canvas. For circles() and lines(), the radius and width arguments are relative to the height of the window.The canvas is cleared after every frame. Always call these methods within the render loop.3D SceneCreate a scenescene = ti.ui.Scene()CopyConfigure cameracamera = ti.ui.Camera()camera.position(1, 2, 3)  # x, y, zcamera.lookat(4, 5, 6)camera.up(0, 1, 0)camera.projection_mode(ti.ui.ProjectionMode.Perspective)scene.set_camera(camera)CopyConfiguring light sourcesAdd a point lightCall point_light() to add a point light to the scene.scene.point_light(pos=(1, 2, 3), color=(0.5, 0.5, 0.5))CopyNote that you need to call point_light() for every frame. Similar to the canvas() methods, call this method within your render loop.3D Geometriesscene.lines(vertices, width, indices, color, per_vertex_color)scene.mesh(vertices_3d, indices, normals, color, per_vertex_color)scene.particles(vertices, radius, color, per_vertex_color)CopyThe arguments vertices, indices, per_vertex_color, and image are all expected to be Taichi fields. If per_vertex_color is provided, color is ignored.The positions/centers of geometries should be in the world-space coordinates.noteIf a mesh has num triangles, the indices should be a 1D scalar field with a shape (num * 3), not a vector field.normals is an optional parameter for scene.mesh().An example of drawing 3d-linesimport taichi as titi.init(arch=ti.cuda)N = 10particles_pos = ti.Vector.field(3, dtype=ti.f32, shape = N)points_pos = ti.Vector.field(3, dtype=ti.f32, shape = N)@ti.kerneldef init_points_pos(points : ti.template()):    for i in range(points.shape[0]):        points[i] = [i for j in ti.static(range(3))]init_points_pos(particles_pos)init_points_pos(points_pos)window = ti.ui.Window("Test for Drawing 3d-lines", (768, 768))canvas = window.get_canvas()scene = ti.ui.Scene()camera = ti.ui.Camera()camera.position(5, 2, 2)while window.running:    camera.track_user_inputs(window, movement_speed=0.03, hold_key=ti.ui.RMB)    scene.set_camera(camera)    scene.ambient_light((0.8, 0.8, 0.8))    scene.point_light(pos=(0.5, 1.5, 1.5), color=(1, 1, 1))    scene.particles(particles_pos, color = (0.68, 0.26, 0.19), radius = 0.1)    # Draw 3d-lines in the scene    scene.lines(points_pos, color = (0.28, 0.68, 0.99), width = 5.0)    canvas.scene(scene)    window.show()CopyAdvanced 3d Geometriesscene = ti.ui.Scene()width = 2radius = 5scene.lines(vertices, width, indices, color, per_vertex_color, vertex_offset=0, vertex_count=10, index_offset=0, index_count=10)scene.mesh(vertices_3d, indices, normals, color, per_vertex_color, vertex_offset=0, vertex_count=10, index_offset=0, index_count=10, show_wireframe=True)scene.particles(vertices, radius, color, per_vertex_color, index_offset=0, index_count=10)scene.mesh_instance(vertices_3d, indices, normals, color, per_vertex_color, vertex_offset=0, vertex_count=10, index_offset=0, index_count=10, show_wireframe=True)CopyThe additional arguments vertex_offset, vertex_count, index_offset and index_count control the visible part of the particles and mesh. For the mesh() and mesh_instance() methods, set whether to show wireframe mode through setting show_wireframe.:::exampleExample of drawing a part of the mesh/particlesscene = ti.ui.Scene()center = ti.Vector.field(3, ti.f32, shape=10)# For particles# draw the 2-th to 7-th particlesscene.particles(center, radius=1, index_offset = 1, index_count = 6)# For mesh# 1. with indicesscene.mesh(    vertices_3d, indices, index_offset=1, index_count=3,    # vertex_offset is set to 0 by default, and it is not necessary    # to assign vertex_offset a value that otherwise you must.    vertex_offset = 1    )# usually used as below:# draw the 11-th to 111-th mesh vertexesscene.mesh(vertices_3d, indices, index_offset=10, index_count=100)# 2. without indices (similar to the particles' example above)scene.mesh(    vertices_3d,    vertex_offset=2,  # user defined first vertex index    vertex_count=3,  # user defined vertex count    )CopyAn example of drawing part of linesimport taichi as titi.init(arch=ti.cuda)N = 10particles_pos = ti.Vector.field(3, dtype=ti.f32, shape = N)points_pos = ti.Vector.field(3, dtype=ti.f32, shape = N)points_indices = ti.Vector.field(1, dtype=ti.i32, shape = N)@ti.kerneldef init_points_pos(points : ti.template()):    for i in range(points.shape[0]):        points[i] = [i for j in range(3)]        # points[i] = [ti.sin(i * 1.0), i * 0.2, ti.cos(i * 1.0)]@ti.kerneldef init_points_indices(points_indices : ti.template()):    for i in range(N):        points_indices[i][0] = i // 2 + i % 2init_points_pos(particles_pos)init_points_pos(points_pos)init_points_indices(points_indices)window = ti.ui.Window("Test for Drawing 3d-lines", (768, 768))canvas = window.get_canvas()scene = ti.ui.Scene()camera = ti.ui.Camera()camera.position(5, 2, 2)while window.running:    camera.track_user_inputs(window, movement_speed=0.03, hold_key=ti.ui.RMB)    scene.set_camera(camera)    scene.ambient_light((0.8, 0.8, 0.8))    scene.point_light(pos=(0.5, 1.5, 1.5), color=(1, 1, 1))    scene.particles(particles_pos, color = (0.68, 0.26, 0.19), radius = 0.1)    # Here you will get visible part from the 3rd point with (N - 4) points.    scene.lines(points_pos, color = (0.28, 0.68, 0.99), width = 5.0, vertex_count = N - 4, vertex_offset = 2)    # Using indices to indicate which vertex to use    # scene.lines(points_pos, color = (0.28, 0.68, 0.99), width = 5.0, indices = points_indices)    # Case 1, vertex_count will be changed to N - 2 when drawing.    # scene.lines(points_pos, color = (0.28, 0.68, 0.99), width = 5.0, vertex_count = N - 1, vertex_offset = 0)    # Case 2, vertex_count will be changed to N - 2 when drawing.    # scene.lines(points_pos, color = (0.28, 0.68, 0.99), width = 5.0, vertex_count = N, vertex_offset = 2)    canvas.scene(scene)    window.show()CopyDetails of mesh instancingscene = ti.ui.Scene()num_instance = 100m_transforms = ti.Matrix.field(4, 4, dtype = ti.f32, shape = num_instance)# For example: An object is scaled by 2, rotated by rotMat, and translated by t = [1, 2, 3], then## The ScaleMatrix is:# 2, 0, 0, 0# 0, 2, 0, 0# 0, 0, 2, 0# 0, 0, 0, 1## The RotationMatrix is:# https://en.wikipedia.org/wiki/Rotation_matrix#General_rotations## The TranslationMatrix is:# 1, 0, 0, 1# 0, 1, 0, 2# 0, 0, 1, 3# 0, 0, 0, 1## Let TransformMatrix = TranslationMatrix @ RotationMatrix @ ScaleMatrix, then the final TransformMatrix is:#   2 * rotMat00,     rotMat01,       rotMat02, 1#       rotMat10, 2 * rotMat11,       rotMat12, 2#       rotMat20,     rotMat21,   2 * rotMat22, 3#              0,            0,              0, 1...# Draw mesh instances (from the 1st instance)scene.mesh_instance(vertices_3d, indices, transforms = m_transforms, instance_offset = 1)CopyExample of setting wireframe modewindow = ti.ui.Window("Display Mesh", (1024, 1024), vsync=True)canvas = window.get_canvas()scene = ti.ui.Scene()camera = ti.ui.Camera()# slider_int usagesome_int_type_value = 0def show_options():    global some_int_type_value    window.GUI.begin("Display Panel", 0.05, 0.1, 0.2, 0.15)    display_mode = window.GUI.slider_int("Value Range", some_int_type_value, 0, 5)    window.GUI.end()while window.running:    ...    # if to show wireframe    scene.mesh_instance(vertices_3d, indices, instance_count = 100 , show_wireframe = True)    canvas.scene(scene)    show_options()    window.show()CopynoteIf indices is not provided, consider using like this:scene = ti.ui.Scene()scene.mesh(vertices_3d, normals, color, per_vertex_color, vertex_offset=0, vertex_count=50, show_wireframe=True)CopyIf indices is provided, consider using like this:scene.mesh(vertices_3d, indices, normals, color, per_vertex_color, vertex_offset=0, index_offset=0, index_count=50, show_wireframe=True)CopyRendering the sceneYou can render a scene on a canvas.window = ti.ui.Window(name='Title', res=(640, 360))canvas = window.get_canvas()canvas.scene(scene)CopyFetching Color/Depth informationimg = window.get_image_buffer_as_numpy()window.get_depth_buffer(scene_depth)depth = window.get_depth_buffer_as_numpy()CopyAfter rendering the current scene, you can fetch the color and depth information of the current scene using get_image_buffer_as_numpy() and get_depth_buffer_as_numpy(), which copy the gpu data to a NumPy array(cpu).
get_depth_buffer() copies the GPU data to a Taichi field (depend on the arch you choose) or copies data from GPU to GPU.:::exampleExample of fetching color informationwindow = ti.ui.Window("Test for getting image buffer from ggui", (768, 768), vsync=True)video_manager = ti.tools.VideoManager("OutputDir")while window.running:    # render_scene()    img = window.get_image_buffer_as_numpy()    video_manager.write_frame(img)    window.show()video_manager.make_video(gif=True, mp4=True)CopyAn example of fetching the depth datawindow_shape = (720, 1080)window = ti.ui.Window("Test for copy depth data", window_shape)canvas = window.get_canvas()scene = ti.ui.Scene()camera = ti.ui.Camera()# Get the shape of the windoww, h = window.get_window_shape()# The field/ndarray stores the depth information, and must be of the ti.f32 data type and have a 2d shape.# or, in other words, the shape must equal the window's shapescene_depth = ti.ndarray(ti.f32, shape = (w, h))# scene_depth = ti.field(ti.f32, shape = (w, h))while window.running:    # render()    canvas.scene(scene)    window.get_depth_buffer(scene_depth)    window.show()CopyGUI componentsThe design of GGUI's GUI components follows the Dear ImGui APIs.window = ti.ui.Window("Test for GUI", res=(512, 512))gui = window.get_gui()value = 0color = (1.0, 1.0, 1.0)with gui.sub_window("Sub Window", x=10, y=10, width=300, height=100):    gui.text("text")    is_clicked = gui.button("name")    value = gui.slider_float("name1", value, minimum=0, maximum=100)    color = gui.color_edit_3("name2", color)CopyShow a windowCall show() to show a window.window.show()CopyCall this method only at the end of the render loop for each frame.User input processingTo retrieve the events that have occurred since the last method call:events = window.get_events()CopyEach event in events is an instance of ti.ui.Event. It has the following properties:event.action, which can be ti.ui.PRESS, ti.ui.RELEASE, or ti.ui.MOTION.event.key: the key related to this event.To retrieve the mouse position:window.get_cursor_pos()To check if a key is pressed:window.is_pressed(key)The following is a user input processing example from mpm128:gravity = ti.Vector.field(2, ti.f32, shape=())attractor_strength = ti.field(ti.f32, shape=())while window.running:    # keyboard event processing    if window.get_event(ti.ui.PRESS):        if window.event.key == 'r': reset()        elif window.event.key in [ti.ui.ESCAPE]: break    if window.event is not None: gravity[None] = [0, 0]  # if had any event    if window.is_pressed(ti.ui.LEFT, 'a'): gravity[None][0] = -1    if window.is_pressed(ti.ui.RIGHT, 'd'): gravity[None][0] = 1    if window.is_pressed(ti.ui.UP, 'w'): gravity[None][1] = 1    if window.is_pressed(ti.ui.DOWN, 's'): gravity[None][1] = -1    # mouse event processing    mouse = window.get_cursor_pos()    # ...    if window.is_pressed(ti.ui.LMB):        attractor_strength[None] = 1    if window.is_pressed(ti.ui.RMB):        attractor_strength[None] = -1    window.show()CopyImage I/OTo write the current frame in the window to an image file:window.save_image('frame.png')CopyNote that you must call window.save_image() before calling window.show().Off-screen renderingGGUI supports saving frames to images without showing the window. This is also known as "headless" rendering. To enable this mode, set the argument show_window to False when initializing a window.window = ti.ui.Window('Window Title', (640, 360), show_window = False)CopyThen you can call window.save_image() as normal and remove the window.show() call at the end.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Create a window2D CanvasCreate a canvasDraw on the canvas3D SceneCreate a sceneConfigure cameraConfiguring light sources3D GeometriesAdvanced 3d GeometriesRendering the sceneFetching Color/Depth informationGUI componentsShow a windowUser input processingImage I/OOff-screen renderingCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Export Your Results | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationGUI SystemA New UI system: GGUIExport Your ResultsPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Visualization>>Export Your ResultsVersion: v1.6.0On this pageExport Your ResultsTaichi has functions that help you export visual results to images or
videos. This tutorial demonstrates how to use them step by step.Export imagesThere are two ways to export visual results of your program to
images.The first and easier way is to make use of ti.GUI.The second way is to call some Taichi functions such as
ti.tools.imwrite.Export images using ti.GUI.showti.GUI.show(filename) can not only display the GUI canvas on your
screen, but also save the image to your specified filename.Note that the format of the image is fully determined by the suffix
of filename.Taichi now supports saving to png, jpg, and bmp formats.We recommend using png format. For example:import taichi as tiimport osti.init()pixels = ti.field(ti.u8, shape=(512, 512, 3))@ti.kerneldef paint():    for i, j, k in pixels:        pixels[i, j, k] = ti.random() * 255iterations = 1000gui = ti.GUI("Random pixels", res=512)# mainloopfor i in range(iterations):    paint()    gui.set_image(pixels)    filename = f'frame_{i:05d}.png'   # create filename with suffix png    print(f'Frame {i} is recorded in {filename}')    gui.show(filename)  # export and show in GUICopyAfter running the code above, you will get a series of images in the current folder.Export images using ti.tools.imwriteTo save images without invoking ti.GUI.show(filename), use
ti.tools.imwrite(filename). For example:import taichi as titi.init()pixels = ti.field(ti.u8, shape=(512, 512, 3))@ti.kerneldef set_pixels():    for i, j, k in pixels:        pixels[i, j, k] = ti.random() * 255set_pixels()filename = f'imwrite_export.png'ti.tools.imwrite(pixels.to_numpy(), filename)print(f'The image has been saved to {filename}')Copyti.tools.imwrite can export Taichi fields (ti.Matrix.field,
ti.Vector.field, ti.field) and numpy arrays np.ndarray.Same as above ti.GUI.show(filename), the image format (png,
jpg and bmp) is also controlled by the suffix of filename in
ti.tools.imwrite(filename).Meanwhile, the resulted image type (grayscale, RGB, or RGBA) is
determined by the number of channels in the input field, i.e.,
the length of the third dimension (field.shape[2]).In other words, a field that has shape (w, h) or (w, h, 1) will
be exported as a grayscale image.If you want to export RGB or RGBA images instead, the input
field should have a shape (w, h, 3) or (w, h, 4) respectively.noteAll Taichi fields have their own data types, such as ti.u8 and
ti.f32. Different data types can lead to different behaviors of
ti.tools.imwrite. Please check out GUI system for
more details.Taichi offers other helper functions that read and show images in
addition to ti.tools.imwrite. They are also demonstrated in
GUI system.Convert PNGs to videoSometimes it's convenient to convert a series of png files into a
single video when showing your result to others.For example, suppose you have 000000.png, 000001.png, ... generated
according to Export your results in the
current working directory.Then you could run ti video to create a file video.mp4 containing
all these images as frames (sorted by file name).Use ti video -f40 for creating a video with 40 FPS.Export videosnoteThe video export utilities of Taichi depend on ffmpeg. If ffmpeg is
not installed on your machine, please follow the installation
instructions of ffmpeg at the end of this page.ti.tools.VideoManager can help you export results in mp4 or gif
format. For example,import taichi as titi.init()pixels = ti.field(ti.u8, shape=(512, 512, 3))@ti.kerneldef paint():    for i, j, k in pixels:        pixels[i, j, k] = ti.random() * 255result_dir = "./results"video_manager = ti.tools.VideoManager(output_dir=result_dir, framerate=24, automatic_build=False)for i in range(50):    paint()    pixels_img = pixels.to_numpy()    video_manager.write_frame(pixels_img)    print(f'\rFrame {i+1}/50 is recorded', end='')print()print('Exporting .mp4 and .gif videos...')video_manager.make_video(gif=True, mp4=True)print(f'MP4 video is saved to {video_manager.get_output_filename(".mp4")}')print(f'GIF video is saved to {video_manager.get_output_filename(".gif")}')CopyAfter running the code above, you will find the output videos in the
./results/ folder.Convert video to GIFSometimes you may need gif images to post a result on forums.To do so, run ti gif -i video.mp4, where video.mp4 is the
mp4 video (generated with instructions above).Use ti gif -i video.mp4 -f40 to create a GIF at 40 FPS.Install ffmpegInstall ffmpeg on WindowsDownload the ffmpeg archive(named ffmpeg-2020xxx.zip) from
ffmpeg.Unzip this archive to a folder, such as D:/YOUR_FFMPEG_FOLDER.Important: add D:/YOUR_FFMPEG_FOLDER/bin to the PATH
environment variable.Open the Windows cmd or PowerShell and type the line of code
below to test your installation. If ffmpeg is set up properly, the
version information will be printed.ffmpeg -versionCopyInstall ffmpeg on LinuxMost Linux distribution came with ffmpeg natively, so you do not
need to read this part if the ffmpeg command is already there on
your machine.Install ffmpeg on Ubuntusudo apt-get updatesudo apt-get install ffmpegCopyInstall ffmpeg on CentOS and RHELsudo yum install ffmpeg ffmpeg-develCopyInstall ffmpeg on Arch Linux:pacman -S ffmpegCopyTest your installation usingffmpeg -hCopyInstall ffmpeg on macOSffmpeg can be installed on macOS using homebrew:brew install ffmpegCopyExport PLY filesti.tools.PLYWriter can help you export results in the ply format.
Below is a short example of exporting 10 frames of a moving cube
with vertices randomly colored,import taichi as tiimport numpy as npti.init(arch=ti.cpu)num_vertices = 1000pos = ti.Vector.field(3, dtype=ti.f32, shape=(10, 10, 10))rgba = ti.Vector.field(4, dtype=ti.f32, shape=(10, 10, 10))@ti.kerneldef place_pos():    for i, j, k in pos:        pos[i, j, k] = 0.1 * ti.Vector([i, j, k])@ti.kerneldef move_particles():    for i, j, k in pos:        pos[i, j, k] += ti.Vector([0.1, 0.1, 0.1])@ti.kerneldef fill_rgba():    for i, j, k in rgba:        rgba[i, j, k] = ti.Vector(            [ti.random(), ti.random(), ti.random(), ti.random()])place_pos()series_prefix = "example.ply"for frame in range(10):    move_particles()    fill_rgba()    # now adding each channel only supports passing individual np.array    # so converting into np.ndarray, reshape    # remember to use a temp var to store so you dont have to convert back    np_pos = np.reshape(pos.to_numpy(), (num_vertices, 3))    np_rgba = np.reshape(rgba.to_numpy(), (num_vertices, 4))    # create a PLYWriter    writer = ti.tools.PLYWriter(num_vertices=num_vertices)    writer.add_vertex_pos(np_pos[:, 0], np_pos[:, 1], np_pos[:, 2])    writer.add_vertex_rgba(        np_rgba[:, 0], np_rgba[:, 1], np_rgba[:, 2], np_rgba[:, 3])    writer.export_frame_ascii(frame, series_prefix)CopyAfter running the code above, you will find the output sequence of ply
files in the current working directory. Next, we will break down the
usage of ti.tools.PLYWriter into 4 steps and show some examples.Setup ti.tools.PLYWriter# num_vertices must be a positive int# num_faces is optional, default to 0# face_type can be either "tri" or "quad", default to "tri"# in our previous example, a writer with 1000 vertices and 0 triangle faces is creatednum_vertices = 1000writer = ti.tools.PLYWriter(num_vertices=num_vertices)# in the below example, a writer with 20 vertices and 5 quadrangle faces is createdwriter2 = ti.tools.PLYWriter(num_vertices=20, num_faces=5, face_type="quad")CopyAdd required channels# A 2D grid with quad faces#     y#     |# z---/#    x#         19---15---11---07---03#         |    |    |    |    |#         18---14---10---06---02#         |    |    |    |    |#         17---13---19---05---01#         |    |    |    |    |#         16---12---08---04---00writer = ti.tools.PLYWriter(num_vertices=20, num_faces=12, face_type="quad")# For the vertices, the only required channel is the position,# which can be added by passing 3 np.array x, y, z into the following function.x = np.zeros(20)y = np.array(list(np.arange(0, 4))*5)z = np.repeat(np.arange(5), 4)writer.add_vertex_pos(x, y, z)# For faces (if any), the only required channel is the list of vertex indices that each face contains.indices = np.array([0, 1, 5, 4]*12)+np.repeat(    np.array(list(np.arange(0, 3))*4)+4*np.repeat(np.arange(4), 3), 4)writer.add_faces(indices)CopyAdd optional channels# Add custom vertex channel, the input should include a key, a supported datatype and, the data np.arrayvdata = np.random.rand(20)writer.add_vertex_channel("vdata1", "double", vdata)# Add custom face channelfoo_data = np.zeros(12)writer.add_face_channel("foo_key", "foo_data_type", foo_data)# error! because "foo_data_type" is not a supported datatype. Supported ones are# ['char', 'uchar', 'short', 'ushort', 'int', 'uint', 'float', 'double']# PLYwriter already defines several useful helper functions for common channels# Add vertex color, alpha, and rgba# using float/double r g b alpha to reprent color, the range should be 0 to 1r = np.random.rand(20)g = np.random.rand(20)b = np.random.rand(20)alpha = np.random.rand(20)writer.add_vertex_color(r, g, b)writer.add_vertex_alpha(alpha)# equivilantly# add_vertex_rgba(r, g, b, alpha)# vertex normalwriter.add_vertex_normal(np.ones(20), np.zeros(20), np.zeros(20))# vertex index, and piece (group id)writer.add_vertex_id()writer.add_vertex_piece(np.ones(20))# Add face index, and piece (group id)# Indexing the existing faces in the writer and add this channel to face channelswriter.add_face_id()# Set all the faces is in group 1writer.add_face_piece(np.ones(12))CopyExport filesseries_prefix = "example.ply"series_prefix_ascii = "example_ascii.ply"# Export a single file# use ascii so you can read the contentwriter.export_ascii(series_prefix_ascii)# alternatively, use binary for a bit better performance# writer.export(series_prefix)# Export a sequence of files, ie in 10 framesfor frame in range(10):    # write each frame as i.e. "example_000000.ply" in your current running folder    writer.export_frame_ascii(frame, series_prefix_ascii)    # alternatively, use binary    # writer.export_frame(frame, series_prefix)    # update location/color    x = x + 0.1*np.random.rand(20)    y = y + 0.1*np.random.rand(20)    z = z + 0.1*np.random.rand(20)    r = np.random.rand(20)    g = np.random.rand(20)    b = np.random.rand(20)    alpha = np.random.rand(20)    # re-fill    writer = ti.tools.PLYWriter(num_vertices=20, num_faces=12, face_type="quad")    writer.add_vertex_pos(x, y, z)    writer.add_faces(indices)    writer.add_vertex_channel("vdata1", "double", vdata)    writer.add_vertex_color(r, g, b)    writer.add_vertex_alpha(alpha)    writer.add_vertex_normal(np.ones(20), np.zeros(20), np.zeros(20))    writer.add_vertex_id()    writer.add_vertex_piece(np.ones(20))    writer.add_face_id()    writer.add_face_piece(np.ones(12))CopyImport ply files into Houdini and BlenderHoudini supports importing a series of ply files sharing the same
prefix/post-fix. Our export_frame can achieve the requirement for you.In Houdini, click File->Import->Geometry and navigate to the folder
containing your frame results, which should be collapsed into one single
entry like example_$F6.ply (0-9). Double-click this entry to finish
the importing process.Blender requires an add-on called
Stop-motion-OBJ to
load the result sequences. Detailed
documentation is
provided by the author on how to install and use the add-on. If you're
using the latest version of Blender (2.80+), download and install the
latest
release
of Stop-motion-OBJ. For Blender 2.79 and older, use version v1.1.1 of
the add-on.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Export imagesExport images using ti.GUI.showExport images using ti.tools.imwriteConvert PNGs to videoExport videosConvert video to GIFInstall ffmpegInstall ffmpeg on WindowsInstall ffmpeg on LinuxInstall ffmpeg on macOSExport PLY filesImport ply files into Houdini and BlenderCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Profiler | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceProfilerPerformance TuningDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Performance>>ProfilerVersion: v1.6.0On this pageProfilerOverviewTaichi includes a collection of profiling tools to help with code debugging and optimization. These tools collect hardware and Taichi-related information to measure program performance and identify bottlenecks.Currently, Taichi provides two profiling tools:ScopedProfiler, which is responsible for analyzing the performance of the Taichi JIT compiler (host).KernelProfiler, which is responsible for analyzing the performance of Taichi kernels (device). Its advanced mode, which works with the CUDA backend only, provides detailed low-level performance metrics, such as memory bandwidth consumption.ScopedProfilerScopedProfiler is a profiler in Taichi that tracks the time spent on host tasks such as JIT compilation. It is enabled by default. To display results in a hierarchical format, you can call ti.profiler.print_scoped_profiler_info().For example:import taichi as titi.init(arch=ti.cpu)var = ti.field(ti.f32, shape=1)@ti.kerneldef compute():    var[0] = 1.0    print("Setting var[0] =", var[0])compute()ti.profiler.print_scoped_profiler_info()CopynoteScopedProfiler is a C++ class in Taichi.KernelProfilerKernelProfiler retrieves kernel profiling records from the backend, aggregates them in the Python scope, and prints the results to the console. Note that kernelProfiler supports CPU and CUDA only. Ensure that you call ti.sync() before performance profiling if your program runs on GPU.To enable the profiler, set kernel_profiler=True when calling ti.init().Use ti.profiler.print_kernel_profiler_info() to display profiling results. There are two printing modes:In "count" mode (the default), profiling recordings with the same kernel name are counted as a single profiling result.In "trace" mode, the profiler prints a list of kernels launched on hardware during the profiling period, including comprehensive performance and hardware characteristics for each kernel.Use ti.profiler.clear_kernel_profiler_info() to clear the entries in this profiler.For example:import taichi as titi.init(ti.cpu, kernel_profiler=True)x = ti.field(ti.f32, shape=1024*1024)@ti.kerneldef fill():    for i in x:        x[i] = ifor i in range(8):    fill()ti.profiler.print_kernel_profiler_info('trace')ti.profiler.clear_kernel_profiler_info()  # Clears all recordsfor i in range(100):    fill()ti.profiler.print_kernel_profiler_info()  # The default mode: 'count'CopyThe profiler outputs the following:=========================================================================X64 Profiler(trace)=========================================================================[      % |     time    ] Kernel name[  0.00% |    0.000  ms] jit_evaluator_0_kernel_0_serial[ 60.11% |    2.668  ms] fill_c4_0_kernel_1_range_for[  6.06% |    0.269  ms] fill_c4_0_kernel_1_range_for[  5.73% |    0.254  ms] fill_c4_0_kernel_1_range_for[  5.68% |    0.252  ms] fill_c4_0_kernel_1_range_for[  5.61% |    0.249  ms] fill_c4_0_kernel_1_range_for[  5.63% |    0.250  ms] fill_c4_0_kernel_1_range_for[  5.61% |    0.249  ms] fill_c4_0_kernel_1_range_for[  5.59% |    0.248  ms] fill_c4_0_kernel_1_range_for-------------------------------------------------------------------------[100.00%] Total kernel execution time:   0.004 s   number of records:  9==================================================================================================================================================X64 Profiler(count)=========================================================================[      %     total   count |      min       avg       max   ] Kernel name[100.00%   0.033 s    100x |    0.244     0.329     2.970 ms] fill_c4_0_kernel_1_range_for-------------------------------------------------------------------------[100.00%] Total kernel execution time:   0.033 s   number of records:  1=========================================================================Copynotejit_evaluator_xxx can be ignored because it is automatically generated by the system.Taichi recommends running performance profiling multiple times to observe the minimum or average execution time.Advanced modeKernelProfiler provides an experimental GPU profiling toolkit based on the Nvidia CUPTI for the CUDA backend, which offers minimal and predictable profiling overhead and can record over 6,000 hardware metrics.To use the CUPTI-based GPU profiler, you must first satisfy the following prerequisites:Install the CUDA Toolkit.Build Taichi from source with the CUDA toolkit using the command: TAICHI_CMAKE_ARGS="-DTI_WITH_CUDA_TOOLKIT:BOOL=ON" python3 setup.py develop --user.Resolve any permission issues related to the Nvidia profiling module by:Adding the options nvidia NVreg_RestrictProfilingToAdminUsers=0 line to the /etc/modprobe.d/nvidia-kernel-common.conf file.After modifying the configuration file, reboot the system, which should resolve the permission issue. Note that you may need to run update-initramfs -u before rebooting the system.Refer to the ERR_NVGPUCTRPERM documentation for more information.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?OverviewScopedProfilerKernelProfilerAdvanced modeCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Performance Tuning | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceProfilerPerformance TuningDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Performance>>Performance TuningVersion: v1.6.0On this pagePerformance TuningFor-loop decoratorsAs discussed in previous topics, Taichi kernels automatically parallelize for-loops in the outermost scope. Our compiler sets the settings automatically to best explore the target architecture. Nonetheless, for Ninjas seeking the final few percent of speed, we give several APIs to allow developers to fine-tune their programs. Specifying a proper block_dim, for example, might result in a nearly 3x speed gain in examples/mpm3d.py.You can use ti.loop_config to set the loop directives for the next for loop. Available directives are:parallelize: Sets the number of threads to use on CPUblock_dim: Sets the number of threads in a block on GPUserialize: If you set serialize to True, the for loop will run serially, and you can write break statements inside it
(Only applies on range/ndrange fors). Equals to setting parallelize to 1.@ti.kerneldef break_in_serial_for() -> ti.i32:    a = 0    ti.loop_config(serialize=True)    for i in range(100):  # This loop runs serially        a += i        if i == 10:            break    return abreak_in_serial_for()  # returns 55Copyn = 128val = ti.field(ti.i32, shape=n)@ti.kerneldef fill():    ti.loop_config(parallelize=8, block_dim=16)    # If the kernel is run on the CPU backend, 8 threads will be used to run it    # If the kernel is run on the CUDA backend, each block will have 16 threads.    for i in range(n):        val[i] = iCopyBackground: Thread hierarchy of GPUsIt is worthy to quickly discuss the thread hierarchy on contemporary GPU architectures in order to help you understand how the previously mentioned for-loop is parallelized.From fine-grained to coarse-grained, the computation units are as follows: iteration, thread, block, and grid.Iteration: The body of a for-loop is an iteration. Each iteration corresponds to a different i value in the for-loop.Thread: Iterations are classified as threads. A thread is the smallest parallelized unit. All iterations inside a thread are serial in nature. To maximize parallel efficiency, we normally employ one iteration per thread.Block: Threads are organized into groups called blocks. All threads within a block are executed in parallel and can share block local storage.Grid: Blocks are grouped into grids. A grid is the minimal unit that is launched from the host. All blocks within a grid are
executed in parallel. In Taichi, each parallelized for-loop
is represented as a grid.For more details, please see the CUDA C programming
guide.
Note that we employ the CUDA terminology here, other backends such as OpenGL and Metal follow a similar thread hierarchy.Example: Tuning the block-level parallelism of a for-loopProgrammers may prepend some decorator(s) to tweak the property of a
for-loop, e.g.:@ti.kerneldef func():    for i in range(8192):  # no decorator, use default settings        ...    ti.loop_config(block_dim=128)      # change the property of next for-loop:    for i in range(8192):  # will be parallelized with block_dim=128        ...    for i in range(8192):  # no decorator, use default settings        ...CopyData layoutsBecause Taichi separates data structures from computation, developers may experiment with alternative data layouts. Choosing an efficient layout, like in other programming languages, may significantly enhance performance. Please consult the Fields (advanced) section for further information on advanced data layouts in Taichi.Local Storage OptimizationsTaichi has a few speed enhancements that take use of fast memory (e.g., CUDA shared memory, L1 cache). Simply, Taichi replaces access to global memory (slow) with access to local memory (quick) wherever feasible, and writes data in local memory (e.g., CUDA shared memory) back to global memory at the conclusion. Such changes keep the original program's semantics (will be explained later).Thread Local Storage (TLS)TLS is mostly designed to optimize parallel reduction. When Taichi identifies
a global reduction pattern in a @ti.kernel, it automatically applies the TLS
optimizations during code generation, similar to those found in common GPU
reduction implementations.We will walk through an example using CUDA's terminology.x = ti.field(ti.f32, shape=1000000)s = ti.field(ti.f32, shape=())@ti.kerneldef sum():  for i in x:    s[None] += x[i]sum()CopyTaichi's parallel loop is implemented internally with Grid-Stride Loops.
This means that each physical CUDA thread may handle several items in x.
In other words, the number of threads started for sum can be less than the shape of x.One optimization offered by this method is the substitution of a thread-local memory access for a global memory access. Instead of directly and atomically adding x[i] to the global memory destination s[None], Taichi preallocates a thread-local buffer upon entering the thread, accumulates (non-atomically) the value of x into this buffer, and then atomically adds the result of the buffer back to s[None] before exiting the thread. If each thread handles N items in x, the number of atomic additions is reduced to one-Nth of its original amount.Additionally, the last atomic add to the global memory s[None] is optimized using
CUDA's warp-level intrinsics, further reducing the number of required atomic adds.Currently, Taichi supports TLS optimization for these reduction operators: add,
sub, min and max on 0D scalar/vector/matrix ti.fields. It is not yet
supported on ti.ndarrays. Here
is a benchmark comparison when running a global max reduction on a 1-D Taichi field
of 8M floats on an Nvidia GeForce RTX 3090 card:TLS disabled: 5.2 x 1e3 usTLS enabled: 5.7 x 1e1 usTLS has resulted in a 100x increase in speed. We also demonstrate that TLS reduction sum achieves equivalent performance to CUDA implementations; for more information, see the benchmark report.Block Local Storage (BLS)Context: For a sparse field whose last layer is a dense SNode (i.e., its layer
hierarchy matches ti.root.(sparse SNode)+.dense), Taichi will assign one CUDA
thread block to each dense container (or dense block). BLS optimization works
specifically for such kinds of fields.BLS intends to enhance stencil computing processes by utilizing CUDA shared memory. This optimization begins with users annotating the set of fields they want to cache using ti.block local. At compile time, Taichi tries to identify the accessing range in relation to the dense block of these annotated fields. If Taichi is successful, it creates code that first loads all of the accessible data in range into a block local buffer (CUDA's shared memory), then replaces all accesses to the relevant slots into this buffer.Here is an example illustrating the usage of BLS. a is a sparse field with a
block size of 4x4.a = ti.field(ti.f32)b = ti.field(ti.f32)# `a` has a block size of 4x4ti.root.pointer(ti.ij, 32).dense(ti.ij, 4).place(a)@ti.kerneldef foo():  # Taichi will cache `a` into the CUDA shared memory  ti.block_local(a)  for i, j in a:    print(a[i - 1, j], a[i, j + 2])CopyEach loop iteration accesses items with an offset [-1, 0] and [0, 2] to its
coordinates, respectively. Therefore, for an entire block spanning from [M, N]
(inclusive) to [M + 4, N + 4] (exclusive), the accessed range w.r.t this block
is [M - 1, M + 4) x [N, N + 6) (derived from [M + (-1), M + 4) x [N, N + 4 + 2)).
The mapping between the global coordinates i, j and the local indices into the
buffer is shown below:You do not need to be concerned about these fundamental elements as a user.
Taichi automatically does all inference and global/block-local mapping.
That is, Taichi will preallocate a CUDA shared memory buffer of size 5x6, preload a's contents into this buffer, then replace all a (global memory) accesses with the buffer in the loop body. While this basic example does not change a itself, if a block-cached field is written, Taichi produces code that returns the buffer to global memory.noteBLS does not come cheap. Remember that BLS is intended for stencil computations with a high number of overlapping global memory accesses. If this is not the case, pre-loading and post-storing may actually degrade performance.Furthermore, recent generations of Nvidia GPU cards have closed the read-only access gap between global memory and shared memory. Currently, we discovered that BLS is more effective for storing the destinations of atomic actions.As a general rule of thumb, we recommend running benchmarks to determine whether or not you should enable BLS.Offline CacheThe first time a Taichi kernel is called, it is implicitly compiled. To decrease the cost in subsequent function calls, the compilation results are retained in an online in-memory cache. The kernel can be loaded and launched immediately as long as it remains unaltered. When the application exits, the cache is no longer accessible. When you restart the programme, Taichi must recompile all kernel routines and rebuild the online in-memory cache. Because of the compilation overhead, the first launch of a Taichi function can typically be slow.We address this problem by introducing the offline cache feature, which dumps and saves the compilation cache on disk for future runs. The first launch overhead can be drastically reduced in repeated runs. Taichi now constructs and maintains an offline cache by default, as well as providing several options in ti.init() for configuring the offline cache behavior.offline_cache: bool: Enables or disables offline cache. Default: True.offline_cache_file_path: str: Directory holding the offline cached files. Default: 'C:\taichi_cache\ticache\' on Windows and '~/.cache/taichi/ticache/' on unix-like systems. Directories are automatically populated.offline_cache_max_size_of_files: int32: Maximum size of the cached files in Bytes. Default: 100MB. A cleaning process is triggered when the size of the cached files exceeds this limit.offline_cache_cleaning_policy: str: Policy about how to replace outdated files in the cache. Options: 'never', 'version', 'lru' and 'fifo'. Default: 'lru'.'never': Never cleans and keeps all the cached files regardless of the offline_cache_max_size_of_files configuration;'version': Discards only the old-version cached files with respect to the kernel function;'lru': Discards the cached files least used recently;'fifo': Discards the cached files added in the earliest.To verify the effect, run some examples twice and observe the launch overhead:
noteIf your code behaves abnormally, disable offline cache by setting the environment variable TI_OFFLINE_CACHE=0 or offline_cache=False in the ti.init() method call and file an issue with us on Taichi's GitHub repo.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?For-loop decoratorsBackground: Thread hierarchy of GPUsExample: Tuning the block-level parallelism of a for-loopData layoutsLocal Storage OptimizationsThread Local Storage (TLS)Block Local Storage (BLS)Offline CacheCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Debugging | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Debugging>>DebuggingVersion: v1.6.0On this pageDebuggingTo aid with debugging your parallel programs, Taichi has the following mechanisms:print in the Taichi scope checks the value of a variable.Serialization of your program or a specific parallel for loop.Activated with ti.init(debug=True), debug mode detects out-of-bound array accesses.Static or non-static assert verifies an assertion condition at compile time or runtime respectively.sys.tracebacklimit produces a conciser traceback.Runtime print in Taichi scopeOne of the most naive ways to debug code is to print particular messages to check how your code runs in different states. You can call print() in the Taichi scope to debug your program:print(*args, sep='', end='\n')CopyWhen passed into a runtime print() in the Taichi scope, args can take string literal, scalar, vector, and matrix expressions.For example:@ti.kerneldef inside_taichi_scope():    x = 256    print('hello', x)    #=> hello 256    print('hello', x * 2 + 200)    #=> hello 712    print('hello', x, sep='')    #=> hello256    print('hello', x, sep='', end='')    print('world', x, sep='')    #=> hello256world256    m = ti.Matrix([[2, 3, 4], [5, 6, 7]])    print('m =', m)    #=> m = [[2, 3, 4], [5, 6, 7]]    v = ti.Vector([3, 4])    print('v =', v)    #=> v = [3, 4]    ray = ti.Struct({        "ori": ti.Vector([0.0, 0.0, 0.0]),        "dir": ti.Vector([0.0, 0.0, 1.0]),        "len": 1.0        })    # print(ray)    # Print a struct directly in Taichi-scope has not been supported yet    # Instead, use:    print('ray.ori =', ray.ori, ', ray.dir =', ray.dir, ', ray.len =', ray.len)    #=> ray.ori = [0.0, 0.0, 0.0], ray.dir = [0.0, 0.0, 1.0], ray.len = 1.0CopyApplicable backendsprint in the Taichi scope is supported on the CPU, CUDA, and Vulkan backends only.noteTo enable printing on Vulkan, pleasemake sure the validation layer is installed via vulkan sdk.turn on debug mode via ti.init(debug=True).Printing is not supported on the macOS Vulkan backend.Printing comma-separated strings, f-strings, or formatted stringsIn Taichi scope, you can print both scalar and matrix values using the print function. There are multiple ways to format your output, including comma-separated strings, f-strings, and formatted strings via the str.format() method.For instance, suppose you have a scalar field a and want to print its value. Here are some examples:import taichi as titi.init(arch=ti.cpu)a = ti.field(ti.f32, 4)@ti.kerneldef print_scalar():    a[0] = 1.0    # comma-separated string    print('a[0] =', a[0])    # f-string    print(f'a[0] = {a[0]}')    # with format specifier    print(f'a[0] = {a[0]:.1f}')    # without conversion    print(f'a[0] = {a[0]:.1}')    # with self-documenting expressions (Python 3.8+)    print(f'{a[0] = :.1f}')    # formatted string via `str.format()` method    print('a[0] = {}'.format(a[0]))    # with format specifier    print('a[0] = {:.1f}'.format(a[0]))    # without conversion    print('a[0] = {:.1}'.format(a[0]))    # with positional arguments    print('a[3] = {3:.3f}, a[2] = {2:.2f}, a[1] = {1:.1f}, a[0] = {0:.0f}'.format(a[0], a[1], a[2], a[3]))CopyIf you have a matrix field m, you can print it as well. Here are some examples:@ti.kerneldef print_matrix():    m = ti.Matrix([[2e1, 3e2, 4e3], [5e4, 6e5, 7e6]], ti.f32)    # comma-separated string    print('m =', m)    # f-string    print(f'm = {m}')    # with format specifier    print(f'm = {m:.1f}')    # without conversion    print(f'm = {m:.1}')    # with self-documenting expressions    print(f'{m = :g}')    # formatted string via `str.format()` method    print('m = {}'.format(m))    # with format specifier    print('m = {:e}'.format(m))    # without conversion    print('m = {:.1}'.format(m))CopynoteBuilding formatted strings using the % operator is currently not supported in Taichi.Compile-time ti.static_printIt can be useful to print Python objects and their properties like data types or SNodes in the Taichi scope. Similar to ti.static, which makes the compiler evaluate an argument at compile time (see the Metaprogramming for more information), ti.static_print prints compile-time constants in the Taichi scope:x = ti.field(ti.f32, (2, 3))y = 1@ti.kerneldef inside_taichi_scope():    ti.static_print(y)    # => 1    ti.static_print(x.shape)    # => (2, 3)    ti.static_print(x.dtype)    # => DataType.float32    for i in range(4):        ti.static_print(i.dtype)        # => DataType.int32        # Only print onceCopyIn the Taichi scope, ti.static_print acts similarly to print. But unlike print, ti.static_print outputs the expression only once at compile time, incurring no runtime penalty.Serial executionBecause threads are processed in random order, Taichi's automated parallelization may result in non-deterministic behaviour. Serializing program execution may be advantageous for debugging purposes, such as achieving reproducible results or identifying data races. You have the option of serialising the complete Taichi program or a single for loop.Serialize an entire Taichi programIf you choose CPU as the backend, you can set cpu_max_num_threads=1 when initializing Taichi to serialize the program. Then the program runs on a single thread and its behavior becomes deterministic. For example:ti.init(arch=ti.cpu, cpu_max_num_threads=1)CopyIf your program works well in serial but fails in parallel, check if there are parallelization-related issues, such as data races.Serialize a specified parallel for loopBy default, Taichi automatically parallelizes the for loops at the outermost scope in a Taichi kernel. But some scenarios require serial execution. In this case, you can prevent automatic parallelization with ti.loop_config(serialize=True). Note that only the outermost for loop that immediately follows this line is serialized. To illustrate:import taichi as titi.init(arch=ti.cpu)n = 1024val = ti.field(dtype=ti.i32, shape=n)val.fill(1)@ti.kerneldef prefix_sum():    ti.loop_config(serialize=True) # Serializes the next for loop    for i in range(1, n):        val[i] += val[i - 1]    for i in range(1, n):  # Parallel for loop        val[i] += val[i - 1]prefix_sum()print(val)Copynoteti.loop_config works only for the range-for loop at the outermost scope.Out-of-bound array accessThe array index out of bounds error occurs frequently. However, Taichi disables bounds checking by default and continues without generating a warning. As a result, a program with such an issue may provide incorrect results or possibly cause segmentation faults, making debugging difficult.Taichi detects array index out of bound errors in debug mode. You can activate this mode by setting debug=True in the ti.init() call:import taichi as titi.init(arch=ti.cpu, debug=True)f = ti.field(dtype=ti.i32, shape=(32, 32))@ti.kerneldef test() -> ti.i32:    return f[0, 73]print(test())CopyThe code snippet above raises a TaichiAssertionError because you are trying to access elements from a field of shape (32, 32) with indices [0, 73].noteAutomatic bound checks are supported on the CPU and CUDA beckends only.Your program performance may worsen if you set debug=True.Runtime assert in Taichi scopeYou can use assert statements in the Taichi scope to verify the assertion conditions. If an assertion fails, the program throws a TaichiAssertionError.noteassert is currently supported on the CPU, CUDA, and Metal backends.Ensure that you activate debug mode before using assert statements in the Taichi scope:import taichi as titi.init(arch=ti.cpu, debug=True)x = ti.field(ti.f32, 128)x.fill(-1)@ti.kerneldef do_sqrt_all():    for i in x:        assert x[i] >= 0, f"The {i}-th element cannot be negative"        x[i] = ti.sqrt(x[i])do_sqrt_all()CopyWhen you are done with debugging, set debug=False. Then, the program ignores all assert statements in the Taichi scope, which can avoid additional runtime overhead.Compile-time ti.static_assertBesides ti.static_print, Taichi also provides a static version of assert: ti.static_assert, which may be used to create assertions on data types, dimensionality, and shapes.ti.static_assert(cond, msg=None)CopyIt works whether or not debug=True is used. A false ti.static_assert statement, like a false assert statement in the Python scope, causes an AssertionError, as shown below:@ti.funcdef copy(dst: ti.template(), src: ti.template()):    ti.static_assert(dst.shape == src.shape, "copy() needs src and dst fields to be same shape")    for I in ti.grouped(src):        dst[I] = src[I]CopyConciser tracebacks in Taichi scopeTaichi reports the traceback of an error in the Taichi scope. For example, the code snippet below triggers an AssertionError, with a lengthy traceback message:import taichi as titi.init()@ti.funcdef func3():    ti.static_assert(1 + 1 == 3)@ti.funcdef func2():    func3()@ti.funcdef func1():    func2()@ti.kerneldef func0():    func1()func0()CopyOutput:Traceback (most recent call last):  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer_utils.py", line 23, in __call__    return method(ctx, node)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer.py", line 342, in build_Call    node.ptr = node.func.ptr(*args, **keywords)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/impl.py", line 471, in static_assert    assert condAssertionErrorDuring handling of the above exception, another exception occurred:Traceback (most recent call last):  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer_utils.py", line 23, in __call__    return method(ctx, node)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer.py", line 360, in build_Call    node.ptr = node.func.ptr(*args, **keywords)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/kernel_impl.py", line 59, in decorated    return fun.__call__(*args)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/kernel_impl.py", line 178, in __call__    ret = transform_tree(tree, ctx)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/transform.py", line 8, in transform_tree    ASTTransformer()(ctx, tree)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer_utils.py", line 26, in __call__    raise e  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer_utils.py", line 23, in __call__    return method(ctx, node)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer.py", line 488, in build_Module    build_stmt(ctx, stmt)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer_utils.py", line 26, in __call__    raise e  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer_utils.py", line 23, in __call__    return method(ctx, node)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer.py", line 451, in build_FunctionDef    build_stmts(ctx, node.body)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer.py", line 1086, in build_stmts    build_stmt(ctx, stmt)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer_utils.py", line 26, in __call__    raise e  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer_utils.py", line 23, in __call__    return method(ctx, node)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer.py", line 964, in build_Expr    build_stmt(ctx, node.value)  File "/Users/lanhaidong/taichi/taichi/python/taichi/lang/ast/ast_transformer_utils.py", line 32, in __call__    raise TaichiCompilationError(msg)taichi.lang.exception.TaichiCompilationError: File "misc/demo_traceback.py", line 10:    ti.static_assert(1 + 1 == 3)    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^AssertionError:...CopyIt takes time to read through the message. In addition, many stack frames reveal implementation details, which are irrelevant to debugging.Taichi allows you to access a conciser and more intuitive version of traceback messages via sys.tracebacklimit:import taichi as tiimport syssys.tracebacklimit=0...CopyThe traceback contains the following information only:AssertionErrorDuring handling of the above exception, another exception occurred:taichi.lang.exception.TaichiCompilationError: File "misc/demo_traceback.py", line 10:    ti.static_assert(1 + 1 == 3)    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^AssertionError:...CopyHowever, always unset sys.tracebacklimit and submit the full traceback messages when filing an issue with us.Debugging tipsThe above built-in tools cannot guarantee a smooth debugging experience, though. Here, we conclude some common bugs that one may encounter in a
Taichi program.Static type systemTaichi translates Python code into a statically typed language for high performance. Therefore, code in the Taichi scope may behave differently from native Python code, especially when it comes to variable types.In the Taichi scope, the type of a variable is determined upon initialization and never changes afterwards.Although Taichi's static typing system delivers a better performance, it may lead to unexpected results if you fail to specify the correct types. For example, the code below leads to an unexpected result due to a misuse of Taichi's static typing system. The Taichi compiler shows a warning::@ti.kerneldef buggy():    ret = 0  # 0 is an integer, so `ret` is typed as int32    for i in range(3):        ret += 0.1 * i  # i32 += f32, the result is still stored in int32!    print(ret)  # will show 0buggy()CopyOutput:[W 06/27/20 21:43:51.853] [type_check.cpp:visit@66] [$19] Atomic add (float32 to int32) may lose precision.CopyThis means that a precision loss occurs when Taichi converts a float32 result to int32. The solution is to initialize ret as a floating-point value:@ti.kerneldef not_buggy():    ret = 0.0  # 0 is a floating point number, so `ret` is typed as float32    for i in range(3):        ret += 0.1 * i  # f32 += f32. OK!    print(ret)  # will show 0.6not_buggy()CopyAdvanced OptimizationBy default, Taichi runs a number of advanced IR optimizations to maximize the performance of your Taichi kernels. However, advanced optimizations may occasionally lead to compilation errors, such as:RuntimeError: [verify.cpp:basic_verify@40] stmt 8 cannot have operand 7.You can use the ti.init(advanced_optimization=False) setting to turn off advanced optimizations and see if it makes a difference. If this issue persists, feel free to report it on GitHub.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Runtime print in Taichi scopeApplicable backendsPrinting comma-separated strings, f-strings, or formatted stringsCompile-time ti.static_printSerial executionSerialize an entire Taichi programSerialize a specified parallel for loopOut-of-bound array accessRuntime assert in Taichi scopeCompile-time ti.static_assertConciser tracebacks in Taichi scopeDebugging tipsStatic type systemAdvanced OptimizationCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Tutorial: Run Taichi programs in C++ applications | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTutorial: Run Taichi programs in C++ applicationsTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Deployment>>Tutorial: Run Taichi programs in C++ applicationsVersion: v1.6.0On this pageTutorial: Run Taichi programs in C++ applicationsTaichi makes it easy to write high-performance programs with efficient parallelism, but in many applications we cannot simply deploy the Python scripts. Taichi offers a runtime library (TiRT) with a C interface as well as its C++ wrapper, so your Taichi kernels can be launched in any native application. In this tutorial, we'll walk through the steps to deploy a Taichi program in a C++ application.OverviewIn Python, when you call a function decorated with @ti.kernel, Taichi immediately compiles the kernel and sends it to the device for execution. This is called just-in-time (JIT) compilation. However, in general, we don't want to compile the kernels on a mobile phone, or to expose the source code to the users. For this Taichi introduced ahead-of-time (AOT) compilation so that you can compile kernels on a development machine, and launch them on user devices via TiRT.In summary, running a Taichi program in C++ applications involves two steps:Compile Taichi kernels from Python and save the artifacts.Load the AOT modules with TiRT and launch them in your applications.Although this tutorial only demonstrates integrating Taichi in a C++ application, the C interface allows you to integrate TiRT with many other programming languages including C/C++, Swift, Rust, C# (via P/Invoke) and Java (via JNI).Quick-StartIn this section, we will write a Taichi kernel for generating images for Julia fractal and deploy it in a C++ application. The following shows the project layout. Next, we will walk through the steps to see what they do..├── cmake│   └── FindTaichi.cmake    // finds the Taichi runtime library├── CMakeLists.txt          // builds the project├── app.py                  // defines and compiles the Taichi kernel├── app.cpp                 // deploys the compiled artifact to the application└── module.tcm              // the compiled Taichi kernel artifactCopyBefore we start, it is recommended to install Taichi through taichi-nightly Python wheels using the following command. Be aware that there's no strong version compatibility enforced yet, so it's highly recommended to use the Taichi built from exactly the same commit.pip install -i https://pypi.taichi.graphics/simple/ taichi-nightlyCopy1. Compile Taichi kernel in Python scriptWe firstly write a Python script named app.py, which compiles the Taichi kernel as an artifact. Save the following code to your local machine and run the program, you will obtain an archived module.tcm in the same directory as app.py.import taichi as titi.init(arch=ti.vulkan)if ti.lang.impl.current_cfg().arch != ti.vulkan:    raise RuntimeError("Vulkan is not available.")@ti.kerneldef paint(n: ti.u32, t: ti.f32, pixels: ti.types.ndarray(dtype=ti.f32, ndim=2)):    for i, j in pixels:  # Parallelized over all pixels        c = ti.Vector([-0.8, ti.cos(t) * 0.2])        z = ti.Vector([i / n - 1, j / n - 0.5]) * 2        iterations = 0        while z.norm() < 20 and iterations < 50:            z = ti.Vector([z[0]**2 - z[1]**2, z[1] * z[0] * 2]) + c            iterations += 1        pixels[i, j] = 1 - iterations * 0.02mod = ti.aot.Module(ti.vulkan)mod.add_kernel(paint)mod.archive("module.tcm")CopyLet's dive into the code example to see what happened.We initialize Taichi specifing the backend as ti.vulkan at the beginning. Considering that Taichi will fall back to CPU device if the target architecture is unavailable, we check if the current backend meets our requirement.ti.init(arch=ti.vulkan)if ti.lang.impl.current_cfg().arch != ti.vulkan:    raise RuntimeError("Vulkan is not available.")CopyThen, we define our Taichi kernel for computing each pixel in our program. A Taichi kernel describes two aspects of a computer program: the computation itself, and the data it operates on. Because we don't know what kind of data will be fed into the kernel before execution, we have to clearly annotate the argument types for the AOT compiler.Taichi AOT module supports the following argument types: ti.i32, ti.f32, ti.Ndarray. Despite integers and floating-point numbers, we have a commonly-used data container called Ndarray. It's similar to an ndarray in NumPy, or a Tensor in PyTorch. It can be multidimensional and is laid out continuously in memory. If you have experienced the multidimensional arrays in C++, You can treat it as a nested array type like float[6][14].Our Taichi kernel accepts an integer n, a float-pointing number t and a 2-dimensional Ndarray pixels as arguments. Each element of pixels is a floating-point number ranges from 0.0 to 1.0.@ti.kerneldef paint(n: ti.i32, t: ti.f32, pixels: ti.types.ndarray(dtype=ti.f32, ndim=2)):    for i, j in pixels:  # Parallelized over all pixels        c = ti.Vector([-0.8, ti.cos(t) * 0.2])        z = ti.Vector([i / n - 1, j / n - 0.5]) * 2        iterations = 0        while z.norm() < 20 and iterations < 50:            z = ti.Vector([z[0]**2 - z[1]**2, z[1] * z[0] * 2]) + c            iterations += 1        pixels[i, j] = 1 - iterations * 0.02CopyFinally, we compile the kernel into an artifact. The following piece of code initializes the AOT module and add the kernel to the module. The compiled artifact is saved as module.tcm in the working directory.mod = ti.aot.Module(ti.vulkan)mod.add_kernel(paint)mod.archive("module.tcm")Copy2. Work with Taichi C-API in C++ programWe are now done with Python and well prepared to build our application. The compiled artifacts saved as module.tcm and the Taichi Runtime Libirary (TiRT) are all we need. TiRT provides a fundamental C interface to help achieve optimal portability, however we also provide a header-only C++ wrapper to save you from writing verbose C code. For simplicity purpose, we'll stick with the C++ wrapper in this tutorial.Firstly, we need to include the C++ wrapper header of Taichi C-API.#include <taichi/cpp/taichi.hpp>CopyNext, create a Taichi runtime with target architecture. We will further load the compiled artifacts from module.tcm and load our paint kernel from the module.ti::Runtime runtime(TI_ARCH_VULKAN);ti::AotModule aot_module = runtime.load_aot_module("module.tcm");ti::Kernel kernel_paint = aot_module.get_kernel("paint");CopyThe paint kernel accepts three arguments, and thus we need to declare corresponding variables in C++ program. We allocate memory through TiRT's allocate_ndarray interface for the pixels, the width and the height are set to 2 * n and n respectively, and the element shape is set to 1.int n = 320;float t = 0.0f;ti::NdArray<float> pixels = runtime.allocate_ndarray<float>({(uint32_t)(2 * n), (uint32_t)n}, {1}, true);CopyThen, we specify the arguments for the kernel, where the index for kernel_paint indicates the position in the kernel's argument list. Launch the kernel, and wait for the Taichi kernel process to finish.kernel_paint[0] = n;kernel_paint[1] = t;kernel_paint[2] = pixels;kernel_paint.launch();runtime.wait();CopyFinally, the pixels Ndarray holds the kernel output. Before we read the output pixel data, we must map a device memory to a user-addressable space. The image data is saved in a plain text ppm format with a utility function save_ppm. For the ppm format, please refer to Wikipedia.auto pixels_data = (const float*)pixels.map();save_ppm(pixels_data, 2 * n, n, "result.ppm");pixels.unmap();CopyThe complete C++ source code is shown below, which is saved as app.cpp in the same directory as app.py.#include <fstream>#include <taichi/cpp/taichi.hpp>void save_ppm(const float* pixels, uint32_t w, uint32_t h, const char* path) {  std::fstream f(path, std::ios::out | std::ios::trunc);  f << "P3\n" << w << ' ' << h << "\n255\n";  for (int j = h - 1; j >= 0; --j) {    for (int i = 0; i < w; ++i) {      f << static_cast<uint32_t>(255.999 * pixels[i * h + j]) << ' '        << static_cast<uint32_t>(255.999 * pixels[i * h + j]) << ' '        << static_cast<uint32_t>(255.999 * pixels[i * h + j]) << '\n';    }  }  f.flush();  f.close();}int main(int argc, const char** argv) {  ti::Runtime runtime(TI_ARCH_VULKAN);  ti::AotModule aot_module = runtime.load_aot_module("module.tcm");  ti::Kernel kernel_paint = aot_module.get_kernel("paint");  int n = 320;  float t = 0.0f;  ti::NdArray<float> pixels = runtime.allocate_ndarray<float>({(uint32_t)(2 * n), (uint32_t)n}, {1}, true);  kernel_paint[0] = n;  kernel_paint[1] = t;  kernel_paint[2] = pixels;  kernel_paint.launch();  runtime.wait();  auto pixels_data = (const float*)pixels.map();  save_ppm(pixels_data, 2 * n, n, "result.ppm");  pixels.unmap();  return 0;}Copy3. Build project with CMakeCMake is utilized to build our project, and we introduce the utility CMake module cmake/FindTaichi.cmake. It firstly find Taichi installation directory according to the environment variable TAICHI_C_API_INSTALL_DIR, without which CMake will find the Taichi library in Python wheel. Then, it will define the Taichi::Runtime target which is linked to our project.The utility module is further included in the CMakeLists.txt which looks like as below.cmake_minimum_required(VERSION 3.17)set(TAICHI_AOT_APP_NAME TaichiAot)project(${TAICHI_AOT_APP_NAME} LANGUAGES C CXX)set(CMAKE_CXX_STANDARD 17)set(CMAKE_CXX_STANDARD_REQUIRED ON)# Declare executable target.add_executable(${TAICHI_AOT_APP_NAME} app.cpp)target_include_directories(${TAICHI_AOT_APP_NAME} PUBLIC ${TAICHI_C_API_INSTALL_DIR}/include)# Find and link Taichi runtime library.set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)find_package(Taichi REQUIRED)target_link_libraries(${TAICHI_AOT_APP_NAME} Taichi::Runtime)CopyBuild the project with the commands:cmake -B buildcmake --build buildCopyRun the executable TaichiAOT demo:./build/TaichiAOTCopyAn image of Julia fractal shown below is saved as result.ppm in the project directory.FAQMap your Taichi data types from Python to C++PythonC++scalarC++ scalar typeti.vector / ti.matrixstd::vectorti.ndarrayti::Ndarrayti.Textureti::Textureti.fieldWIPDoes Taichi support device import/export?Yes! We understand that in real applications it's pretty common to hook Taichi in your existing Vulkan pipeline. As a result, you can choose to import an external device for Taichi to use, or export a device that Taichi creates to share with the external applicationWhich backends & hardware are supported?Currently ti.vulkan, ti.opengl, ti.x86 and ti.cuda are supported. ti.metal is not yet supported.How can I debug a C++ application with embedded Taichi?Check ti_get_last_error() whenever you call a Taichi C API.Enable backward-cpp in your application to locate the source of crashes. E.g. https://github.com/taichi-dev/taichi-aot-demo/pull/69Get values of ndarrays back on host using ndarray.read(), e.g. https://github.com/taichi-dev/taichi-aot-demo/pull/57/files#diff-d94bf1ff63835d9cf87e700ca3c37d1e9a3c09e5994944db2adcddf132a71d0cR32Enable printing in shaders, e.g. https://github.com/taichi-dev/taichi-aot-demo/pull/55Does Taichi support generating shaders for different deployment targets?Yes, you can specify the target device capabilities in ti.aot.Module(arch=, caps=[]). Future support for compiling to a different architecture from ti.init() is planned.Are Taichi compiled artifacts versioned?There is no official versioning yet (pre-release). For now, use Taichi and C++ runtime built from the same commit for compatibility.Can I hook Taichi into a render pipeline?Yes! If you already have a rendering pipeline, you can interop with Taichi via https://docs.taichi-lang.org/docs/taichi_vulkan.If you don't have one already, please check out our demos at https://github.com/taichi-dev/taichi-aot-demoI just want to use raw shaders generated by Taichi. Where can I find them?Yes, you can find the raw shaders generated by Taichi in the target folder of the aot save. However, it's important to note that launching Taichi shaders requires a special setup that relates to the implementation details in Taichi and may change without notice. If you have strict size limitations for your application and the provided runtime is too large to fit, you may consider writing a minimal Taichi runtime in C that consumes these raw shaders.Can I build the libtaichi_c_api.so from source?Usually, for simplicity and stability, we recommend using the official nightly taichi wheels and the c_api shipped inside the wheel. But if you want a runtime library with special build configuration:TAICHI_CMAKE_ARGS="-DTI_WITH_VULKAN:BOOL=ON -DTI_WITH_C_API:BOOL=ON" python setup.py develop# Other commonly used CMake options- TI_WITH_OPENGL- TI_WITH_CPU- TI_WITH_CUDACopyYou can find the built libtaichi_c_api.so and its headers in the _skbuild/ folder.Taichi/C API Reference Manualhttps://docs.taichi-lang.org/docs/taichi_core#api-referenceWhen do I need to recompile my artifacts?It is recommended to recompile the Taichi artifacts when changes are made to the following:Updates to the kernels and their corresponding launch logic in PythonThe need to use a newer version of either the Python Taichi or runtime libraryThe target device has a different set of capabilitiesUpdating some Python constants that are encoded as constants in the Taichi compiled artifactsPlease note that due to the nature of Ndarray handling in Taichi, the generated shaders can be used for Ndarrays with different shapes as long as their ranks match. This is a convenient feature if you need to use a single set of shaders for various scenarios, such as different screen sizes on Android phones.How can I set values for ndarrays in C++?In the C++ wrapper we provide these convenient read/write() methods on NdArray class. https://github.com/taichi-dev/taichi/blob/master/c_api/include/taichi/cpp/taichi.hpp#L192-L215In C API you can allocate your memory as host accessible and then use map/unmap. https://docs.taichi-lang.org/docs/taichi_coreEdit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?OverviewQuick-Start1. Compile Taichi kernel in Python script2. Work with Taichi C-API in C++ program3. Build project with CMakeFAQMap your Taichi data types from Python to C++Does Taichi support device import/export?Which backends & hardware are supported?How can I debug a C++ application with embedded Taichi?Does Taichi support generating shaders for different deployment targets?Are Taichi compiled artifacts versioned?Can I hook Taichi into a render pipeline?I just want to use raw shaders generated by Taichi. Where can I find them?Can I build the libtaichi_c_api.so from source?Taichi/C API Reference ManualWhen do I need to recompile my artifacts?How can I set values for ndarrays in C++?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Core Functionality | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APICore FunctionalityVulkan Backend FeaturesMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Taichi Runtime C-API>>Core FunctionalityVersion: v1.6.0On this pageCore FunctionalityTaichi Core exposes all necessary interfaces for offloading the AOT modules to Taichi. The following is a list of features that are available regardless of your backend. The corresponding APIs are still under development and subject to change.AvailabilityTaichi C-API intends to support the following backends:BackendOffload TargetMaintenance TierStabilized?VulkanGPUTier 1YesMetalGPU (macOS, iOS)Tier 2NoCUDA (LLVM)GPU (NVIDIA)Tier 2NoCPU (LLVM)CPUTier 2NoOpenGLGPUTier 2NoOpenGL ESGPUTier 2NoDirectX 11GPU (Windows)N/ANoThe backends with tier-1 support are being developed and tested more intensively. And most new features will be available on Vulkan first because it has the most outstanding cross-platform compatibility among all the tier-1 backends.
For the backends with tier-2 support, you should expect a delay in the fixes to minor issues.For convenience, in the following text and other C-API documents, the term host refers to the user of the C-API; the term device refers to the logical (conceptual) compute device, to which Taichi's runtime offloads its compute tasks. A device may not be a physical discrete processor other than the CPU and the host may not be able to access the memory allocated on the device.Unless otherwise specified, device, backend, offload target, and GPU are interchangeable; host, user code, user procedure, and CPU are interchangeable.HowToThe following section provides a brief introduction to the Taichi C-API.Create and destroy a Runtime InstanceYou must create a runtime instance before working with Taichi, and only one runtime per thread. Currently, we do not officially claim that multiple runtime instances can coexist in a process, but please feel free to file an issue with us if you run into any problem with runtime instance coexistence.// Create a Taichi Runtime on Vulkan device at index 0.TiRuntime runtime = ti_create_runtime(TI_ARCH_VULKAN, 0);CopyWhen your program runs to the end, ensure that:You destroy the runtime instance,All related resources are destroyed before the TiRuntime itself.ti_destroy_runtime(runtime);CopyAllocate and free memoryAllocate a piece of memory that is visible only to the device. On the GPU backends, it usually means that the memory is located in the graphics memory (GRAM).TiMemoryAllocateInfo mai {};mai.size = 1024; // Size in bytes.mai.usage = TI_MEMORY_USAGE_STORAGE_BIT;TiMemory memory = ti_allocate_memory(runtime, &mai);CopyAllocated memory is automatically freed when the related TiRuntime is destroyed. You can also manually free the allocated memory.ti_free_memory(runtime, memory);CopyAllocate host-accessible memoryBy default, memory allocations are physically or conceptually local to the offload target for performance reasons. You can configure the TiMemoryAllocateInfo to enable host access to memory allocations. But please note that host-accessible allocations may slow down computation on GPU because of the limited bus bandwidth between the host memory and the device.You must set host_write to TI_TRUE to allow zero-copy data streaming to the memory.TiMemoryAllocateInfo mai {};mai.size = 1024; // Size in bytes.mai.host_write = TI_TRUE;mai.usage = TI_MEMORY_USAGE_STORAGE_BIT;TiMemory steaming_memory = ti_allocate_memory(runtime, &mai);// ...std::vector<uint8_t> src = some_random_data_source();void* dst = ti_map_memory(runtime, steaming_memory);std::memcpy(dst, src.data(), src.size());ti_unmap_memory(runtime, streaming_memory);CopyTo read data back to the host, host_read must be set to TI_TRUE.TiMemoryAllocateInfo mai {};mai.size = 1024; // Size in bytes.mai.host_read = TI_TRUE;mai.usage = TI_MEMORY_USAGE_STORAGE_BIT;TiMemory read_back_memory = ti_allocate_memory(runtime, &mai);// ...std::vector<uint8_t> dst(1024);void* src = ti_map_memory(runtime, read_back_memory);std::memcpy(dst.data(), src, dst.size());ti_unmap_memory(runtime, read_back_memory);ti_free_memory(runtime, read_back_memory);CopyYou can set host_read and host_write at the same time.Load and destroy a Taichi AOT moduleYou can load a Taichi AOT module from the filesystem.TiAotModule aot_module = ti_load_aot_module(runtime, "/path/to/aot/module");Copy/path/to/aot/module should point to the directory that contains a metadata.json.You can destroy an unused AOT module, but please ensure that there is no kernel or compute graph related to it pending to ti_flush.ti_destroy_aot_module(aot_module);CopyLaunch kernels and compute graphsYou can extract kernels and compute graphs from an AOT module. Kernel and compute graphs are a part of the module, so you don't have to destroy them.TiKernel kernel = ti_get_aot_module_kernel(aot_module, "foo");TiComputeGraph compute_graph = ti_get_aot_module_compute_graph(aot_module, "bar");CopyYou can launch a kernel with positional arguments. Please ensure the types, the sizes and the order matches the source code in Python.TiNdArray ndarray{};ndarray.memory = get_some_memory();ndarray.shape.dim_count = 1;ndarray.shape.dims[0] = 16;ndarray.elem_shape.dim_count = 2;ndarray.elem_shape.dims[0] = 4;ndarray.elem_shape.dims[1] = 4;ndarray.elem_type = TI_DATA_TYPE_F32;std::array<TiArgument, 3> args{};TiArgument& arg0 = args[0];arg0.type = TI_ARGUMENT_TYPE_I32;arg0.value.i32 = 123;TiArgument& arg1 = args[1];arg1.type = TI_ARGUMENT_TYPE_F32;arg1.value.f32 = 123.0f;TiArgument& arg2 = args[2];arg2.type = TI_ARGUMENT_TYPE_NDARRAY;arg2.value.ndarray = ndarray;ti_launch_kernel(runtime, kernel, args.size(), args.data());CopyYou can launch a compute graph in a similar way. But additionally please ensure the argument names matches those in the Python source.std::array<TiNamedArgument, 3> named_args{};TiNamedArgument& named_arg0 = named_args[0];named_arg0.name = "foo";named_arg0.argument = args[0];TiNamedArgument& named_arg1 = named_args[1];named_arg1.name = "bar";named_arg1.argument = args[1];TiNamedArgument& named_arg2 = named_args[2];named_arg2.name = "baz";named_arg2.argument = args[2];ti_launch_compute_graph(runtime, compute_graph, named_args.size(), named_args.data());CopyWhen you have launched all kernels and compute graphs for this batch, you should ti_flush and ti_wait for the execution to finish.ti_flush(runtime);ti_wait(runtime);CopyWARNING This part is subject to change. We will introduce multi-queue in the future.API ReferenceAlias TiBoolStable since Taichi version: 1.4.0// alias.booltypedef uint32_t TiBool;CopyA boolean value. Can be either TI_TRUE or TI_FALSE. Assignment with other values could lead to undefined behavior.Definition TI_FALSEStable since Taichi version: 1.4.0// definition.false#define TI_FALSE 0CopyA condition or a predicate is not satisfied; a statement is invalid.Definition TI_TRUEStable since Taichi version: 1.4.0// definition.true#define TI_TRUE 1CopyA condition or a predicate is satisfied; a statement is valid.Alias TiFlagsStable since Taichi version: 1.4.0// alias.flagstypedef uint32_t TiFlags;CopyA bit field that can be used to represent 32 orthogonal flags. Bits unspecified in the corresponding flag enum are ignored.Enumerations and bit-field flags in the C-API have a TI_XXX_MAX_ENUM case to ensure the enum has a 32-bit range and in-memory size. It has no semantical impact and can be safely ignored.Definition TI_NULL_HANDLEStable since Taichi version: 1.4.0// definition.null_handle#define TI_NULL_HANDLE 0CopyA sentinal invalid handle that will never be produced from a valid call to Taichi C-API.Handle TiRuntimeStable since Taichi version: 1.4.0// handle.runtimetypedef struct TiRuntime_t* TiRuntime;CopyTaichi runtime represents an instance of a logical backend and its internal dynamic state. The user is responsible to synchronize any use of TiRuntime. The user must not manipulate multiple TiRuntimes in the same thread.Handle TiAotModuleStable since Taichi version: 1.4.0// handle.aot_moduletypedef struct TiAotModule_t* TiAotModule;CopyAn ahead-of-time (AOT) compiled Taichi module, which contains a collection of kernels and compute graphs.Handle TiMemoryStable since Taichi version: 1.4.0// handle.memorytypedef struct TiMemory_t* TiMemory;CopyA contiguous allocation of device memory.Handle TiImageStable since Taichi version: 1.4.0// handle.imagetypedef struct TiImage_t* TiImage;CopyA contiguous allocation of device image.Handle TiKernelStable since Taichi version: 1.4.0// handle.kerneltypedef struct TiKernel_t* TiKernel;CopyA Taichi kernel that can be launched on the offload target for execution.Handle TiComputeGraphStable since Taichi version: 1.4.0// handle.compute_graphtypedef struct TiComputeGraph_t* TiComputeGraph;CopyA collection of Taichi kernels (a compute graph) to launch on the offload target in a predefined order.Enumeration TiErrorStable since Taichi version: 1.4.0// enumeration.errortypedef enum TiError {  TI_ERROR_SUCCESS = 0,  TI_ERROR_NOT_SUPPORTED = -1,  TI_ERROR_CORRUPTED_DATA = -2,  TI_ERROR_NAME_NOT_FOUND = -3,  TI_ERROR_INVALID_ARGUMENT = -4,  TI_ERROR_ARGUMENT_NULL = -5,  TI_ERROR_ARGUMENT_OUT_OF_RANGE = -6,  TI_ERROR_ARGUMENT_NOT_FOUND = -7,  TI_ERROR_INVALID_INTEROP = -8,  TI_ERROR_INVALID_STATE = -9,  TI_ERROR_INCOMPATIBLE_MODULE = -10,  TI_ERROR_OUT_OF_MEMORY = -11,  TI_ERROR_MAX_ENUM = 0xffffffff,} TiError;CopyErrors reported by the Taichi C-API.TI_ERROR_SUCCESS: The Taichi C-API invocation finished gracefully.TI_ERROR_NOT_SUPPORTED: The invoked API, or the combination of parameters is not supported by the Taichi C-API.TI_ERROR_CORRUPTED_DATA: Provided data is corrupted.TI_ERROR_NAME_NOT_FOUND: Provided name does not refer to any existing item.TI_ERROR_INVALID_ARGUMENT: One or more function arguments violate constraints specified in C-API documents, or kernel arguments mismatch the kernel argument list defined in the AOT module.TI_ERROR_ARGUMENT_NULL: One or more by-reference (pointer) function arguments point to null.TI_ERROR_ARGUMENT_OUT_OF_RANGE: One or more function arguments are out of its acceptable range; or enumeration arguments have undefined value.TI_ERROR_ARGUMENT_NOT_FOUND: One or more kernel arguments are missing.TI_ERROR_INVALID_INTEROP: The intended interoperation is not possible on the current arch. For example, attempts to export a Vulkan object from a CUDA runtime are not allowed.TI_ERROR_INVALID_STATE: The Taichi C-API enters an unrecoverable invalid state. Related Taichi objects are potentially corrupted. The users should release the contaminated resources for stability. Please feel free to file an issue if you encountered this error in a normal routine.TI_ERROR_INCOMPATIBLE_MODULE: The AOT module is not compatible with the current runtime.Enumeration TiArchStable since Taichi version: 1.4.0// enumeration.archtypedef enum TiArch {  TI_ARCH_RESERVED = 0,  TI_ARCH_VULKAN = 1,  TI_ARCH_METAL = 2,  TI_ARCH_CUDA = 3,  TI_ARCH_X64 = 4,  TI_ARCH_ARM64 = 5,  TI_ARCH_OPENGL = 6,  TI_ARCH_GLES = 7,  TI_ARCH_MAX_ENUM = 0xffffffff,} TiArch;CopyTypes of backend archs.TI_ARCH_VULKAN: Vulkan GPU backend.TI_ARCH_METAL: Metal GPU backend.TI_ARCH_CUDA: NVIDIA CUDA GPU backend.TI_ARCH_X64: x64 native CPU backend.TI_ARCH_ARM64: Arm64 native CPU backend.TI_ARCH_OPENGL: OpenGL GPU backend.TI_ARCH_GLES: OpenGL ES GPU backend.Enumeration TiCapabilityStable since Taichi version: 1.4.0// enumeration.capabilitytypedef enum TiCapability {  TI_CAPABILITY_RESERVED = 0,  TI_CAPABILITY_SPIRV_VERSION = 1,  TI_CAPABILITY_SPIRV_HAS_INT8 = 2,  TI_CAPABILITY_SPIRV_HAS_INT16 = 3,  TI_CAPABILITY_SPIRV_HAS_INT64 = 4,  TI_CAPABILITY_SPIRV_HAS_FLOAT16 = 5,  TI_CAPABILITY_SPIRV_HAS_FLOAT64 = 6,  TI_CAPABILITY_SPIRV_HAS_ATOMIC_INT64 = 7,  TI_CAPABILITY_SPIRV_HAS_ATOMIC_FLOAT16 = 8,  TI_CAPABILITY_SPIRV_HAS_ATOMIC_FLOAT16_ADD = 9,  TI_CAPABILITY_SPIRV_HAS_ATOMIC_FLOAT16_MINMAX = 10,  TI_CAPABILITY_SPIRV_HAS_ATOMIC_FLOAT = 11,  TI_CAPABILITY_SPIRV_HAS_ATOMIC_FLOAT_ADD = 12,  TI_CAPABILITY_SPIRV_HAS_ATOMIC_FLOAT_MINMAX = 13,  TI_CAPABILITY_SPIRV_HAS_ATOMIC_FLOAT64 = 14,  TI_CAPABILITY_SPIRV_HAS_ATOMIC_FLOAT64_ADD = 15,  TI_CAPABILITY_SPIRV_HAS_ATOMIC_FLOAT64_MINMAX = 16,  TI_CAPABILITY_SPIRV_HAS_VARIABLE_PTR = 17,  TI_CAPABILITY_SPIRV_HAS_PHYSICAL_STORAGE_BUFFER = 18,  TI_CAPABILITY_SPIRV_HAS_SUBGROUP_BASIC = 19,  TI_CAPABILITY_SPIRV_HAS_SUBGROUP_VOTE = 20,  TI_CAPABILITY_SPIRV_HAS_SUBGROUP_ARITHMETIC = 21,  TI_CAPABILITY_SPIRV_HAS_SUBGROUP_BALLOT = 22,  TI_CAPABILITY_SPIRV_HAS_NON_SEMANTIC_INFO = 23,  TI_CAPABILITY_SPIRV_HAS_NO_INTEGER_WRAP_DECORATION = 24,  TI_CAPABILITY_MAX_ENUM = 0xffffffff,} TiCapability;CopyDevice capabilities.Structure TiCapabilityLevelInfoStable since Taichi version: 1.4.0// structure.capability_level_infotypedef struct TiCapabilityLevelInfo {  TiCapability capability;  uint32_t level;} TiCapabilityLevelInfo;CopyAn integral device capability level. It currently is not guaranteed that a higher level value is compatible with a lower level value.Enumeration TiDataTypeStable since Taichi version: 1.4.0// enumeration.data_typetypedef enum TiDataType {  TI_DATA_TYPE_F16 = 0,  TI_DATA_TYPE_F32 = 1,  TI_DATA_TYPE_F64 = 2,  TI_DATA_TYPE_I8 = 3,  TI_DATA_TYPE_I16 = 4,  TI_DATA_TYPE_I32 = 5,  TI_DATA_TYPE_I64 = 6,  TI_DATA_TYPE_U1 = 7,  TI_DATA_TYPE_U8 = 8,  TI_DATA_TYPE_U16 = 9,  TI_DATA_TYPE_U32 = 10,  TI_DATA_TYPE_U64 = 11,  TI_DATA_TYPE_GEN = 12,  TI_DATA_TYPE_UNKNOWN = 13,  TI_DATA_TYPE_MAX_ENUM = 0xffffffff,} TiDataType;CopyElementary (primitive) data types. There might be vendor-specific constraints on the available data types so it's recommended to use 32-bit data types if multi-platform distribution is desired.TI_DATA_TYPE_F16: 16-bit IEEE 754 half-precision floating-point number.TI_DATA_TYPE_F32: 32-bit IEEE 754 single-precision floating-point number.TI_DATA_TYPE_F64: 64-bit IEEE 754 double-precision floating-point number.TI_DATA_TYPE_I8: 8-bit one's complement signed integer.TI_DATA_TYPE_I16: 16-bit one's complement signed integer.TI_DATA_TYPE_I32: 32-bit one's complement signed integer.TI_DATA_TYPE_I64: 64-bit one's complement signed integer.TI_DATA_TYPE_U8: 8-bit unsigned integer.TI_DATA_TYPE_U16: 16-bit unsigned integer.TI_DATA_TYPE_U32: 32-bit unsigned integer.TI_DATA_TYPE_U64: 64-bit unsigned integer.Enumeration TiArgumentTypeStable since Taichi version: 1.4.0// enumeration.argument_typetypedef enum TiArgumentType {  TI_ARGUMENT_TYPE_I32 = 0,  TI_ARGUMENT_TYPE_F32 = 1,  TI_ARGUMENT_TYPE_NDARRAY = 2,  TI_ARGUMENT_TYPE_TEXTURE = 3,  TI_ARGUMENT_TYPE_SCALAR = 4,  TI_ARGUMENT_TYPE_MAX_ENUM = 0xffffffff,} TiArgumentType;CopyTypes of kernel and compute graph argument.TI_ARGUMENT_TYPE_I32: 32-bit one's complement signed integer.TI_ARGUMENT_TYPE_F32: 32-bit IEEE 754 single-precision floating-point number.TI_ARGUMENT_TYPE_NDARRAY: ND-array wrapped around a TiMemory.TI_ARGUMENT_TYPE_TEXTURE: Texture wrapped around a TiImage.TI_ARGUMENT_TYPE_SCALAR: Typed scalar.BitField TiMemoryUsageFlagsStable since Taichi version: 1.4.0// bit_field.memory_usagetypedef enum TiMemoryUsageFlagBits {  TI_MEMORY_USAGE_STORAGE_BIT = 1 << 0,  TI_MEMORY_USAGE_UNIFORM_BIT = 1 << 1,  TI_MEMORY_USAGE_VERTEX_BIT = 1 << 2,  TI_MEMORY_USAGE_INDEX_BIT = 1 << 3,} TiMemoryUsageFlagBits;typedef TiFlags TiMemoryUsageFlags;CopyUsages of a memory allocation. Taichi requires kernel argument memories to be allocated with TI_MEMORY_USAGE_STORAGE_BIT.TI_MEMORY_USAGE_STORAGE_BIT: The memory can be read/write accessed by any kernel.TI_MEMORY_USAGE_UNIFORM_BIT: The memory can be used as a uniform buffer in graphics pipelines.TI_MEMORY_USAGE_VERTEX_BIT: The memory can be used as a vertex buffer in graphics pipelines.TI_MEMORY_USAGE_INDEX_BIT: The memory can be used as an index buffer in graphics pipelines.Structure TiMemoryAllocateInfoStable since Taichi version: 1.4.0// structure.memory_allocate_infotypedef struct TiMemoryAllocateInfo {  uint64_t size;  TiBool host_write;  TiBool host_read;  TiBool export_sharing;  TiMemoryUsageFlags usage;} TiMemoryAllocateInfo;CopyParameters of a newly allocated memory.size: Size of the allocation in bytes.host_write: True if the host needs to write to the allocated memory.host_read: True if the host needs to read from the allocated memory.export_sharing: True if the memory allocation needs to be exported to other backends (e.g., from Vulkan to CUDA).usage: All possible usage of this memory allocation. In most cases, TI_MEMORY_USAGE_STORAGE_BIT is enough.Structure TiMemorySliceStable since Taichi version: 1.4.0// structure.memory_slicetypedef struct TiMemorySlice {  TiMemory memory;  uint64_t offset;  uint64_t size;} TiMemorySlice;CopyA subsection of a memory allocation. The sum of offset and size cannot exceed the size of memory.memory: The subsectioned memory allocation.offset: Offset from the beginning of the allocation.size: Size of the subsection.Structure TiNdShapeStable since Taichi version: 1.4.0// structure.nd_shapetypedef struct TiNdShape {  uint32_t dim_count;  uint32_t dims[16];} TiNdShape;CopyMulti-dimensional size of an ND-array. Dimension sizes after dim_count are ignored.dim_count: Number of dimensions.dims: Dimension sizes.Structure TiNdArrayStable since Taichi version: 1.4.0// structure.nd_arraytypedef struct TiNdArray {  TiMemory memory;  TiNdShape shape;  TiNdShape elem_shape;  TiDataType elem_type;} TiNdArray;CopyMulti-dimensional array of dense primitive data.memory: Memory bound to the ND-array.shape: Shape of the ND-array.elem_shape: Shape of the ND-array elements. It must not be empty for vector or matrix ND-arrays.elem_type: Primitive data type of the ND-array elements.BitField TiImageUsageFlagsStable since Taichi version: 1.4.0// bit_field.image_usagetypedef enum TiImageUsageFlagBits {  TI_IMAGE_USAGE_STORAGE_BIT = 1 << 0,  TI_IMAGE_USAGE_SAMPLED_BIT = 1 << 1,  TI_IMAGE_USAGE_ATTACHMENT_BIT = 1 << 2,} TiImageUsageFlagBits;typedef TiFlags TiImageUsageFlags;CopyUsages of an image allocation. Taichi requires kernel argument images to be allocated with TI_IMAGE_USAGE_STORAGE_BIT and TI_IMAGE_USAGE_SAMPLED_BIT.TI_IMAGE_USAGE_STORAGE_BIT: The image can be read/write accessed by any kernel.TI_IMAGE_USAGE_SAMPLED_BIT: The image can be read-only accessed by any kernel.TI_IMAGE_USAGE_ATTACHMENT_BIT: The image can be used as a color or depth-stencil attachment depending on its format.Enumeration TiImageDimensionStable since Taichi version: 1.4.0// enumeration.image_dimensiontypedef enum TiImageDimension {  TI_IMAGE_DIMENSION_1D = 0,  TI_IMAGE_DIMENSION_2D = 1,  TI_IMAGE_DIMENSION_3D = 2,  TI_IMAGE_DIMENSION_1D_ARRAY = 3,  TI_IMAGE_DIMENSION_2D_ARRAY = 4,  TI_IMAGE_DIMENSION_CUBE = 5,  TI_IMAGE_DIMENSION_MAX_ENUM = 0xffffffff,} TiImageDimension;CopyDimensions of an image allocation.TI_IMAGE_DIMENSION_1D: The image is 1-dimensional.TI_IMAGE_DIMENSION_2D: The image is 2-dimensional.TI_IMAGE_DIMENSION_3D: The image is 3-dimensional.TI_IMAGE_DIMENSION_1D_ARRAY: The image is 1-dimensional and it has one or more layers.TI_IMAGE_DIMENSION_2D_ARRAY: The image is 2-dimensional and it has one or more layers.TI_IMAGE_DIMENSION_CUBE: The image is 2-dimensional and it has 6 layers for the faces towards +X, -X, +Y, -Y, +Z, -Z in sequence.Enumeration TiImageLayoutStable since Taichi version: 1.4.0// enumeration.image_layouttypedef enum TiImageLayout {  TI_IMAGE_LAYOUT_UNDEFINED = 0,  TI_IMAGE_LAYOUT_SHADER_READ = 1,  TI_IMAGE_LAYOUT_SHADER_WRITE = 2,  TI_IMAGE_LAYOUT_SHADER_READ_WRITE = 3,  TI_IMAGE_LAYOUT_COLOR_ATTACHMENT = 4,  TI_IMAGE_LAYOUT_COLOR_ATTACHMENT_READ = 5,  TI_IMAGE_LAYOUT_DEPTH_ATTACHMENT = 6,  TI_IMAGE_LAYOUT_DEPTH_ATTACHMENT_READ = 7,  TI_IMAGE_LAYOUT_TRANSFER_DST = 8,  TI_IMAGE_LAYOUT_TRANSFER_SRC = 9,  TI_IMAGE_LAYOUT_PRESENT_SRC = 10,  TI_IMAGE_LAYOUT_MAX_ENUM = 0xffffffff,} TiImageLayout;CopyTI_IMAGE_LAYOUT_UNDEFINED: Undefined layout. An image in this layout does not contain any semantical information.TI_IMAGE_LAYOUT_SHADER_READ: Optimal layout for read-only access, including sampling.TI_IMAGE_LAYOUT_SHADER_WRITE: Optimal layout for write-only access.TI_IMAGE_LAYOUT_SHADER_READ_WRITE: Optimal layout for read/write access.TI_IMAGE_LAYOUT_COLOR_ATTACHMENT: Optimal layout as a color attachment.TI_IMAGE_LAYOUT_COLOR_ATTACHMENT_READ: Optimal layout as an input color attachment.TI_IMAGE_LAYOUT_DEPTH_ATTACHMENT: Optimal layout as a depth attachment.TI_IMAGE_LAYOUT_DEPTH_ATTACHMENT_READ: Optimal layout as an input depth attachment.TI_IMAGE_LAYOUT_TRANSFER_DST: Optimal layout as a data copy destination.TI_IMAGE_LAYOUT_TRANSFER_SRC: Optimal layout as a data copy source.TI_IMAGE_LAYOUT_PRESENT_SRC:  Optimal layout as a presentation source.Enumeration TiFormatStable since Taichi version: 1.4.0// enumeration.formattypedef enum TiFormat {  TI_FORMAT_UNKNOWN = 0,  TI_FORMAT_R8 = 1,  TI_FORMAT_RG8 = 2,  TI_FORMAT_RGBA8 = 3,  TI_FORMAT_RGBA8SRGB = 4,  TI_FORMAT_BGRA8 = 5,  TI_FORMAT_BGRA8SRGB = 6,  TI_FORMAT_R8U = 7,  TI_FORMAT_RG8U = 8,  TI_FORMAT_RGBA8U = 9,  TI_FORMAT_R8I = 10,  TI_FORMAT_RG8I = 11,  TI_FORMAT_RGBA8I = 12,  TI_FORMAT_R16 = 13,  TI_FORMAT_RG16 = 14,  TI_FORMAT_RGB16 = 15,  TI_FORMAT_RGBA16 = 16,  TI_FORMAT_R16U = 17,  TI_FORMAT_RG16U = 18,  TI_FORMAT_RGB16U = 19,  TI_FORMAT_RGBA16U = 20,  TI_FORMAT_R16I = 21,  TI_FORMAT_RG16I = 22,  TI_FORMAT_RGB16I = 23,  TI_FORMAT_RGBA16I = 24,  TI_FORMAT_R16F = 25,  TI_FORMAT_RG16F = 26,  TI_FORMAT_RGB16F = 27,  TI_FORMAT_RGBA16F = 28,  TI_FORMAT_R32U = 29,  TI_FORMAT_RG32U = 30,  TI_FORMAT_RGB32U = 31,  TI_FORMAT_RGBA32U = 32,  TI_FORMAT_R32I = 33,  TI_FORMAT_RG32I = 34,  TI_FORMAT_RGB32I = 35,  TI_FORMAT_RGBA32I = 36,  TI_FORMAT_R32F = 37,  TI_FORMAT_RG32F = 38,  TI_FORMAT_RGB32F = 39,  TI_FORMAT_RGBA32F = 40,  TI_FORMAT_DEPTH16 = 41,  TI_FORMAT_DEPTH24STENCIL8 = 42,  TI_FORMAT_DEPTH32F = 43,  TI_FORMAT_MAX_ENUM = 0xffffffff,} TiFormat;CopyTexture formats. The availability of texture formats depends on runtime support.Structure TiImageOffsetStable since Taichi version: 1.4.0// structure.image_offsettypedef struct TiImageOffset {  uint32_t x;  uint32_t y;  uint32_t z;  uint32_t array_layer_offset;} TiImageOffset;CopyOffsets of an image in X, Y, Z, and array layers.x: Image offset in the X direction.y: Image offset in the Y direction. Must be 0 if the image has a dimension of TI_IMAGE_DIMENSION_1D or TI_IMAGE_DIMENSION_1D_ARRAY.z: Image offset in the Z direction. Must be 0 if the image has a dimension of TI_IMAGE_DIMENSION_1D, TI_IMAGE_DIMENSION_2D, TI_IMAGE_DIMENSION_1D_ARRAY, TI_IMAGE_DIMENSION_2D_ARRAY or TI_IMAGE_DIMENSION_CUBE_ARRAY.array_layer_offset: Image offset in array layers. Must be 0 if the image has a dimension of TI_IMAGE_DIMENSION_1D, TI_IMAGE_DIMENSION_2D or TI_IMAGE_DIMENSION_3D.Structure TiImageExtentStable since Taichi version: 1.4.0// structure.image_extenttypedef struct TiImageExtent {  uint32_t width;  uint32_t height;  uint32_t depth;  uint32_t array_layer_count;} TiImageExtent;CopyExtents of an image in X, Y, Z, and array layers.width: Image extent in the X direction.height: Image extent in the Y direction. Must be 1 if the image has a dimension of TI_IMAGE_DIMENSION_1D or TI_IMAGE_DIMENSION_1D_ARRAY.depth: Image extent in the Z direction. Must be 1 if the image has a dimension of TI_IMAGE_DIMENSION_1D, TI_IMAGE_DIMENSION_2D, TI_IMAGE_DIMENSION_1D_ARRAY, TI_IMAGE_DIMENSION_2D_ARRAY or TI_IMAGE_DIMENSION_CUBE_ARRAY.array_layer_count: Image extent in array layers. Must be 1 if the image has a dimension of TI_IMAGE_DIMENSION_1D, TI_IMAGE_DIMENSION_2D or TI_IMAGE_DIMENSION_3D. Must be 6 if the image has a dimension of TI_IMAGE_DIMENSION_CUBE_ARRAY.Structure TiImageAllocateInfoStable since Taichi version: 1.4.0// structure.image_allocate_infotypedef struct TiImageAllocateInfo {  TiImageDimension dimension;  TiImageExtent extent;  uint32_t mip_level_count;  TiFormat format;  TiBool export_sharing;  TiImageUsageFlags usage;} TiImageAllocateInfo;CopyParameters of a newly allocated image.dimension: Image dimension.extent: Image extent.mip_level_count: Number of mip-levels.format: Image texel format.export_sharing: True if the memory allocation needs to be exported to other backends (e.g., from Vulkan to CUDA).usage: All possible usages of this image allocation. In most cases, TI_IMAGE_USAGE_STORAGE_BIT and TI_IMAGE_USAGE_SAMPLED_BIT enough.Structure TiImageSliceStable since Taichi version: 1.4.0// structure.image_slicetypedef struct TiImageSlice {  TiImage image;  TiImageOffset offset;  TiImageExtent extent;  uint32_t mip_level;} TiImageSlice;CopyA subsection of a memory allocation. The sum of offset and extent in each dimension cannot exceed the size of image.image: The subsectioned image allocation.offset: Offset from the beginning of the allocation in each dimension.extent: Size of the subsection in each dimension.mip_level: The subsectioned mip-level.Structure TiTextureStable since Taichi version: 1.4.0// structure.texturetypedef struct TiTexture {  TiImage image;  TiSampler sampler;  TiImageDimension dimension;  TiImageExtent extent;  TiFormat format;} TiTexture;CopyImage data bound to a sampler.image: Image bound to the texture.sampler: The bound sampler that controls the sampling behavior of image.dimension: Image Dimension.extent: Image extent.format: Image texel format.Union TiScalarValueStable since Taichi version: 1.5.0// union.scalar_valuetypedef union TiScalarValue {  uint8_t x8;  uint16_t x16;  uint32_t x32;  uint64_t x64;} TiScalarValue;CopyScalar value represented by a power-of-two number of bits.NOTE The unsigned integer types merely hold the number of bits in memory and doesn't reflect any type of the underlying data. For example, a 32-bit floating-point scalar value is assigned by *(float*)&scalar_value.x32 = 0.0f; a 16-bit signed integer is assigned by *(int16_t)&scalar_vaue.x16 = 1. The actual type of the scalar is hinted via type.x8: Scalar value that fits into 8 bits.x16: Scalar value that fits into 16 bits.x32: Scalar value that fits into 32 bits.x64: Scalar value that fits into 64 bits.Structure TiScalarStable since Taichi version: 1.5.0// structure.scalartypedef struct TiScalar {  TiDataType type;  TiScalarValue value;} TiScalar;CopyA typed scalar value.Union TiArgumentValueStable since Taichi version: 1.4.0// union.argument_valuetypedef union TiArgumentValue {  int32_t i32;  float f32;  TiNdArray ndarray;  TiTexture texture;  TiScalar scalar;} TiArgumentValue;CopyA scalar or structured argument value.i32: Value of a 32-bit one's complement signed integer. This is equivalent to x32 with TI_DATA_TYPE_I32.f32: Value of a 32-bit IEEE 754 single-precision floating-poing number. This is equivalent to x32 with TI_DATA_TYPE_F32.ndarray: An ND-array to be bound.texture: A texture to be bound.scalar: An scalar to be bound.Structure TiArgumentStable since Taichi version: 1.4.0// structure.argumenttypedef struct TiArgument {  TiArgumentType type;  TiArgumentValue value;} TiArgument;CopyAn argument value to feed kernels.type: Type of the argument.value: Value of the argument.Structure TiNamedArgumentStable since Taichi version: 1.4.0// structure.named_argumenttypedef struct TiNamedArgument {  const char* name;  TiArgument argument;} TiNamedArgument;CopyA named argument value to feed compute graphs.name: Name of the argument.argument: Argument body.Function ti_get_versionStable since Taichi version: 1.4.0// function.get_versionTI_DLL_EXPORT uint32_t TI_API_CALL ti_get_version();CopyGet the current taichi version. It has the same value as TI_C_API_VERSION as defined in taichi_core.h.Function ti_get_available_archsStable since Taichi version: 1.4.0// function.get_available_archsTI_DLL_EXPORT void TI_API_CALL ti_get_available_archs(  uint32_t* arch_count,  TiArch* archs);CopyGets a list of available archs on the current platform. An arch is only available if:The Runtime library is compiled with its support;The current platform is installed with a capable hardware or an emulation software.An available arch has at least one device available, i.e., device index 0 is always available. If an arch is not available on the current platform, a call to ti_create_runtime with that arch is guaranteed failing.WARNING Please also note that the order or returned archs is undefined.Function ti_get_last_errorStable since Taichi version: 1.4.0// function.get_last_errorTI_DLL_EXPORT TiError TI_API_CALL ti_get_last_error(  uint64_t* message_size,  char* message);CopyGets the last error raised by Taichi C-API invocations. Returns the semantical error code.message_size: Size of textual error message in messagemessage: Text buffer for the textual error message. Ignored when message_size is 0.Function ti_set_last_errorStable since Taichi version: 1.4.0// function.set_last_errorTI_DLL_EXPORT void TI_API_CALL ti_set_last_error(  TiError error,  const char* message);CopySets the provided error as the last error raised by Taichi C-API invocations. It can be useful in extended validation procedures in Taichi C-API wrappers and helper libraries.error: Semantical error code.message: A null-terminated string of the textual error message or nullptr for empty error message.Function ti_create_runtimeStable since Taichi version: 1.4.0// function.create_runtimeTI_DLL_EXPORT TiRuntime TI_API_CALL ti_create_runtime(  TiArch arch,  uint32_t device_index);CopyCreates a Taichi Runtime with the specified TiArch.arch: Arch of Taichi Runtime.device_index: The index of device in arch to create Taichi Runtime on.Function ti_destroy_runtimeStable since Taichi version: 1.4.0// function.destroy_runtimeTI_DLL_EXPORT void TI_API_CALL ti_destroy_runtime(  TiRuntime runtime);CopyDestroys a Taichi Runtime.Function ti_set_runtime_capabilities_extStable since Taichi version: 1.4.0// function.set_runtime_capabilitiesTI_DLL_EXPORT void TI_API_CALL ti_set_runtime_capabilities_ext(  TiRuntime runtime,  uint32_t capability_count,  const TiCapabilityLevelInfo* capabilities);CopyForce override the list of available capabilities in the runtime instance.Function ti_get_runtime_capabilitiesStable since Taichi version: 1.4.0// function.get_runtime_capabilitiesTI_DLL_EXPORT void TI_API_CALL ti_get_runtime_capabilities(  TiRuntime runtime,  uint32_t* capability_count,  TiCapabilityLevelInfo* capabilities);CopyGets all capabilities available on the runtime instance.capability_count: The total number of capabilities available.capabilities: Returned capabilities.Function ti_allocate_memoryStable since Taichi version: 1.4.0// function.allocate_memoryTI_DLL_EXPORT TiMemory TI_API_CALL ti_allocate_memory(  TiRuntime runtime,  const TiMemoryAllocateInfo* allocate_info);CopyAllocates a contiguous device memory with provided parameters.Function ti_free_memoryStable since Taichi version: 1.4.0// function.free_memoryTI_DLL_EXPORT void TI_API_CALL ti_free_memory(  TiRuntime runtime,  TiMemory memory);CopyFrees a memory allocation.Function ti_map_memoryStable since Taichi version: 1.4.0// function.map_memoryTI_DLL_EXPORT void* TI_API_CALL ti_map_memory(  TiRuntime runtime,  TiMemory memory);CopyMaps a device memory to a host-addressable space. You must ensure that the device is not being used by any device command before the mapping.Function ti_unmap_memoryStable since Taichi version: 1.4.0// function.unmap_memoryTI_DLL_EXPORT void TI_API_CALL ti_unmap_memory(  TiRuntime runtime,  TiMemory memory);CopyUnmaps a device memory and makes any host-side changes about the memory visible to the device. You must ensure that there is no further access to the previously mapped host-addressable space.Function ti_allocate_imageStable since Taichi version: 1.4.0// function.allocate_imageTI_DLL_EXPORT TiImage TI_API_CALL ti_allocate_image(  TiRuntime runtime,  const TiImageAllocateInfo* allocate_info);CopyAllocates a device image with provided parameters.Function ti_free_imageStable since Taichi version: 1.4.0// function.free_imageTI_DLL_EXPORT void TI_API_CALL ti_free_image(  TiRuntime runtime,  TiImage image);CopyFrees an image allocation.Function ti_copy_memory_device_to_device (Device Command)Stable since Taichi version: 1.4.0// function.copy_memory_device_to_deviceTI_DLL_EXPORT void TI_API_CALL ti_copy_memory_device_to_device(  TiRuntime runtime,  const TiMemorySlice* dst_memory,  const TiMemorySlice* src_memory);CopyCopies the data in a contiguous subsection of the device memory to another subsection. The two subsections must not overlap.Function ti_track_image_extStable since Taichi version: 1.4.0// function.track_imageTI_DLL_EXPORT void TI_API_CALL ti_track_image_ext(  TiRuntime runtime,  TiImage image,  TiImageLayout layout);CopyTracks the device image with the provided image layout. Because Taichi tracks image layouts internally, it is only useful to inform Taichi that the image is transitioned to a new layout by external procedures.Function ti_transition_image (Device Command)Stable since Taichi version: 1.4.0// function.transition_imageTI_DLL_EXPORT void TI_API_CALL ti_transition_image(  TiRuntime runtime,  TiImage image,  TiImageLayout layout);CopyTransitions the image to the provided image layout. Because Taichi tracks image layouts internally, it is only useful to enforce an image layout for external procedures to use.Function ti_launch_kernel (Device Command)Stable since Taichi version: 1.4.0// function.launch_kernelTI_DLL_EXPORT void TI_API_CALL ti_launch_kernel(  TiRuntime runtime,  TiKernel kernel,  uint32_t arg_count,  const TiArgument* args);CopyLaunches a Taichi kernel with the provided arguments. The arguments must have the same count and types in the same order as in the source code.Function ti_launch_compute_graph (Device Command)Stable since Taichi version: 1.4.0// function.launch_compute_graphTI_DLL_EXPORT void TI_API_CALL ti_launch_compute_graph(  TiRuntime runtime,  TiComputeGraph compute_graph,  uint32_t arg_count,  const TiNamedArgument* args);CopyLaunches a Taichi compute graph with provided named arguments. The named arguments must have the same count, names, and types as in the source code.Function ti_flushStable since Taichi version: 1.4.0// function.flushTI_DLL_EXPORT void TI_API_CALL ti_flush(  TiRuntime runtime);CopySubmits all previously invoked device commands to the offload device for execution.Function ti_waitStable since Taichi version: 1.4.0// function.waitTI_DLL_EXPORT void TI_API_CALL ti_wait(  TiRuntime runtime);CopyWaits until all previously invoked device commands are executed. Any invoked command that has not been submitted is submitted first.Function ti_load_aot_moduleStable since Taichi version: 1.4.0// function.load_aot_moduleTI_DLL_EXPORT TiAotModule TI_API_CALL ti_load_aot_module(  TiRuntime runtime,  const char* module_path);CopyLoads a pre-compiled AOT module from the file system.
Returns TI_NULL_HANDLE if the runtime fails to load the AOT module from the specified path.Function ti_create_aot_moduleStable since Taichi version: 1.4.0// function.create_aot_moduleTI_DLL_EXPORT TiAotModule TI_API_CALL ti_create_aot_module(  TiRuntime runtime,  const void* tcm,  uint64_t size);CopyCreates a pre-compiled AOT module from TCM data.
Returns TI_NULL_HANDLE if the runtime fails to create the AOT module from TCM data.Function ti_destroy_aot_moduleStable since Taichi version: 1.4.0// function.destroy_aot_moduleTI_DLL_EXPORT void TI_API_CALL ti_destroy_aot_module(  TiAotModule aot_module);CopyDestroys a loaded AOT module and releases all related resources.Function ti_get_aot_module_kernelStable since Taichi version: 1.4.0// function.get_aot_module_kernelTI_DLL_EXPORT TiKernel TI_API_CALL ti_get_aot_module_kernel(  TiAotModule aot_module,  const char* name);CopyRetrieves a pre-compiled Taichi kernel from the AOT module.
Returns TI_NULL_HANDLE if the module does not have a kernel of the specified name.Function ti_get_aot_module_compute_graphStable since Taichi version: 1.4.0// function.get_aot_module_compute_graphTI_DLL_EXPORT TiComputeGraph TI_API_CALL ti_get_aot_module_compute_graph(  TiAotModule aot_module,  const char* name);CopyRetrieves a pre-compiled compute graph from the AOT module.
Returns TI_NULL_HANDLE if the module does not have a compute graph of the specified name.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?AvailabilityHowToCreate and destroy a Runtime InstanceAllocate and free memoryAllocate host-accessible memoryLoad and destroy a Taichi AOT moduleLaunch kernels and compute graphsAPI ReferenceAlias TiBoolDefinition TI_FALSEDefinition TI_TRUEAlias TiFlagsDefinition TI_NULL_HANDLEHandle TiRuntimeHandle TiAotModuleHandle TiMemoryHandle TiImageHandle TiKernelHandle TiComputeGraphEnumeration TiErrorEnumeration TiArchEnumeration TiCapabilityStructure TiCapabilityLevelInfoEnumeration TiDataTypeEnumeration TiArgumentTypeBitField TiMemoryUsageFlagsStructure TiMemoryAllocateInfoStructure TiMemorySliceStructure TiNdShapeStructure TiNdArrayBitField TiImageUsageFlagsEnumeration TiImageDimensionEnumeration TiImageLayoutEnumeration TiFormatStructure TiImageOffsetStructure TiImageExtentStructure TiImageAllocateInfoStructure TiImageSliceStructure TiTextureUnion TiScalarValueStructure TiScalarUnion TiArgumentValueStructure TiArgumentStructure TiNamedArgumentFunction ti_get_versionFunction ti_get_available_archsFunction ti_get_last_errorFunction ti_set_last_errorFunction ti_create_runtimeFunction ti_destroy_runtimeFunction ti_set_runtime_capabilities_extFunction ti_get_runtime_capabilitiesFunction ti_allocate_memoryFunction ti_free_memoryFunction ti_map_memoryFunction ti_unmap_memoryFunction ti_allocate_imageFunction ti_free_imageFunction ti_copy_memory_device_to_device (Device Command)Function ti_track_image_extFunction ti_transition_image (Device Command)Function ti_launch_kernel (Device Command)Function ti_launch_compute_graph (Device Command)Function ti_flushFunction ti_waitFunction ti_load_aot_moduleFunction ti_create_aot_moduleFunction ti_destroy_aot_moduleFunction ti_get_aot_module_kernelFunction ti_get_aot_module_compute_graphCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Vulkan Backend Features | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APICore FunctionalityVulkan Backend FeaturesMath LibraryContributionReferencesInternalsFAQGlossaryDoc Home>>Taichi Runtime C-API>>Vulkan Backend FeaturesVersion: v1.6.0On this pageVulkan Backend FeaturesTaichi's Vulkan API gives you further control over the Vulkan version and extension requirements and allows you to interop with external Vulkan applications with shared resources.API ReferenceStructure TiVulkanRuntimeInteropInfoStable since Taichi version: 1.4.0// structure.vulkan_runtime_interop_infotypedef struct TiVulkanRuntimeInteropInfo {  PFN_vkGetInstanceProcAddr get_instance_proc_addr;  uint32_t api_version;  VkInstance instance;  VkPhysicalDevice physical_device;  VkDevice device;  VkQueue compute_queue;  uint32_t compute_queue_family_index;  VkQueue graphics_queue;  uint32_t graphics_queue_family_index;} TiVulkanRuntimeInteropInfo;CopyNecessary detail to share the same Vulkan runtime between Taichi and external procedures.get_instance_proc_addr: Pointer to Vulkan loader function vkGetInstanceProcAddr.api_version: Target Vulkan API version.instance: Vulkan instance handle.physical_device: Vulkan physical device handle.device: Vulkan logical device handle.compute_queue: Vulkan queue handle created in the queue family at compute_queue_family_index.compute_queue_family_index: Index of a Vulkan queue family with the VK_QUEUE_COMPUTE_BIT set.graphics_queue: Vulkan queue handle created in the queue family at graphics_queue_family_index.graphics_queue_family_index: Index of a Vulkan queue family with the VK_QUEUE_GRAPHICS_BIT set.NOTE compute_queue and graphics_queue can be the same if the queue family have VK_QUEUE_COMPUTE_BIT and VK_QUEUE_GRAPHICS_BIT set at the same tiem.Structure TiVulkanMemoryInteropInfoStable since Taichi version: 1.4.0// structure.vulkan_memory_interop_infotypedef struct TiVulkanMemoryInteropInfo {  VkBuffer buffer;  uint64_t size;  VkBufferUsageFlags usage;  VkDeviceMemory memory;  uint64_t offset;} TiVulkanMemoryInteropInfo;CopyNecessary detail to share the same piece of Vulkan buffer between Taichi and external procedures.buffer: Vulkan buffer.size: Size of the piece of memory in bytes.usage: Vulkan buffer usage. In most of the cases, Taichi requires the VK_BUFFER_USAGE_STORAGE_BUFFER_BIT.memory: Device memory binded to the Vulkan buffer.offset: Offset in VkDeviceMemory object to the beginning of this allocation, in bytes.Structure TiVulkanImageInteropInfoStable since Taichi version: 1.4.0// structure.vulkan_image_interop_infotypedef struct TiVulkanImageInteropInfo {  VkImage image;  VkImageType image_type;  VkFormat format;  VkExtent3D extent;  uint32_t mip_level_count;  uint32_t array_layer_count;  VkSampleCountFlagBits sample_count;  VkImageTiling tiling;  VkImageUsageFlags usage;} TiVulkanImageInteropInfo;CopyNecessary detail to share the same piece of Vulkan image between Taichi and external procedures.image: Vulkan image.image_type: Vulkan image allocation type.format: Pixel format.extent: Image extent.mip_level_count: Number of mip-levels of the image.array_layer_count: Number of array layers.sample_count: Number of samples per pixel.tiling: Image tiling.usage: Vulkan image usage. In most cases, Taichi requires the VK_IMAGE_USAGE_STORAGE_BIT and the VK_IMAGE_USAGE_SAMPLED_BIT.Function ti_create_vulkan_runtime_extStable since Taichi version: 1.4.0// function.create_vulkan_runtimeTI_DLL_EXPORT TiRuntime TI_API_CALL ti_create_vulkan_runtime_ext(  uint32_t api_version,  uint32_t instance_extension_count,  const char** instance_extensions,  uint32_t device_extension_count,  const char** device_extensions);CopyCreates a Vulkan Taichi runtime with user-controlled capability settings.Function ti_import_vulkan_runtimeStable since Taichi version: 1.4.0// function.import_vulkan_runtimeTI_DLL_EXPORT TiRuntime TI_API_CALL ti_import_vulkan_runtime(  const TiVulkanRuntimeInteropInfo* interop_info);CopyImports the Vulkan runtime owned by Taichi to external procedures.Function ti_export_vulkan_runtimeStable since Taichi version: 1.4.0// function.export_vulkan_runtimeTI_DLL_EXPORT void TI_API_CALL ti_export_vulkan_runtime(  TiRuntime runtime,  TiVulkanRuntimeInteropInfo* interop_info);CopyExports a Vulkan runtime from external procedures to Taichi.Function ti_import_vulkan_memoryStable since Taichi version: 1.4.0// function.import_vulkan_memoryTI_DLL_EXPORT TiMemory TI_API_CALL ti_import_vulkan_memory(  TiRuntime runtime,  const TiVulkanMemoryInteropInfo* interop_info);CopyImports the Vulkan buffer owned by Taichi to external procedures.Function ti_export_vulkan_memoryStable since Taichi version: 1.4.0// function.export_vulkan_memoryTI_DLL_EXPORT void TI_API_CALL ti_export_vulkan_memory(  TiRuntime runtime,  TiMemory memory,  TiVulkanMemoryInteropInfo* interop_info);CopyExports a Vulkan buffer from external procedures to Taichi.Function ti_import_vulkan_imageStable since Taichi version: 1.4.0// function.import_vulkan_imageTI_DLL_EXPORT TiImage TI_API_CALL ti_import_vulkan_image(  TiRuntime runtime,  const TiVulkanImageInteropInfo* interop_info,  VkImageViewType view_type,  VkImageLayout layout);CopyImports the Vulkan image owned by Taichi to external procedures.Function ti_export_vulkan_imageStable since Taichi version: 1.4.0// function.export_vulkan_imageTI_DLL_EXPORT void TI_API_CALL ti_export_vulkan_image(  TiRuntime runtime,  TiImage image,  TiVulkanImageInteropInfo* interop_info);CopyExports a Vulkan image from external procedures to Taichi.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?API ReferenceStructure TiVulkanRuntimeInteropInfoStructure TiVulkanMemoryInteropInfoStructure TiVulkanImageInteropInfoFunction ti_create_vulkan_runtime_extFunction ti_import_vulkan_runtimeFunction ti_export_vulkan_runtimeFunction ti_import_vulkan_memoryFunction ti_export_vulkan_memoryFunction ti_import_vulkan_imageFunction ti_export_vulkan_imageCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Math Module | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryMath ModuleSparse MatrixContributionReferencesInternalsFAQGlossaryDoc Home>>Math Library>>Math ModuleVersion: v1.6.0On this pageMath ModuleTaichi provides a built-in math module that supports frequently used mathematical functions and utility functions, including:Commonly-used mathematical functions that are analogous to those in Python's built-in math module.Small vector and matrix types that are analogous to those in the OpenGL shading language (GLSL).Some GLSL-standard functions.Complex number operations in the form of 2D vectors.Mathematical functionsYou must call the mathematical functions provided by Taichi's math module from within the Taichi scope. For example:import taichi as tiimport taichi.math as tmti.init()@ti.kerneldef test():    a = 1.0    x = tm.sin(a)    y = tm.floor(a)    z = tm.degrees(a)    w = tm.log2(a)    ...CopyThese functions also take vectors and matrices as arguments and operate on them element-wise:@ti.kerneldef test():    a = ti.Vector([1.0, 2.0, 3.0])    x = tm.sin(a)  # [0.841471, 0.909297, 0.141120]    y = tm.floor(a)  #  [1.000000, 2.000000, 3.000000]    z = tm.degrees(a)  #  [57.295780, 114.591560, 171.887344]    b = ti.Vector([2.0, 3.0, 4.0])    w = tm.atan2(b, a)  # [1.107149, 0.982794, 0.927295]    ...CopynoteTaichi's math module overlaps to a large extent with Python's built-in math module. Ensure that you follow a few extra rules when using Taichi's math module:You must call the functions provided by Taichi's math module from within the Taichi scope.Functions in Taichi's math module also take vectors or matrices as arguments.The precision of a function in Taichi's math module depends on the settings of default_fp and arch (backend) in ti.init().Small vector and matrix typesTaichi's math module provides a few small vector and matrix types:vec2/vec3/vec4: 2D/3D/4D floating-point vector types.ivec2/ivec3/ivec4: 2D/3D/4D integer vector types.uvec2/uvec3/uvec4: 2D/3D/4D unsigned integer vector types.mat2/mat3/mat4: 2D/3D/4D floating-point square matrix types.To create one of the vector/matrix types above, use template function ti.types.vector() or ti.types.matrix(). For example, vec2 is defined as follows:vec2 = ti.types.vector(2, float)CopyThe number of precision bits of such a type is determined by default_fp or default_ip in the ti.init() method call. For example, if ti.init(default_fp=ti.f64) is called, then vec2/vec3/vec4 and mat2/mat3/mat4 defined in the Taichi scope all have a 64-bit floating-point precision.You can use these types to instantiate vectors/matrices or annotate data types for function arguments and struct members. See the Type System for more information. Here we emphasize that they have very flexible initialization routines:mat2 = ti.math.mat2vec3 = ti.math.mat3vec4 = ti.math.vec4m = mat2(1)  # [[1., 1.], [1., 1.]]m = mat2(1, 2, 3, 4)  # [[1., 2.], [3, 4.]]m = mat2([1, 2], [3, 4])  # [[1., 2.], [3, 4.]]m = mat2([1, 2, 3, 4])  # [[1., 2.], [3, 4.]]v = vec3(1, 2, 3)m = mat2(v, 4)  # [[1., 2.], [3, 4.]]u = vec4([1, 2], [3, 4])u = vec4(v, 4.0)CopyAnother important feature of vector types created by ti.types.vector() is that they support vector swizzling just as GLSL vectors do. This means you can use xyzw, rgba, stpq to access their elements with indices ≤ four:v = ti.math.vec4(1, 2, 3, 4)u = v.xyz  # vec3(1, 2, 3)u = v.xxx  # vec3(1, 1, 1)u = v.wzyx  # vec4(4, 3, 2, 1)u = v.rraa  # vec4(1, 1, 2, 2)CopyRelations between ti.Vector, ti.types.vector and ti.math.vec3ti.Vector is a function that accepts a 1D array and returns a matrix instance that has only one column. For example, ti.Vector([1, 2, 3, 4, 5]).ti.types.vector is a function that accepts an integer and a primitive type and returns a vector type. For example: vec5f = ti.types.vector(5, float). vec5f can then be used to instantiate 5D vectors or annotate data types of function arguments and struct members:@ti.kerneldef test(v: vec5f):    print(v.xyz)Copy  Unlike ti.Vector, whose input data must be a 1D array, vector types created by ti.types.vector() have more flexible ways to initialize, as explained above.ti.math.vec3 is created by vec3 = ti.types.vector(3, float).GLSL-standard functionsTaichi's math module also supports a few GLSL standard functions. These functions follow the GLSL standard, except that they accept arbitrary vectors and matrices as arguments and operate on them element-wise. For example:import taichi as tiimport taichi.math as tm@ti.kerneldef example():    v = tm.vec3(0., 1., 2.)    w = tm.smoothstep(0.0, 1.0, v)    w = tm.clamp(w, 0.2, 0.8)    w = tm.reflect(v, tm.normalize(tm.vec3(1)))CopynoteTexture support in Taichi is implemented in the ti.types.texture_types module.Complex number operationsTaichi's math module also supports basic complex arithmetic operations on 2D vectors.You can use a 2D vector of type ti.math.vec2 to represent a complex number. In this way, additions and subtractions of complex numbers come in the form of 2D vector additions and subtractions. You can call ti.math.cmul() and ti.math.cdiv() to conduct multiplication and division of complex numbers:import taichi as tiimport taichi.math as tmti.init()@ti.kerneldef test():    x = tm.vec2(1, 1)  # complex number 1+1j    y = tm.vec2(0, 1)  # complex number 1j    z = tm.cmul(x, y)  # vec2(-1, 1) = -1+1j    w = tm.cdiv(x, y)  #  vec2(2, 0) = 2+0jCopyYou can also compute the power, logarithm, and exponential of a complex number:@ti.kerneldef test():    x = tm.vec2(1, 1)  # complex number 1 + 1j    y = tm.cpow(x, 2)  # complex number (1 + 1j)**2 = 2j    z = tm.clog(x)     # complex number (0.346574 + 0.785398j)    w = tm.cexp(x)     # complex number (1.468694 + 2.287355j)CopyEdit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Mathematical functionsSmall vector and matrix typesRelations between ti.Vector, ti.types.vector and ti.math.vec3GLSL-standard functionsComplex number operationsCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Sparse Matrix | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryMath ModuleSparse MatrixContributionReferencesInternalsFAQGlossaryDoc Home>>Math Library>>Sparse MatrixVersion: v1.6.0On this pageSparse MatrixSparse matrices are frequently involved in solving linear systems in science and engineering. Taichi provides useful APIs for sparse matrices on the CPU and CUDA backends.To use sparse matrices in Taichi programs, follow these three steps:Create a builder using ti.linalg.SparseMatrixBuilder().Call ti.kernel to fill the builder with your matrices' data.Build sparse matrices from the builder.WARNINGThe sparse matrix feature is still under development. There are some limitations:The sparse matrix data type on the CPU backend only supports f32 and f64.The sparse matrix data type on the CUDA backend only supports f32.Here's an example:import taichi as tiarch = ti.cpu # or ti.cudati.init(arch=arch)n = 4# step 1: create sparse matrix builderK = ti.linalg.SparseMatrixBuilder(n, n, max_num_triplets=100)@ti.kerneldef fill(A: ti.types.sparse_matrix_builder()):    for i in range(n):        A[i, i] += 1  # Only +=  and -= operators are supported for now.# step 2: fill the builder with data.fill(K)print(">>>> K.print_triplets()")K.print_triplets()# outputs:# >>>> K.print_triplets()# n=4, m=4, num_triplets=4 (max=100)(0, 0) val=1.0(1, 1) val=1.0(2, 2) val=1.0(3, 3) val=1.0# step 3: create a sparse matrix from the builder.A = K.build()print(">>>> A = K.build()")print(A)# outputs:# >>>> A = K.build()# [1, 0, 0, 0]# [0, 1, 0, 0]# [0, 0, 1, 0]# [0, 0, 0, 1]CopyThe basic operations like +, -, *, @ and transpose of sparse matrices are supported now.print(">>>> Summation: C = A + A")C = A + Aprint(C)# outputs:# >>>> Summation: C = A + A# [2, 0, 0, 0]# [0, 2, 0, 0]# [0, 0, 2, 0]# [0, 0, 0, 2]print(">>>> Subtraction: D = A - A")D = A - Aprint(D)# outputs:# >>>> Subtraction: D = A - A# [0, 0, 0, 0]# [0, 0, 0, 0]# [0, 0, 0, 0]# [0, 0, 0, 0]print(">>>> Multiplication with a scalar on the right: E = A * 3.0")E = A * 3.0print(E)# outputs:# >>>> Multiplication with a scalar on the right: E = A * 3.0# [3, 0, 0, 0]# [0, 3, 0, 0]# [0, 0, 3, 0]# [0, 0, 0, 3]print(">>>> Multiplication with a scalar on the left: E = 3.0 * A")E = 3.0 * Aprint(E)# outputs:# >>>> Multiplication with a scalar on the left: E = 3.0 * A# [3, 0, 0, 0]# [0, 3, 0, 0]# [0, 0, 3, 0]# [0, 0, 0, 3]print(">>>> Transpose: F = A.transpose()")F = A.transpose()print(F)# outputs:# >>>> Transpose: F = A.transpose()# [1, 0, 0, 0]# [0, 1, 0, 0]# [0, 0, 1, 0]# [0, 0, 0, 1]print(">>>> Matrix multiplication: G = E @ A")G = E @ Aprint(G)# outputs:# >>>> Matrix multiplication: G = E @ A# [3, 0, 0, 0]# [0, 3, 0, 0]# [0, 0, 3, 0]# [0, 0, 0, 3]print(">>>> Element-wise multiplication: H = E * A")H = E * Aprint(H)# outputs:# >>>> Element-wise multiplication: H = E * A# [3, 0, 0, 0]# [0, 3, 0, 0]# [0, 0, 3, 0]# [0, 0, 0, 3]print(f">>>> Element Access: A[0,0] = {A[0,0]}")# outputs:# >>>> Element Access: A[0,0] = 1.0CopySparse linear solverYou may want to solve some linear equations using sparse matrices.
Then, the following steps could help:Create a solver using ti.linalg.SparseSolver(solver_type, ordering). Currently, the factorization types supported on CPU backends are LLT, LDLT, and LU, and supported orderings include AMD and COLAMD. The sparse solver on CUDA supports the LLT factorization type only.Analyze and factorize the sparse matrix you want to solve using solver.analyze_pattern(sparse_matrix) and solver.factorize(sparse_matrix)Call x = solver.solve(b), where x is the solution and b is the right-hand side of the linear system. On CPU backends, x and b can be NumPy arrays, Taichi Ndarrays, or Taichi fields. On the CUDA backend, x and b must be Taichi Ndarrays.Call solver.info() to check if the solving process succeeds.Here's a full example.import taichi as tiarch = ti.cpu # or ti.cudati.init(arch=arch)n = 4K = ti.linalg.SparseMatrixBuilder(n, n, max_num_triplets=100)b = ti.ndarray(ti.f32, shape=n)@ti.kerneldef fill(A: ti.types.sparse_matrix_builder(), b: ti.template(), interval: ti.i32):    for i in range(n):        A[i, i] += 2.0        if i % interval == 0:            b[i] += 1.0fill(K, b, 3)A = K.build()print(">>>> Matrix A:")print(A)print(">>>> Vector b:")print(b)# outputs:# >>>> Matrix A:# [2, 0, 0, 0]# [0, 2, 0, 0]# [0, 0, 2, 0]# [0, 0, 0, 2]# >>>> Vector b:# [1. 0. 0. 1.]solver = ti.linalg.SparseSolver(solver_type="LLT")solver.analyze_pattern(A)solver.factorize(A)x = solver.solve(b)isSuccess = solver.info()print(">>>> Solve sparse linear systems Ax = b with the solution x:")print(x)print(f">>>> Computation was successful?: {isSuccess}")# outputs:# >>>> Solve sparse linear systems Ax = b with the solution x:# [0.5 0.  0.  0.5]# >>>> Computation was successful?: TrueCopyExamplesPlease have a look at our two demos for more information:Stable fluid: A 2D fluid simulation using a sparse Laplacian matrix to solve Poisson's pressure equation.Implicit mass spring: A 2D cloth simulation demo using sparse matrices to solve the linear systems.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Sparse linear solverExamplesCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Contribution Guidelines | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionContribution GuidelinesDeveloper InstallationDeveloper UtilitiesWrite a Python testWrite a C++ testDevelopment TipsMarkdown SyntaxDocument Style GuideDebugging on WindowsReferencesInternalsFAQGlossaryDoc Home>>Contribution>>Contribution GuidelinesVersion: v1.6.0On this pageContribution GuidelinesThank you for your interest in contributing to Taichi. Taichi was born as an academic research project. Though we are working hard to improve its code quality, Taichi has a long way to go to become a mature, large-scale engineering project. This is also why we decided to open source Taichi from the very beginning: We rely on our community to help Taichi evolve and thrive. From document updates, bug fix, to feature implementation, wherever you spot an issue, you are very welcome to file a PR (pull request) with us!:-)Centered around the common process of taking on an issue, testing, and making a corresponding PR, this document provides guidelines, tips, and major considerations for Taichi's contributors. We highly recommend that you spend some time familiarizing yourself with this contribution guide before contributing to Taichi.General guidelines and tipsThis section provides some general guidelines for the Taichi community and tips that we find practically useful.Be pragmatic & no overkillsAlways use straightforward (sometimes even brute-force) solutions: Complicated code usually suggests a lack of design or over-engineering."There are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies, and the other way is to make it so complicated that there are no obvious deficiencies. The first method is far more difficult." — C.A.R. Hoare"Perfection (in design) is achieved not when there is nothing more to add, but rather when there is nothing more to take away." — Antoine de Saint-ExupéryJuxtapose pros and consWhen it comes to making a design decision, weigh up its pros and cons. A design is good to go so long as its advantages outweigh its disadvantages.Communicate effectivelyOur ultimate goal is to build a sustainable, prosperous Taichi community, and effective communication is the cornerstone of that goal. Following are tips that may contribute to effective communication:Concise:The message behind your words outweighs the number of your words. Use as few words as possible to drive your point home.Use tables, figures, and lists where possible.Professional:Read twice before you post: Would your point get across with your words?Use a spell checker, such as Grammarly, to improve your writing in terms of grammar, style, and tone.Constructive and courteous: Base your feedback and discussions on facts, NOT on personal feelings.Acceptable😃: "This design could be confusing to new Taichi users. If it were designed this way, it could..."Undesirable😞: "This design is terrible."What you can contributeWe welcome all kinds of contributions, including but not limited to:Fixing a bugProposing and implementing new featuresImproving or refactoring an existing documentSuggesting more friendly error messagesAdding new test cases and examples (demos)Posting blog articles and tutorialsEnhancing compiler performanceMinor updates to documentation, codes, or annotations.File an issueIf you would like to propose a new feature, or if you spot a potential issue, you can file an issue with Taichi.noteWhen you try to report potential bugs in an issue, please consider running ti diagnose and offer its output as an attachment. This helps the maintainers to learn more about the context and the system information of your environment to make the debugging process more efficient and solve your issue more easily.cautionWhen filing your issue, review it once again to ensure that no sensitive information about your data or yourself creeps in.Take over an issueExcept for minor updates, most PRs start from a developer taking over an issue. This section provides some corresponding tips and best practices.Where to find issues for startersIssue TagDescriptionTarget developergood first issueIssues that are easy to start withDevelopers new to Taichiwelcome contributionIssues slightly more challengingDevelopers who wish to dive deeper into TaichiBest practicesWhen you plan to take over an issue:Best practice: Leave a message claiming that you are working on it.Goal: Avoid unnecessary repeated work.Example: "I know how to fix this and would like to help."After you take over an issue:Best practice:Briefly describe how you plan to handle it (if no solution has been provided).Hold off until a core developer responds to your action plan.Goal: Keep your implementation neat and effective.Example: See #2610.References for documentation updatesAs part of the effort to increase visibility of the community and to improve developer experience, we highly recommend including documentation updates in your PR if applicable. Here are some of the documentation-specific references and tips:Documentation source files are hosted under docs/.We use GitHub Flavored Markdown (GFM) and Docusaurus to build our documentation site. For information on the supported Markdown syntax, see the  Documentation Writing Guide.When it comes to writing, we adhere to the Google Developer Documentation Style Guide.For instructions on setting up a local server and previewing your updated documentation in real-time, see the Local Development.Add test cases for your local changesIf your PR is to implement a new feature, we recommend that you write your own test cases to cover corner cases for your codes before filing a PR.To write a Python test case, see the Workflow for writing a Python test.To write a C++ test case, see the Workflow for writing a C++ test.Conduct style checks and integration tests locallyWe highly recommend that you complete code style checks and integration tests on your local computer before filing a PR.Enforce code formatTaichi enforces code style via pre-commit hooks, which includes the following checks:C++ codes are formatted by clang-format-10.Python codes are formatted by black v23.3.0.Python codes are statically checked by pylint.You will need to install pre-commit first:pip install pre-commitCopyand run the code checkers:pre-commit run -aCopyWith this command, black will format your Python codes automatically.
You can install it as a pre-commit hook so that it is run before you commit the changes to git:pre-commit installCopy What if I didn't format my code style locally? No problem, the CI bot will run the code checkers and format your codes automatically when you submit a PR.For more style information for your C++ code, see our C++ style.C++ style guideWe generally follow Google C++ Style Guide. One major exception is the naming convention of functions: Taichi adopts the snake case for function naming, as opposed to the camel case suggested in Google's style, e.g. this_is_a_taichi_function().Taichi uses clang-tidy-10 (install via sudo apt install clang-tidy-10) to automatically check C++ code for stype violations, programming errors and enforce coding style best practices. You can find a list of enabled checks in .clang-tidy.Taichi's clang-tidy integration test is enabled per PR basis and you can run it locally via:python ./scripts/run_clang_tidy.py $PWD/taichi -clang-tidy-binary clang-tidy-10 -header-filter=$PWD/taichi -j4Copyclang-tidy also provides an easy way to automatically apply suggested fixes by passing -fix argument:python ./scripts/run_clang_tidy.py $PWD/taichi -clang-tidy-binary clang-tidy-10 -clang-apply-replacements-binary clang-apply-replacements-10 -header-filter=$PWD/taichi -j4 -fixCopyRun integration testsTo run all the C++ and Python tests:
python tests/run_tests.pyExample 1:
python tests/run_tests.py -v -t3 -a cpu,metal -s-v: Verbose output.-t <threads>: Set a custom number of threads for parallel testing.-a <arch(s)>: Test only the specified backends (separated by comma).-s: Original output from the tests.Example 2:
python tests/run_tests.py numpy_io<filename(s)>: Run test cases in specified files only (separated by comma).This command runs all tests in tests/python/test_numpy_io.py.Example 3:
python tests/run_tests.py linalg -k "cross or diag"-k <key>: Run only the tests that match the specified keys (supports expression in a key string).This command runs test_cross() and test_diag() in tests/python/test_linalg.py.To show all available options
python tests/run_tests.py -h We have both Python and C++ test cases, but C++ test cases are disabled by default. To enable C++ test cases:Build Taichi from source using the python setup.py develop command.Set TAICHI_CMAKE_ARGS="-DTI_BUILD_TESTS:BOOL=ON".File a pull request (PR)Now you get to the point where you need to get your hands dirty with your PRs. This section provides the following:Considerations when you create PRsPR naming conventionsPR review & merging checklistConsiderationsWhen implementing a complex feature:Consider breaking it down to multiple separate, self-contained PRs to provide the community with a clearer context and keep a more traceable development history.If you're already a collaborator or maintainer with write access to the Taichi repository, please consider adopting the ghstack workflow.When creating a PR:Have your PR address only one issue:In this way, you keep your changesets small so that potential issues can be readily identified.If you include in your PR irrevelant implementations, ensure that they are minor.Your reviewers have the right to request you to remove massive, irrevelant changes from your PR.If your PR is to implement a new feature, ensure that you have designed test cases for it. See Add test cases for your local changes.You are required to conduct code style checks and integration tests locally for your PR. See Conduct style checks and integration tests locallyWhen describing your PR:Provide sufficient information in the description of your PR to provide the community with clearer context:Link to a specific GitHub issue if applicable, for example fixes #<issue_number>.Share important design decisions in your description.If you create a PR still in progress:Click Convert to draft on your PR page to convert the PR to draft, indicating that you are still working on it.Click Ready for review when you are all set and up for a review.See Draft for more information.PR naming conventionsYour PR will make it into the commit history in the the master branch or even Taichi's release notes, therefore it is important to keep your PR title self-explanatory. This section describes our PR naming conventions:[tag1] [tag2]...[tagN] Your PR title must be short but carry necessary info^----^ ^----^...^----^ ^--------------------------------------------------^|      |        |      ||      |        |      +---> Capitalize the initial of your title.|      |        +---> Adjacent tags are separated with precisely one space.|      +--->  Frequently used tags: [cuda], [lang], [ci], [ir], [refactor].+--->  Prepend at least one tag to your PR title.CopyTag naming conventions:Prepend at least one tag, such as [lang], to your PR title.If you have multiple tags, separate adjacent tags with one space.See misc/prtags.json for a full list of available tags.We differentiate PRs for end-users from PRs for developers by capitalizing tag initial.If a PR deals with a feature visible to the end-users, initialize the most relevant tag and the PR will make it into the release notes. For example, [Metal], [Vulkan], [IR], [Lang], or [CUDA]. Ensure that your PR title has AT MOST one tag dealt this way.If a PR deals with the underlying or intermediate implementation, then it is for the developers and you need to ensure that all its tags are in lowercase. For example, [metal], [vulkan], [ir], [lang], or [cuda].INCORRECT[Lang][refactor] (sans space)CORRECT[Lang] [refactor]INCORRECT[GUI] [Mac] Support modifier keys (both tags have their initial capitalized)CORRECT[gui] [Mac] Support modifier keys (only one tag has its initial capitalized)Title naming conventions:Keep your PR title short enough but ensure that it carries necessary information.Do not include back quotes ("`") in your PR title.Capitalize the initial letter of your title, which is the word immediately after your tag(s).INCORRECT[Doc] improve documentation (the initial of the title is not capitalized)CORRECT[Doc] Improve documentationnoteFollowing are some frequently used tags:[cuda]: Backend-specific changes.[lang]: Frontend language features, including syntax sugars.[ir]: Intermediate representation-specific changes.[refactor]: Code refactoring changes.[ci]: CI/CD workflow-specific changes.[Doc]: Documentation updates.When introducing a new tag, ensure that you add it to misc/prtags.json so that others can follow.PR review & merging checklistFollow this checklist during PR review or merging:Ensure that your PR title follows our naming conventions.Ensure that Taichi's master branch has a linear history. See Linear vs Non-Linear History for more information.Ensure that your PR passes all Continuous Integration (CI) tests before merging it.CI is triggered each time you push a commit to an open PR. It builds and tests all commits in your PR in multiple environments. Keep an eye on the CI test results:A ✔️ on the left-hand side of a commit hash: CI has passed,A ❌ on the left-hand side of a commit hash: CI has failed.Here, we do not want to repeat some best practices summarized in the following Google blog articles. But please spare a couple of minutes reading them if your PR is being reviewed or if you are reviewing a PR. They have our recommendation!Code Health: Understanding Code In ReviewCode Health: Respectful Reviews == Useful ReviewsHow to have your PR merged quicklyImplementing the ghstack workflow for complex changesetsThe standard GitHub PR workflow can become unwieldy when dealing with large changesets.
With the help of ghstack, you can break down a large changeset into multiple PRs, each building upon the previous one.
Each PR will undergo its own review process and CI/CD checks.For details on how to use ghstack, please consult its documentation, with the exception of the landing step.
To land ghstack commits, simply comment /land on the PR. A landing bot will then verify if the corresponding PR and all its predecessors have been approved and passed the CI/CD checks.
If all checks have been cleared, the bot will execute ghstack land for you.Deal with compilation warningsTaichi implements warning-free code by turning on -Werror by default. This means that Taichi takes warnings as errors, and we highly recommend that you resolve a warning as soon as it occurs.In the following section, we provide several practical tips for handling some of the common scenarios that you may encounter during CI compilation.Deal with warnings that occur when compiling third-party header filesThere is little we can do to third-party warnings other than turning them off. To turn off or mute warnings from specific third-party header files, use the SYSTEM option when configuring include_directories in your CMake files. Then, the included header files are treated as system headers. See the following two examples taken from cmake/TaichiCore.cmake:# Treat files under "external/Vulkan-Headers/include" as system headers and mute warnings from them.include_directories(SYSTEM external/Vulkan-Headers/include)# Treat files under "external/VulkanMemoryAllocator/include" as system headers for target "${CORE_LIBRARY_NAME}"target_include_directories(${CORE_LIBRARY_NAME} SYSTEM PRIVATE external/VulkanMemoryAllocator/include)CopyDeal with warnings when compiling third-party libraries or targetsIdeally, third-party libraries or targets ought to be built completely independent of your Taichi project. In practice, because of the design of the CMake system, CMake variables from the Taichi and third-party submodules are sometimes messed up. Therefore, we recommend that you disable warnings from a third-party library or target:Separate the submodule's CMAKE_CXX_FLAGS from the same variable defined in Taichi.Remove the -Wall option from the submodule's CMAKE_CXX_FLAGS variables.Mute specific warning types across the entire Taichi projectYou can find details about how to mute certain warning types from the Clang Compiler User Manual; it usually starts with -Wno-. Please explain what the warning is about and why we should ignore it in the comments.The following examples can be found in cmake/TaichiCXXFlags.cmake:# [Global] Clang warns if a C++ pointer's nullability was not explicitly marked (__nonnull, nullable, ...).# Nullability seems to be a clang-specific feature; thus we disable this warning.set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-nullability-completeness ")# [Global] By evaluating "constexpr", compiler throws a warning for functions known to be dead at compile time.# However, some of these "constexpr" specifiers are debug flags and will be manually enabled upon debugging.set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-unneeded-internal-declaration ")CopyMute warnings from specific code blockscautionThe approach presented here is not recommended and considered your last approach, because it is not reliable.In rare situations where you can neither fix nor mute the warnings from specific code blocks via conventional approaches, your last approach is to mute them by decorating your code block using the #pragma clang diagnostic macros. Beware that #pragmas are not defined in the C++ standard and that their implementations depend heavily on the compiler. That is to say, this solution is neither stable nor elegant.To ignore all warnings from a specific code block, wrap it up with the following two groups of macros. Further, you can even replace -Wall with a group of warning types for finer control. See the following example:#if defined(__clang__)  #pragma clang diagnostic push  #pragma clang diagnostic ignored "-Wall"#endif{Your Code Goes Here}#if defined(__clang__)  #pragma clang diagnostic pop#endifCopyHandle special CI failuresTaichi's CI system is implemented using the Github Actions, the entrance of which lies in testing.yaml. Depending on the CI pipeline, testing.yml will execute one of the corresponding test scripts under this directoryThere are a few CI pipelines that work slightly different from the standard CI pipeline:CI pipeline - Build Android DemosBuild Andriod Demos builds both taichi-repo with your PR applied and an external taichi-aot-demo repo. After that, it executes the demos from taichi-aot-demo with the just-compiled Taichi program and libraries.If your PR to taichi-repo contains changes to some public interface, you may need to adjust the codes in taichi-aot-demo to avoid breaking the demos. To achieve that, please follow these steps:File your PR to taichi-repo. If this PR changes the public interface, then it probably breaks the demos thus fail the Build Android Demos CI pipeline - Don't panic, this is expected.Update the demo codes in taichi-aot-demo to make it work with the above mentioned PR, then file a separate PR to taichi-aot-demo repo and have it merged.In the original PR to taichi-repo, update the commit id for taichi-aot-demo in aot-demo.sh. This time your PR is expected to pass Build Android Demos.Still have issues?If you encounter any issue that is not covered here, feel free to ask us on GitHub discussions or open an issue on GitHub with all the details attached. We are always there to help!Finally, thanks again for your interest in contributing to Taichi. We look forward to seeing your contributions!Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?General guidelines and tipsBe pragmatic & no overkillsJuxtapose pros and consCommunicate effectivelyWhat you can contributeFile an issueTake over an issueWhere to find issues for startersBest practicesReferences for documentation updatesAdd test cases for your local changesConduct style checks and integration tests locallyEnforce code formatC++ style guideRun integration testsFile a pull request (PR)ConsiderationsPR naming conventionsPR review & merging checklistImplementing the ghstack workflow for complex changesetsDeal with compilation warningsDeal with warnings that occur when compiling third-party header filesDeal with warnings when compiling third-party libraries or targetsMute specific warning types across the entire Taichi projectMute warnings from specific code blocksHandle special CI failuresCI pipeline - Build Android DemosStill have issues?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Developer Installation | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionContribution GuidelinesDeveloper InstallationDeveloper UtilitiesWrite a Python testWrite a C++ testDevelopment TipsMarkdown SyntaxDocument Style GuideDebugging on WindowsReferencesInternalsFAQGlossaryDoc Home>>Contribution>>Developer InstallationVersion: v1.6.0On this pageDeveloper InstallationTarget audienceDevelopers who are interested in the compiler, computer graphics, or high-performance computing, and would like to contribute new features or bug fixes to the Taichi programming language.IMPORTANTThis installation guide is NOT intended for end users who only wish to do simulation or high performance numerical computation. We recommend that end users install Taichi via pip install taichi. There is no need for you to build Taichi from source.See the Get Started for more information on quickly setting up Taichi for end users.Introduction  This installation guide covers the following:Prerequisites for building Taichi from sourceInstalling optional dependenciesBuilding Taichi from sourceList of TAICHI_CMAKE_ARGSUsage and behavior of build.pyTroubleshooting and debuggingFrequently asked questionsnoteInstallation instructions vary depending on which operating system (OS) you are using. Choose the right OS or platform before you proceed.noteWith the release of Taichi v1.6.0, a comprehensive build environment preparation script (aka. build.py or ti-build) has been introduced. This script significantly simplifies the process of configuring a suitable build or development environment.This guide will focus on the build.py approach. If you prefer to use the conventional method, you can refer to the previous Developer Installation document.PrerequisitesLinuxMacWindowsCategoryPrerequisitesLinux distributionAnything recent enough, e.g. Ubuntu 20.04Python3.6+, with a usable pip(python3-pip package on Ubuntu)Clang++Clang++ >= 10, Clang++ 15 is recommended.libstdc++-xx-devRun apt install libstdc++-10-dev, or just install g++.CategoryPrerequisitesmacOSmacOS Big Sur or laterPython3.6+ (should be readily available)Command line tools for XcodeRun xcode-select --install to installCategoryPrerequisitesWindowsWindows 7/8/10/11Python3.6+Visual StudioVisual Studio 2022 (any edition) with "Desktop Development with C++" component.Install CompilerTaichi supports building from source with Clang++ >= 10.0 and MSVC from VS2022.For macOS developers, it is recommended to use AppleClang, which comes with the Command Line Tools for Xcode. You can install them by running xcode-select --install. Alternatively, you can also install Xcode.app from the Apple Store.For Linux developers, it is recommended to install Clang using the package manager specific to your operating system. On Ubuntu 22.04, running sudo apt install clang-15 should be sufficient. For older Ubuntu distributions to use a newer version of Clang, please follow the instructions on official LLVM Debian/Ubuntu Nightly Packages.For Windows developers, if none of the VS2022 editions are installed, build.py will automatically start a VS2022 BuildTools installer for you.Install LLVMInstall pre-built, customized LLVM binariesbuild.py will automatically download and setup a suitable version of pre-built LLVM binaries.Alternatively, build LLVM from sourceBuild LLVM 15.0.0 from sourceWe provide instructions here if you need to build LLVM 15.0.0 from source.Linux & macOSWindowswget https://github.com/llvm/llvm-project/archive/refs/tags/llvmorg-15.0.5.tar.gztar zxvf llvmorg-15.0.5.tar.gzcd llvm-project-llvmorg-15.0.5/llvmmkdir buildcd buildcmake .. -DLLVM_ENABLE_RTTI:BOOL=ON -DBUILD_SHARED_LIBS:BOOL=OFF -DCMAKE_BUILD_TYPE=Release -DLLVM_TARGETS_TO_BUILD="X86;NVPTX" -DLLVM_ENABLE_ASSERTIONS=ON -DLLVM_ENABLE_TERMINFO=OFF# If you are building on Apple M1, use -DLLVM_TARGETS_TO_BUILD="AArch64".# If you are building on NVIDIA Jetson TX2, use -DLLVM_TARGETS_TO_BUILD="ARM;NVPTX"# If you are building for a PyPI release, add -DLLVM_ENABLE_Z3_SOLVER=OFF to reduce the library dependency.make -j 8sudo make install# Check your LLVM installationllvm-config --version  # You should get 15.0.5Copy# For Windows# LLVM 15.0.0 + MSVC 2019cmake .. -G "Visual Studio 16 2019" -A x64 -DLLVM_ENABLE_RTTI:BOOL=ON -DBUILD_SHARED_LIBS:BOOL=OFF -DCMAKE_BUILD_TYPE=Release -DLLVM_TARGETS_TO_BUILD="X86;NVPTX" -DLLVM_ENABLE_ASSERTIONS=ON -Thost=x64 -DLLVM_BUILD_TESTS:BOOL=OFF -DCMAKE_INSTALL_PREFIX=installed -DCMAKE_MSVC_RUNTIME_LIBRARY=MultiThreadedDLL -DCMAKE_CXX_STANDARD=17cmake --build . --target=INSTALL --config=ReleaseCopyUse Visual Studio 2017+ to build LLVM.sln.Ensure that you use the Release configuration. After building the INSTALL project (under folder CMakePredefinedTargets in the Solution Explorer window).If you use MSVC 2019+, ensure that you use C++17 for the INSTALL project.When the build completes, add an environment variable LLVM_DIR with value <PATH_TO_BUILD>/build/installed/lib/cmake/llvm.To actually use the compiled LLVM binaries, replace the LLVM folder in the cache directory of build.py (open with ./build.py cache) with your own version.Install optional dependenciesCUDA is NVIDIA's answer to high-performance computing. Taichi has implemented a backend based on CUDA 10.0.0+. Vulkan is a next-generation, cross-platform API, open standard for 3D graphics and computing. Taichi has added a Vulkan backend as of v0.8.0.This section provides instructions on installing these two optional dependencies.Install CUDAThis section works for you if you have a Nvidia GPU supporting CUDA. Note that the required CUDA version is 10.0+.To install CUDA:UbuntuArch LinuxWindowsGo to the official site to download the installer.Choose deb (local) as Installer Type.Check if CUDA is properly installed:nvidia-smiCopypacman -S cudaCheck if CUDA is properly installed:nvidia-smiCopyGo to the official site and download the installer.Choose exe (local) as Installer Type.Check if CUDA is properly installed:nvidia-smiCopyVulkan SDK is required to debug Taichi's Vulkan backend.
build.py will automatically download and setup a suitable version of Vulkan SDK.On Windows, Vulkan SDK requires elevated privileges to install (the installer would set several machine scope environement variables).Ensure a working Vulkan SDKLinuxWindowsEnsure that you have a Vulkan driver from a GPU vendor properly installed.On Ubuntu, check if a JSON file with a name corresponding to your GPU vendor is in: /etc/vulkan/icd.d/ or /usr/share/vulkan/icd.d/.Add an environment variable TAICHI_CMAKE_ARGS with the value -DTI_WITH_VULKAN:BOOL=ON to enable the Vulkan backend: (Otherwise Vulkan backend is disabled by default when compiling from source, and build.py won't setup Vulkan SDK for you).export TAICHI_CMAKE_ARGS="$TAICHI_CMAKE_ARGS -DTI_WITH_VULKAN:BOOL=ON"CopyCheck if the SDK is properly installed: Run vulkaninfo in the build shell:./build.py --shellvulkaninfoCopyAdd an environment variable TAICHI_CMAKE_ARGS with the value -DTI_WITH_VULKAN:BOOL=ON to enable the Vulkan backend: (Otherwise Vulkan backend is disabled by default when compiling from source, and build.py won't setup Vulkan SDK for you).$env:TAICHI_CMAKE_ARGS += " -DTI_WITH_VULKAN:BOOL=ON"CopyCheck if the SDK is properly installed: Run vulkaninfo in the build shell:python ./build.py --shellvulkaninfoCopyBuild Taichi from sourceClone the Taichi repo recursively and build1:Linux & macOSWindowsgit clone --recursive https://github.com/taichi-dev/taichicd taichi# Customize with your own needsexport TAICHI_CMAKE_ARGS="-DTI_WITH_VULKAN:BOOL=ON -DTI_WITH_CUDA:BOOL=ON"# Uncomment if you want to use a different compiler# export CC=/path/to/clang# export CXX=/path/to/clang++# export DEBUG=1 # Uncomment it if you wish to keep debug information.# This would drop into a shell with complete build environment,./build.py --shell# and then you could install Taichi in development modepython3 setup.py developCopygit clone --recursive https://github.com/taichi-dev/taichicd taichi# Customize with your own needs$env:TAICHI_CMAKE_ARGS += " -DTI_WITH_VULKAN:BOOL=ON -DTI_WITH_CUDA:BOOL=ON"# $env:DEBUG = 1 # Uncomment it if you wish to keep debug information.# This would drop into a shell with complete build environment,./build.py --shell# and then you could install Taichi in development modepython3 setup.py developCopynoteAlternatively, you could build a wheel file ready for install if you don't care about the convenience provided by python develop install:./build.pyls dist/*.whlCopyTry out some of the demos in the examples/ folder to see if Taichi is properly installed. For example:python3 python/taichi/examples/simulation/mpm128.pyCopynote1Although the two commands work similarly, ./build.py --shell and python setup.py develop is recommended for you as a developer and ./build.py is more for end users. The difference is:The python setup.py develop command does not actually install anything but only symbolically links the source code to the deployment directory.The ./build.py command builds a wheel suitable for shipping so that you need to rerun the command and install the wheel every time the source code is modified.The develop command serves the developers' needs better because edits to the Python files take effect immediately without the need to rerun the command. A rerun is needed only if you have modified the project's C extension or compiled files. See the Development Mode for more information.List of TAICHI_CMAKE_ARGSFlagDescriptionDefaultBUILD_WITH_ADDRESS_SANITIZERBuild with clang address sanitizerOFFTI_BUILD_EXAMPLESBuild the C++ examplesONTI_BUILD_RHI_EXAMPLESBuild the Unified Device API examplesOFFTI_BUILD_TESTSBuild the C++ testsOFFTI_WITH_AMDGPUBuild with the AMDGPU backendOFFTI_WITH_BACKTRACEUse backward-cpp to print out C++ stack trace upon failureOFFTI_WITH_CUDABuild with the CUDA backendONTI_WITH_CUDA_TOOLKITBuild with the CUDA toolkitOFFTI_WITH_C_APIBuild Taichi runtime C-API libraryONTI_WITH_DX11Build with the DX11 backendOFFTI_WITH_DX12Build with the DX12 backendOFFTI_WITH_GGUIBuild with GGUIOFFTI_WITH_GRAPHVIZGenerate dependency graphs between targetsOFFTI_WITH_LLVMBuild with LLVM backendsONTI_WITH_METALBuild with the Metal backendONTI_WITH_OPENGLBuild with the OpenGL backendONTI_WITH_PYTHONBuild with Python language bindingONTI_WITH_STATIC_C_APIBuild static Taichi runtime C-API libraryOFFTI_WITH_VULKANBuild with the Vulkan backendOFFUSE_LLDUse lld (from llvm) linkerOFFUSE_MOLDUse mold (A Modern Linker)OFFUSE_STDCPPUse -stdlib=libc++OFFDesign goals, behaviors and usage of build.pyCreated to be dead simpleSetting up an appropriate development environment for an unfamiliar project can be quite challenging.
Therefore, build.py has been created to eliminate this friction. If you find any aspect of the environment configuration process to be
'too manual' or suffered to progress, it is considered a bug. Please report such issues on GitHub.Designed to be minimally intrusiveNearly all the dependencies of build.py and Taichi are explicitly placed at the cache folder, which can be opened by:./build.py cacheCopyOr you can find it at:OSCache Folder LocationLinux && macOS~/.cache/ti-build-cacheWindows%LocalAppData%\ti-build-cacheA typical cache dir will contain sub folders below:Sub FolderPurposeCode ResponsiblebootstrapContains Python packages used by build.py itselfbootstrap.pydepsDownloaded external dependencies, before extract/installdep.pyllvm15Managed pre-built LLVM binariesllvm.pymambaforgeManaged conda environment dedicated to build / develop Taichipython.pysccacheCompile cachesccache.pyvulkan-1.x.xxx.xVulkan SDK locationvulkan.pyThe whole cache folder can be safely removed.build.py operates without the need for any third-party libraries to be installed, the requirements will be handled by its bootstrapping process.noteOn Debian/Ubuntu systems, apt install python3-pip is required.Behaviors considered intrusiveOn Ubuntu systems, there's an attempt to install missing development libraries at ospkg.py by invoking sudo apt install libxxxx-dev
if a terminal is detected. It can be skipped by telling apt not to install them.Installing Vulkan SDK on Windows requires elevated privileges, and the installer will set several machine scoped environment variables (VULKAN_SDK and VK_SDK_PATH).Choose your desired Python version, or use your own Python environment.By default, build.py assumes that the same Python version used to invoke it will also be used for building Taichi.
build.py will then create an isolated Python environment and use it for all the subsequent Python related tasks.
To use a different version, please specify the desired version via --python option:# Build a wheel./build.py --python=3.10# Or enter development shell./build.py --python=3.10 --shellCopyIf you prefer to manage Python environments yourself, you could specify --python=native, and build.py will not attempt to use a managed Python environment.# Use your own condaconda activate my-own-conda-env# Build a wheel./build.py --python=native# Or enter development shell./build.py --python=native --shellCopyTroubleshooting and debuggingPermission deniedDescriptionGets a permission denied after python3 setup.py develop or python3 setup.py install.Root causeYou were trying to install packages into the Python environment without write permission.Workaroundpython3 setup.py develop --user or python3 setup.py install --user.Install Conda and use python from within the conda environment.make fails to compileDescriptionmake fails to compile and reports fatal error: 'spdlog/XXX.h' file not found.Root causeYou did not use the --recursive flag when cloning the Taichi repository.WorkaroundRun git submodule update --init --recursive --depth=1.which python still returns the system's Python locationDescriptionwhich python still returns the system's Python location.WorkaroundRun the following commands to enter development shell:./build.py --shellCopyFrequently asked questionsHow can I get a fresh Taichi build?Clean up cache from your previous builds:python3 setup.py cleanCopyUninstall the Taichi package from your Python environment:python setup.py develop --uninstall, if you build Taichi using python setup.py develop.pip uninstall taichi, if you build Taichi using python setup.py install.What if I don't have wget on my macOS?Install Homebrew.Use Homebrew to install wget:brew install wgetStill have issues?See Installation Troubleshooting for issues that may share with the end-user installation.If you encounter any issue that is not covered here, feel free to report it by opening an issue on GitHub and including the details. We are always there to help!Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Target audienceIntroductionPrerequisitesInstall CompilerInstall LLVMInstall optional dependenciesBuild Taichi from sourceList of TAICHI_CMAKE_ARGSDesign goals, behaviors and usage of build.pyCreated to be dead simpleDesigned to be minimally intrusiveChoose your desired Python version, or use your own Python environment.Troubleshooting and debuggingPermission deniedmake fails to compilewhich python still returns the system's Python locationFrequently asked questionsHow can I get a fresh Taichi build?What if I don't have wget on my macOS?Still have issues?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Developer Utilities | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionContribution GuidelinesDeveloper InstallationDeveloper UtilitiesWrite a Python testWrite a C++ testDevelopment TipsMarkdown SyntaxDocument Style GuideDebugging on WindowsReferencesInternalsFAQGlossaryDoc Home>>Contribution>>Developer UtilitiesVersion: v1.6.0On this pageDeveloper UtilitiesThis section provides a detailed description of some commonly used
utilities for Taichi developers.LoggingTaichi uses spdlog as its logging
system. Logs can have different levels, from low to high, they are:LEVELStracedebuginfowarnerrorThe higher the level is, the more critical the message is.The default logging level is info. You may override the default
logging level by:Setting the environment variable like export TI_LOG_LEVEL=warn.Setting the log level from Python side:
ti.set_logging_level(ti.WARN).In Python, you may write logs using the ti.* interface:# Pythonti.trace("Hello world!")ti.debug("Hello world!")ti.info("Hello world!")ti.warn("Hello world!")ti.error("Hello world!")CopyIn C++, you may write logs using the TI_* interface:// C++TI_TRACE("Hello world!");TI_DEBUG("Hello world!");TI_INFO("Hello world!");TI_WARN("Hello world!");TI_ERROR("Hello world!");CopyIf one raises a message of the level error, Taichi will be
terminated immediately and result in a RuntimeError on Python
side.// C++int func(void *p) {  if (p == nullptr)    TI_ERROR("The pointer cannot be null!");  // will not reach here if p == nullptr  do_something(p);}CopynoteFor people from Linux kernels, TI_ERROR is just panic.You may also simplify the above code by using TI_ASSERT:int func(void *p) {  TI_ASSERT_INFO(p != nullptr, "The pointer cannot be null!");  // or  // TI_ASSERT(p != nullptr);  // will not reach here if p == nullptr  do_something(p);}CopyDebug taichi program using gdbPrepare a script that can reproduce the issue, e.g. python repro.py.Build taichi with debug information using DEBUG=1 python setup.py develop (or install).Run gdb --args python repro.py, now you can debug from there! For example, you can set a
breakpoint using b foo.cpp:102 or b Program::compile().However if your issue cannot be reproduced consistently this solution isn't a great fit.
In that case it's recommended to follow the section below so that gdb is triggered automatically
when the program crashes.(Linux only) Trigger gdb when programs crash# Pythonti.init(gdb_trigger=True)Copy// C++CoreState::set_trigger_gdb_when_crash(true);Copy# Shellexport TI_GDB_TRIGGER=1CopynoteQuickly pinpointing segmentation faults/assertion failures using
gdb: When Taichi crashes, gdb will be triggered and attach to the
current thread. You might be prompt to enter sudo password required for
gdb thread attaching. After entering gdb, check the stack backtrace
with command bt (backtrace), then find the line of code triggering
the error.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?LoggingDebug taichi program using gdb(Linux only) Trigger gdb when programs crashCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Write a Python test | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionContribution GuidelinesDeveloper InstallationDeveloper UtilitiesWrite a Python testWrite a C++ testDevelopment TipsMarkdown SyntaxDocument Style GuideDebugging on WindowsReferencesInternalsFAQGlossaryDoc Home>>Contribution>>Write a Python testVersion: v1.6.0On this pageWrite a Python testNormally we write functional tests in Python.We use pytest for our Python
test infrastructure.Python tests should be added to tests/python/test_xxx.py.For example, you've just added a utility function ti.log10. Now you
want to write a test to ensure that it functions properly.noteBefore running the test launcher tests/run_tests.py, you need to install the corresponding
dependencies:pip install -r requirements_test.txtCopyAdd a new test caseLook into tests/python, see if there is already a file suitable for your
test. If not, create a new file for it. In this case,
let's create a new file tests/python/test_logarithm.py for
simplicity.Add a function, the function name must start with test_ so
that pytest could find it. e.g:import taichi as tidef test_log10():    passCopyAdd some tests that make use of ti.log10 to ensure it works
well. Hint: You may pass/return values to/from Taichi-scope using 0-D
fields, i.e. r[None].import taichi as tidef test_log10():    ti.init(arch=ti.cpu)    r = ti.field(ti.f32, ())    @ti.kernel    def foo():        r[None] = ti.log10(r[None])    r[None] = 100    foo()    assert r[None] == 2CopyExecute python tests/run_tests.py logarithm, and the functions starting with test_ in
tests/python/test_logarithm.py will be executed.Test against multiple backendsThe line ti.init(arch=ti.cpu) in the test above means that it will only test on the CPU backend. In order to test against multiple backends, please use the @ti.test decorator, as illustrated below:import taichi as ti# will test against both CPU and CUDA backends@ti.test(arch=[ti.cpu, ti.cuda])def test_log10():    r = ti.field(ti.f32, ())    @ti.kernel    def foo():        r[None] = ti.log10(r[None])    r[None] = 100    foo()    assert r[None] == 2CopyIn this case, Taichi will execute this test case on both ti.cpu and ti.cuda backends.And you may test against all available backends (depends on your system and building environment) by simply not specifying the
argument:import taichi as ti# will test against all backends available on your end@ti.test()def test_log10():    r = ti.field(ti.f32, ())    @ti.kernel    def foo():        r[None] = ti.log10(r[None])    r[None] = 100    foo()    assert r[None] == 2CopyUse ti.approx for comparison with toleranceSometimes the precision of math operations could be limited on certain backends such as OpenGL,
e.g., ti.log10(100) may return 2.000001 or 1.999999 in this case.Adding tolerance with ti.approx can be helpful to mitigate
such errors on different backends, for example 2.001 == ti.approx(2)
will return True on the OpenGL backend.import taichi as ti# will test against all backends available on your end@ti.test()def test_log10():    r = ti.field(ti.f32, ())    @ti.kernel    def foo():        r[None] = ti.log10(r[None])    r[None] = 100    foo()    assert r[None] == ti.approx(2)CopycautionSimply using pytest.approx won't work well here, since it's
tolerance won't vary among different Taichi backends. It'll likely
fail on the OpenGL backend.ti.approx also correctly treats boolean types, e.g.:
2 == ti.approx(True).Parametrize test inputsIn the test above, r[None] = 100 means that it will only test that ti.log10 works correctly for the input 100. In order to test against different input values, you may use the @pytest.mark.parametrize decorator:import taichi as tiimport pytestimport math@pytest.mark.parametrize('x', [1, 10, 100])@ti.test()def test_log10(x):    r = ti.field(ti.f32, ())    @ti.kernel    def foo():        r[None] = ti.log10(r[None])    r[None] = x    foo()    assert r[None] == math.log10(x)CopyUse a comma-separated list for multiple input values:import taichi as tiimport pytestimport math@pytest.mark.parametrize('x,y', [(1, 2), (1, 3), (2, 1)])@ti.test()def test_atan2(x, y):    r = ti.field(ti.f32, ())    s = ti.field(ti.f32, ())    @ti.kernel    def foo():        r[None] = ti.atan2(r[None])    r[None] = x    s[None] = y    foo()    assert r[None] == math.atan2(x, y)CopyUse two separate parametrize to test all combinations of input
arguments:import taichi as tiimport pytestimport math@pytest.mark.parametrize('x', [1, 2])@pytest.mark.parametrize('y', [1, 2])# same as:  .parametrize('x,y', [(1, 1), (1, 2), (2, 1), (2, 2)])@ti.test()def test_atan2(x, y):    r = ti.field(ti.f32, ())    s = ti.field(ti.f32, ())    @ti.kernel    def foo():        r[None] = ti.atan2(r[None])    r[None] = x    s[None] = y    foo()    assert r[None] == math.atan2(x, y)CopySpecify ti.init configurationsYou may specify keyword arguments to ti.init() in ti.test(), e.g.:@ti.test(ti.cpu, debug=True, log_level=ti.TRACE)def test_debugging_utils():    # ... (some tests have to be done in debug mode)Copyis the same as:def test_debugging_utils():    ti.init(arch=ti.cpu, debug=True, log_level=ti.TRACE)    # ... (some tests have to be done in debug mode)CopyExclude some backends from testSome backends are not capable of executing certain tests, you may have to
exclude them from the test in order to move forward:# Run this test on all backends except for OpenGL@ti.test(exclude=[ti.opengl])def test_sparse_field():    # ... (some tests that requires sparse feature which is not supported by OpenGL)CopyYou may also use the extensions keyword to exclude backends without
a specific feature:# Run this test on all backends except for OpenGL@ti.test(extensions=[ti.extension.sparse])def test_sparse_field():    # ... (some tests that requires sparse feature which is not supported by OpenGL)CopyRequire extension in testsIf a test case depends on some extensions, you should add an argument require in the @ti.test decorator.@ti.test(require=ti.extension.sparse)def test_struct_for_pointer_block():    n = 16    block_size = 8    f = ti.field(dtype=ti.f32)    block = ti.root.pointer(ti.ijk, n // block_size)    block.dense(ti.ijk, block_size).place(f)    f[0, 2, 3] = 1    @ti.kernel    def count() -> int:        tot = 0        for I in ti.grouped(block):            tot += 1        return tot    assert count() == 1CopyNow, Taichi supports the following extensions:NameExtension detailssparseSparse data structuresquant_basicBasic operations in quantizationquantFull quantization functionalitiesdata6464-bit data and arithmeticadstackFor keeping the history of mutable local variables in autodiffblsBlock-local storageassertionRun-time asserts in Taichi kernelsextfuncSupport inserting external function calls or backend sourceEdit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Add a new test caseTest against multiple backendsUse ti.approx for comparison with toleranceParametrize test inputsSpecify ti.init configurationsExclude some backends from testRequire extension in testsCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Write a C++ test | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionContribution GuidelinesDeveloper InstallationDeveloper UtilitiesWrite a Python testWrite a C++ testDevelopment TipsMarkdown SyntaxDocument Style GuideDebugging on WindowsReferencesInternalsFAQGlossaryDoc Home>>Contribution>>Write a C++ testVersion: v1.6.0On this pageWrite a C++ testWe strongly recommend each developer to write C++ unit tests when sending a PR.We use googletest as the C++
test infrastructure.C++ tests should be added to the tests/cpp/ directory.Make sure your C++ test source file is covered by this CMake glob.Build and run Taichi C++ tests# build taichi with tests enabledTAICHI_CMAKE_ARGS="-DTI_BUILD_TESTS:BOOL=ON" python setup.py develop# run the C++ testpython tests/run_tests.py --cppCopynoteConsider polishing the C++ test infrastructure:Separate each translation unit into its own test executableHave a unified script to control the execution of which set of testsAdd a new test casePlease follow Googletest Primer and Advanced googletest Topics.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Build and run Taichi C++ testsAdd a new test caseCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Language Reference | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesLanguage ReferenceSyntax SugarsGlobal SettingsOperatorsDifferences between Taichi and Python ProgramsSIMT IntrinsicsInternalsFAQGlossaryDoc Home>>References>>Language ReferenceVersion: v1.6.0On this pageLanguage ReferenceThis article describes the syntax and semantics of the Taichi programming
language.To users: If you have gone through the user tutorials and still feel uncertain
about your program behavior, then you are in the right place. If you find the
actual behavior different from what is described in this article, feel free to
create an issue.
You should not rely solely on this article since things unspecified are subject to changes.To contributors: This article specifies what the language should be. That
is, you should try to match the implementation of the Taichi compiler with this
article. You can clearly determine a certain behavior is correct, buggy, or
undefined from this article.IntroductionTaichi is a domain-specific language embedded in Python.
Kernels and functions clearly defines the boundary between
the Taichi language and the Python language - code in the Taichi scope is
treated as the former, while code in the Python scope is treated as the latter.
It should be emphasized that this article is about the Taichi language.That said, because Taichi is embedded in Python, the syntax of Taichi is a
subset of that of Python. To make life easier, this article is modeled after
the Python language reference. The
notation and
lexical analysis
parts exactly follow Python. Please familiarize yourself with them if they seem
new.Basic conceptsBefore detailing syntax and semantics in the next few chapters, many basic but
important concepts and general evaluation principles specific to Taichi are
listed here.Values and typesLike many other programming languages, each expression in Taichi will be
evaluated to a value, and each value has a type. Because Taichi provides easy
interaction with Python and meta-programming support, there
are actually two kinds of evaluation: compile-time evaluation and runtime
evaluation. There are also two kinds of values: Python values and Taichi
values.noteFor readers familiar with programming language terms, such behavior is inspired
by multi-stage programming
or partial evaluation.A Python value is simply a
Python object,
which directly comes from the following sources:LiteralsArguments passed via ti.template()Free variablesFurthermore, as long as all the operands of an operation are Python values,
compile-time evaluation will take place, producing a result Python value. For
meta-programming purposes, Taichi provides an advanced environment for
compile-time evaluation: ti.static(), where more operations are supported.A Python value only exists at compile time. After compile-time evaluation, all
the remaining expressions will be evaluated to Taichi values at runtime.A Taichi value has a Taichi type, which is one of the following:A primitive type, as described in Type systemA compound type, as described in Type systemAn ndarray type, as introduced in Tutorial: Run Taichi programs in C++ applicationA sparse matrix builder type, as introduced in Sparse
MatrixnoteAn informal quick summary of evaluation rules:Python value + Python value = Python valuePython value + Taichi value = Taichi valueTaichi value + Taichi value = Taichi valueVariables and scopeA variable contains a name, a type and a value. In Taichi, a variable can
be defined in the following ways:A parameter. The name of the variable is the parameter name. The type of the
variable is the parameter type annotation. The value of the variable is passed
in at runtime.An assignment statement, if the name on the
left-hand side appears for the first time. The name of the variable is the name
on the left-hand side. If there is a type annotation on the left-hand side, the
type of the variable is the type annotation; otherwise, the type of the
variable is inferred from the expression on the right-hand side. The value of
the variable is the evaluation result of the expression on the right-hand side
at runtime.Taichi is statically-typed. That is, you cannot change the type of a variable
after its definition. However, you can change the value of a variable if there
is another assignment statement after its definition.Taichi adopts lexical scope.
Therefore, if a variable is defined in a block, it is
invisible outside that block.Common rules of binary operationsFollowing the Values and types section, if both operands
of a binary operation are Python values, compile-time evaluation is triggered
and a result Python value is produced. If only one operand is a Python value,
it is first turned into a Taichi value with
default type.
Now the only remaining case is that both operands are Taichi values.Binary operations can happen between Taichi values of either primitive type or
compound type. There are three cases in total:Two primitive type values. The return type is also a primitive type.One primitive type value and one compound type value. The primitive type
value is first broadcast into the shape of the compound type value. Now it
belongs to the case of two compound type values.Two compound type values. For operators other than matrix multiplication,
both values are required to have the same shape, and the operator is conducted
element-wise, resulting in a compound type value with same shape.ExpressionsThe section explains the syntax and semantics of expressions in Taichi.AtomsAtoms are the most basic elements of expressions. The simplest atoms are
identifiers or literals. Forms enclosed in parentheses, brackets or braces
are also categorized syntactically as atoms.atom      ::= identifier | literal | enclosureenclosure ::= parenth_form | list_display | dict_displayCopyIdentifiers (Names)Lexical definition of
identifiers
(also referred to as names) in Taichi follows Python.There are three cases during evaluation:The name is visible and corresponds to a variable defined in Taichi. Then the
evaluation result is the value of the variable at runtime.The name is only visible in Python, i.e., the name binding is outside Taichi.
Then compile-time evaluation is triggered, resulting in the Python value bound
to that name.The name is invisible. Then a TaichiNameError is thrown.LiteralsTaichi supports integer
and floating-point
literals, whose lexical definitions follow Python.literal ::= integer | floatnumberCopyLiterals are evaluated to Python values at compile time.Parenthesized formsparenth_form ::= "(" [expression_list] ")"CopyA parenthesized expression list is evaluated to whatever the expression list is
evaluated to. An empty pair of parentheses is evaluated to an empty tuple at
compile time.List and dictionary displaysTaichi supports
displays
for container (list and dictionary only) construction. Like in Python, a
display is one of:listing the container items explicitly;providing a comprehension (a set of looping and filtering instructions) to
compute the container items.list_display       ::= "[" [expression_list | list_comprehension] "]"list_comprehension ::= assignment_expression comp_fordict_display       ::= "{" [key_datum_list | dict_comprehension] "}"key_datum_list     ::= key_datum ("," key_datum)* [","]key_datum          ::= expression ":" expressiondict_comprehension ::= key_datum comp_forcomp_for           ::= "for" target_list "in" or_test [comp_iter]comp_iter          ::= comp_for | comp_ifcomp_if            ::= "if" or_test [comp_iter]CopyThe semantics of list and dict displays in Taichi mainly follow Python. Note
that they are evaluated at compile time, so all expressions in comp_for,
as well as keys in key_datum, are required to be evaluated to Python values.For example, in the following code snippet, a can be successfully defined
while b cannot because p cannot be evaluated to a Python value at compile
time.@ti.kerneldef test(p: ti.i32):    a = ti.Matrix([i * p for i in range(10)])  # valid    b = ti.Matrix([i * p for i in range(p)])  # compile errorCopyPrimariesPrimaries represent the most tightly bound operations.primary ::= atom | attributeref | subscription | slicing | callCopyAttribute referencesattributeref ::= primary "." identifierCopyAttribute references are evaluated at compile time. The primary must be
evaluated to a Python value with an attribute named identifier. Common use
cases in Taichi include metadata queries of
field and
matrices.Subscriptionssubscription ::= primary "[" expression_list "]"CopyIf primary is evaluated to a Python value (e.g., a list or a dictionary),
then all expressions in expression_list are required to be evaluated to
Python values, and the subscription is evaluated at compile time following
Python.Otherwise, primary has a Taichi type. All Taichi types excluding primitive
types support subscriptions. You can refer to documentation of these types
for subscription usage.Slicingsslicing      ::= primary "[" slice_list "]"slice_list   ::= slice_item ("," slice_item)* [","]slice_item   ::= expression | proper_sliceproper_slice ::= [expression] ":" [expression] [ ":" [expression] ]CopyCurrently, slicings are only supported when primary has a Taichi matrix type,
and the evaluation happens at compile time.
When slice_item is in the form of:a single expression: it is required to be evaluated to a Python value
unless ti.init(dynamic_index=True) is set.proper_slice: all expressions (the lower bound, the upper bound, and the
stride) inside have to be evaluated to Python values.Callscall                 ::= primary "(" [positional_arguments] ")"positional_arguments ::= positional_item ("," positional_item)*positional_item      ::= assignment_expression | "*" expressionCopyThe primary must be evaluated to one of:A Taichi function.A Taichi builtin function.A Taichi primitive type. In this case, the positional_arguments must only
contain one item. If the item is evaluated to a Python value, then the
primitive type serves as a type annotation for a literal, and the Python value
will be turned into a Taichi value with that annotated type. Otherwise, the
primitive type serves as a syntax sugar for ti.cast(), but the item cannot
have a compound type.A Python callable object. If not inside a static expression, a warning is produced.The power operatorpower ::= primary ["**" u_expr]CopyThe power operator has the same semantics as the builtin pow() function.Unary arithmetic and bitwise operationsu_expr ::= power | "-" power | "+" power | "~" powerCopySimilar to rules for binary operations,
if the operand is a Python value, compile-time evaluation is triggered and a
result Python value is produced. Now the remaining case is that the operand is
a Taichi value:If the operand is a primitive type value, the return type is also a primitive
type.If the operand is a compound type value, the operator is conducted
element-wise, resulting in a compound type value with same shape.See arithmetic operators and
bitwise operators for operator details.
Note that ~ can only be used with integer type values.Binary arithmetic operationsm_expr ::= u_expr | m_expr "*" u_expr | m_expr "@" m_expr | m_expr "//" u_expr | m_expr "/" u_expr | m_expr "%" u_expra_expr ::= m_expr | a_expr "+" m_expr | a_expr "-" m_exprCopySee common rules for binary operations,
implicit type casting in binary operations,
and arithmetic operators. Note that
the @ operator is for matrix multiplication and only operates on matrix type
arguments.Shifting operationsshift_expr::= a_expr | shift_expr ( "<<" | ">>" ) a_exprCopySee common rules for binary operations,
implicit type casting in binary operations,
and bitwise operators. Note that both operands
are required to have integer types.Binary bitwise operationsand_expr ::= shift_expr | and_expr "&" shift_exprxor_expr ::= and_expr | xor_expr "^" and_expror_expr  ::= xor_expr | or_expr "|" xor_exprCopySee common rules for binary operations,
implicit type casting in binary operations,
and bitwise operators. Note that both operands
are required to have integer types.Comparisonscomparison    ::= or_expr (comp_operator or_expr)*comp_operator ::= "<" | ">" | "==" | ">=" | "<=" | "!=" | ["not"] "in"CopyComparisons can be chained arbitrarily, e.g., x < y <= z is equivalent to (x < y) & (y <= z).Value comparisonsSee common rules for binary operations,
implicit type casting in binary operations,
and comparison operators.Membership test operationsThe semantics of membership test operations follow
Python,
but they are only supported in static expressions.Boolean operationsor_test  ::= and_test | or_test "or" and_testand_test ::= not_test | and_test "and" not_testnot_test ::= comparison | "not" not_testCopyWhen the operator is inside a static expression,
the evaluation rule of the operator follows Python.
Otherwise, the behavior depends on the short_circuit_operators option of ti.init():If short_circuit_operators is False (default), a logical and will be
treated as a bitwise AND, and a logical or will be treated as a bitwise
OR. See binary bitwise operations for details.If short_circuit_operators is True, the normal short circuiting behavior
is adopted, and the operands are required to be boolean values. Since Taichi
does not have boolean type yet, ti.i32 is served as a temporary alternative.
A ti.i32 value is considered False if and only if the value is evaluated to 0.Assignment expressionsassignment_expression ::= [identifier ":="] expressionCopyAn assignment expression assigns an expression to an identifier (see
assignment statements for more details),
while also returning the value of the expression.Example:@ti.kerneldef foo() -> ti.i32:    b = 2 + (a := 5)    b += a    return b# the return value should be 12CopynoteThis operator is supported since Python 3.8.Conditional expressionsconditional_expression ::= or_test ["if" or_test "else" expression]expression             ::= conditional_expressionCopyThe expression x if C else y first evaluates the condition, C rather than x.
If C is True (the meaning of True and False has been mentioned at boolean operations), x is evaluated and its value is returned; otherwise,y is evaluated and its value is returned.Static expressionsstatic_expression ::= "ti.static(" positional_arguments ")"CopyStatic expressions are expressions that are wrapped by a call to ti.static().
The positional_arguments is evaluated at compile time, and the items inside must be evaluated to Python values.ti.static() receives one or more arguments.When a single argument is passed in, it returns the argument.When multiple arguments are passed in, it returns a tuple containing all the arguments in the same order as they are passed.The static expressions work as a mechanism to trigger many metaprogramming functions in Taichi,
such as compile-time loop unrolling and compile-time branching.The static expressions can also be used to create aliases for Taichi fields and Taichi functions.Expression listsexpression_list ::= expression ("," expression)* [","]CopyExcept when part of a list display, an expression list containing at least one
comma is evaluated to a tuple at compile time. The component expressions are
evaluated from left to right.The trailing comma is required only to create a tuple with length 1; it is
optional in all other cases. A single expression without a trailing comma
is evaluated to the value of that expression.Simple statementsThis section explains the syntax and semantics of compound statements in Taichi. A simple statement is comprised within a single logical line. Several simple statements may occur on a single line separated by semicolons.simple_stmt ::= expression_stmt                | assert_stmt                | assignment_stmt                | augmented_assignment_stmt                | annotated_assignment_stmt                | pass_stmt                | return_stmt                | break_stmt                | continue_stmtCopyExpression statementsexpression_stmt    ::= expression_listCopyAn expression statement evaluates the expression list (which may be a single expression).Assignment statementsassignment_stmt ::= (target_list "=")+ expression_listtarget_list     ::= target ("," target)* [","]target          ::= identifier                    | "(" [target_list] ")"                    | "[" [target_list] "]"                    | attributeref                    | subscriptionCopyThe recursive definition of an assignment statement basically follows
Python,
with the following points to notice:According to the Variables and scope section, if a
target is an identifier appearing for the first time, a variable is defined
with that name and inferred type from the corresponding right-hand side
expression. If the expression is evaluated to a Python value, it will be turned
into a Taichi value with default type.If a target is an existing identifier, the corresponding right-hand side
expression must be evaluated to a Taichi value with the type of the
corresponding variable of that identifier. Otherwise, an implicit cast will
happen.Augmented assignment statementsaugmented_assignment_stmt ::= augtarget augop expression_listaugtarget                 ::= identifier | attributeref | subscriptionaugop                     ::= "+=" | "-=" | "*=" | "/=" | "//=" | "%=" |                              "**="| ">>=" | "<<=" | "&=" | "^=" | "|="CopyDifferent from Python, some augmented assignments (e.g., x[i] += 1) are automatically atomic in Taichi.Annotated assignment statementsannotated_assignment_stmt ::= identifier ":" expression "=" expressionCopyThe differences from normal assignment statements are:Only single identifier target is allowed.If the identifier appears for the first time, a variable is defined
with that name and type annotation (the expression after ":"). The right-hand
side expression is cast to a Taichi value with the annotated type.If the identifier already exists, the type annotation must be the same as the
type of the corresponding variable of the identifier.The assert statementAssert statements are a convenient way to insert debugging assertions into a program:assert_stmt ::= "assert" expression ["," expression]CopyAssert statements are currently supported on the CPU, CUDA, and Metal backends.Assert statements only work in debug mode (when debug=True is set in the arguments of ti.init()),
otherwise they are equivalent to no-op.The simple form, assert expression, raises TaichiAssertionError (which is a subclass of AssertionError)
when expression is equal to False, with the code of expression as the error message.The extended form, assert expression1, expression2, raises TaichiAssertionError when expression1 is equal to False,
with expression2 as the error message. expression2 must be a constant or a formatted string. The variables in the
formatted string must be scalars.The pass statementpass_stmt ::= "pass"Copypass is a null operation — when it is executed, nothing happens.
It is useful as a placeholder when a statement is required syntactically, but no code needs to be executed.The return statementreturn_stmt ::= "return" [expression_list]CopyThe return statement may only occur once in a Taichi kernel or a Taichi function,
and it must be at the bottom of the function body.
Note that this is subject to change, and Taichi might relax it in the future.If a Taichi kernel or Taichi function has a return type hint,
it must have a return statement that returns a value other than None.If a Taichi kernel has a return statement that returns a value other than None, it must have a return type hint.
The return type hint for Taichi function is optional but recommended.
Note that this is subject to change, and Taichi might enforce it in the future.A kernel can have at most one return value, which can be a scalar, ti.Matrix, or ti.Vector,
and the number of elements in the return value must not exceed 30.
Note that this number is an implementation detail, and Taichi might relax it in the future.A Taichi function can have multiple return values in a return statement,
and the return values can be scalar, ti.Vector, ti.Matrix, ti.Struct, and more.The break statementbreak_stmt ::= "break"CopyThe break statement may only occur syntactically nested in a for or while loop, and it terminates the nearest enclosing loop.Break statement is not allowed when the nearest enclosing loop is a parallel range/ndrange for loop,
a struct for loop, or a mesh for loop.The continue statementcontinue_stmt ::= "continue"CopyThe continue statement may only occur syntactically nested in a for or while loop,
and it continues with the next cycle of the nearest enclosing loop.Compound statementsThis section explains the syntax and semantics of compound statements in Taichi.A compound statement consists of one or more clauses.
A clause consists of a header and a suite.
The clause headers of a particular compound statement are all at the same indentation level.
Each clause header begins with a uniquely identifying keyword and ends with a colon.
A suite is a group of statements controlled by a clause.compound_stmt ::= if_stmt | while_stmt | for_stmtsuite         ::= stmt_list NEWLINE | NEWLINE INDENT statement+ DEDENTstatement     ::= stmt_list NEWLINE | compound_stmtstmt_list     ::= simple_stmt (";" simple_stmt)* [";"]CopyThe difference between the compound statements in Taichi and Python is that Taichi introduces
compile time evaluation. If the expression in the clause header is a static expression,
Taichi replaces the compound statement at compile time according to the evaluation result of the expression.The if statementThe if statement is used for conditional execution:if_stmt ::= "if" (static_expression | assignment_expression) ":" suite            ("elif" (static_expression | assignment_expression) ":" suite)*            ["else" ":" suite]CopyThe elif clause is a syntax sugar for a if statement inside a else clause.
For example:if cond_a:    body_aelif cond_b:    body_belif cond_c:    body_celse:    body_dCopyis equivalent toif cond_a:    body_aelse:    if cond_b:        body_b    else:        if cond_c:            body_c        else:            body_dCopyTaichi first transforms elif clause as above, and then deal with the if statement with only an if clause and possibly an else clause as below.If the expression of the if clause is found to be true (see section Boolean operations for the definition of true and false),
the suite of the if clause is executed. Otherwise, the suite of the else clause, if present, is executed.An if statement whose expression is a static expression is called a static if statement.
The expression of a static if clause is evaluated at compile time, and it replaces the compound statement as below at compile time.If the static expression is found to be true, the suite of the if clause replaces the static if statement.If the static expression is found to be false, and there is an else clause, the suite of the else clause replaces the static if statement.If the static expression is found to be false, and there is no else clause, a pass statement replaces the static if statement.The while statementThe while statement is used for repeated execution as long as an expression is true:while_stmt ::= "while" assignment_expression ":" suiteCopyThis repeatedly tests the expression and, if it is true, executes the suite;
if the expression is false (which may be the first time it is tested) the loop terminates.A break statement executed in the suite terminates the loop.
A continue statement executed in the suite skips the rest of the suite and
goes back to testing the expression.The for statementThe for statement in Taichi is used to iterate over a range of numbers, multidimensional ranges, or the indices of elements in a field.for_stmt        ::= "for" target_list "in" iter_expression ":" suiteiter_expression ::= static_expression | expressionCopyTaichi does not support else clause in for statements.The for loops can iterate in parallel if they are in the outermost scope.
When a for loop is parallelized, the order of iteration is not determined,
and it cannot be terminated by break statements.Taichi uses ti.loop_config function to set directives for the loop right after it.
You can write ti.loop_config(serialize=True) before a range/ndrange for loop to let it run serially,
then it can be terminated by break statements.There are four kinds of for statements:The range for statementThe ndrange for statementThe struct for statementThe static for statementThe range for statementThe range for statement iterates over a range of numbers.The iter_expression of range for statement must be like range(start, stop) or range(stop),
and they mean the same as the Python range function,
except that the step argument is not supported.The target_list of range for statement must be an identifier which
is not occupied in the current scope.The range for loops are by default parallelized when the loops are in the outermost scope.The ndrange for statementThe ndrange for iterates over multidimensional ranges.The iter_expression of ndrange for statement must be a call to ti.ndrange() or a nested call to ti.grouped(ti.ndrange()).If the iter_expression is a call to ti.range(), it is a normal ndrange for.If the iter_expression is a call to ti.grouped(ti.range()), it is a grouped ndrange for.You can use grouped for loops to write dimensionality-independent programs.ti.ndrange receives arbitrary numbers of arguments.
The k-th argument represents the iteration range of the k-th dimension,
and the loop iterates over the direct product of the iteration range of each dimension.Every argument must be an integer or a tuple of two integers.If the k-th argument is an integer stop, the range of the k-th dimension
is equivalent to the range of range(stop) in Python.If the k-th argument is a tuple of two integers (start, stop), the range of the k-th dimension
is equivalent to the range of range(start, stop) in Python.The target_list of an n-dimensional normal ndrange for statement must be n different identifiers which
are not occupied in the current scope, and the k-th identifier is assigned an integer which is the loop variable of the k-th dimension.The target_list of an n-dimensional grouped ndrange for statement must be one identifier which
is not occupied in the current scope, and the identifier is assigned a ti.Vector with length n, which contains the loop variables of all n dimensions.The ndrange for loops are by default parallelized when the loops are in the outermost scope.The struct for statementThe struct for statement iterates over every active elements in a Taichi field.The iter_expression of a struct for statement must be a Taichi field or a call to ti.grouped(x) where x is a Taichi field.If the iter_expression is a Taichi field, it is a normal struct for.If the iter_expression is a call to ti.grouped(x) where x is a Taichi field, it is a grouped struct for.The target_list of a normal struct for statement on an n-dimensional field must be n different identifiers which
are not occupied in the current scope, and the k-th identifier is assigned an integer which is the loop variable of the k-th dimension.The target_list of a grouped struct for statement on an n-dimensional field must be one identifier which
is not occupied in the current scope, and the identifier is assigned a ti.Vector with length n, which contains the loop variables of all n dimensions.The struct for statement must be at the outermost scope of the kernel,
and it cannot be terminated by a break statement even when it is run serially.The static for statementThe static for statement unrolls a range/ndrange for loop at compile time.If the iter_expression of the for statement is a static_expression,
the for statement is a static for statement.The positional_arguments of the static_expression must meet the requirement on
iter_expression of the range/ndrange for.For example,for i in ti.static(range(5)):    print(i)Copyis unrolled toprint(0)print(1)print(2)print(3)print(4)Copyat compile time.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?IntroductionBasic conceptsValues and typesVariables and scopeCommon rules of binary operationsExpressionsAtomsPrimariesThe power operatorUnary arithmetic and bitwise operationsBinary arithmetic operationsShifting operationsBinary bitwise operationsComparisonsBoolean operationsAssignment expressionsConditional expressionsStatic expressionsExpression listsSimple statementsExpression statementsAssignment statementsThe assert statementThe pass statementThe return statementThe break statementThe continue statementCompound statementsThe if statementThe while statementThe for statementCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Syntax Sugars | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesLanguage ReferenceSyntax SugarsGlobal SettingsOperatorsDifferences between Taichi and Python ProgramsSIMT IntrinsicsInternalsFAQGlossaryDoc Home>>References>>Syntax SugarsVersion: v1.6.0On this pageSyntax SugarsAliasesCreating aliases for global variables and functions with cumbersome
names can sometimes improve readability. In Taichi, this can be done by
assigning kernel and function local variables with ti.static(), which
forces Taichi to use standard python pointer assignment.For example, consider the simple kernel:@ti.kerneldef my_kernel():    for i, j in field_a:        field_b[i, j] = some_function(field_a[i, j])CopyThe fields and function be aliased to new names with ti.static:@ti.kerneldef my_kernel():    a, b, fun = ti.static(field_a, field_b, some_function)    for i, j in a:        b[i, j] = fun(a[i, j])CopyAliases can also be created for class members and methods, which can
help prevent cluttering objective data-oriented programming code with
self.For example, consider class kernel to compute the 2-D laplacian of some field:@ti.kerneldef compute_laplacian(self):  for i, j in a:    self.b[i, j] = (self.a[i + 1, j] - 2.0*self.a[i, j] + self.a[i-1, j])/(self.dx**2) \                 + (self.a[i, j + 1] - 2.0*self.a[i, j] + self.a[i, j-1])/(self.dy**2)CopyUsing ti.static(), it can be simplified to:@ti.kerneldef compute_laplacian(self):    a, b, dx, dy = ti.static(self.a, self.b, self.dx, self.dy)    for i, j in a:        b[i, j] = (a[i+1, j] - 2.0*a[i, j] + a[i-1, j])/(dx**2) \                + (a[i, j+1] - 2.0*a[i, j] + a[i, j-1])/(dy**2)Copynoteti.static can also be used in combination with:if (compile-time
branching) andfor (compile-time unrolling)See Metaprogramming for more details.Here, we are using it for compile-time const values, i.e. the
field/function handles are constants at compile time.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?AliasesCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Global Settings | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesLanguage ReferenceSyntax SugarsGlobal SettingsOperatorsDifferences between Taichi and Python ProgramsSIMT IntrinsicsInternalsFAQGlossaryDoc Home>>References>>Global SettingsVersion: v1.6.0On this pageGlobal SettingsEvery Taichi program starts with ti.init(). You can customize your Taichi runtime by passing arguments to ti.init() or setting environment variables. Each argument or environment variable controls one specific behavior of the Taichi runtime. For example, the argument arch specifies the backend, and the argument debug decides whether to run the program in debug mode.Taichi executes the following to initialize a specific configuration in the ti.init() method call. Taking the arch argument as an example:Taichi first looks for the arguments passed to ti.init(). In this case, after Taichi reads ti.init(arch=cuda), it chooses CUDA as the backend and ignores the corresponding environment variable for backend setting, namely, TI_ARCH.If no argument is found, Taichi checks the corresponding environment variable. In this case, if arch is not specified but the environment variable is set to export TI_ARCH=cuda, Taichi still chooses CUDA as the backend.If no customized setting is found, Taichi uses a default configuration. In this case, if neither the argument arch is specified nor an environment variable TI_ARCH is found, Taichi adopts the default backend arch=ti.cpu.Following are some frequently-used configurations in ti.init():Customize Taichi runtime via arguments[Backend Options]    arch: [ti.cpu, ti.gpu, ti.cuda, ti.vulkan, ...]        Specify which architecture to use.    device_memory_GB: float        Specify the pre-allocated memory size for CUDA.[Compilation Options]    advanced_optimization: bool        Enable/disable advanced optimization. Turning off the setting can save compile time and reduce possible errors.    fast_math: bool        Enable/disable fast math. Turning off the setting can prevent possible undefined math behavior.    print_ir: bool        Turn on/off the printing of the intermediate IR generated.[Runtime Options]    cpu_max_num_threads: int        Set the number of threads used by the CPU thread pool.    debug: bool        Run your program in debug mode.    default_cpu_block_dim: int        Set the number of threads in a block on CPU.    default_gpu_block_dim: int        Set the number of threads in a block on GPU.    default_fp: [ti.f32, ti.f64]        Set the default precision of floating-point numbers in the Taichi scope.    default_ip: [ti.i32, ti.i64]        Set the default precision of integers in the Taichi scope.    kernel_profiler: bool        Turn on/off kernel performance profiling.    offline_cache: bool        Enable/disable offline cache of the compiled kernels.    offline_cache_file_path: str        Set a directory for holding the offline cached files.    random_seed: int        Set a custom seed for the random number generator.[Logging Options]    log_level: [ti.INFO, ti.TRACE, ti.WARN, ti.ERROR, ti.CRITICAL, ti.DEBUG]        Set the logging level.    verbose: bool        Turn on/off verbose outputs. For example, `ti.init(verbose=False)` prevents verbose outputs.[Develop Options]    gdb_trigger: bool        Enable/disable GDB when Taichi crashes. For example, `ti.init(gdb_trigger=True)` enables GDB.CopyCustomize Taichi runtime via environment variablesTaichi allows a number of environment variables for runtime customization. Some provide an alternative to the ti.init() arguments listed above. For example, as mentioned above, arch is interchangeable with TI_ARCH. But not all of the environment variables can be replaced by arguments, and vice versa.[Backend Options]    CUDA_VISIBLE_DEVICES        Specify which GPU to use for CUDA: `export CUDA_VISIBLE_DEVICES=[gpuid]`.    TI_ARCH        Specify which architecture to run the program. For example, `export TI_ARCH=cuda` designates CUDA as the backend.    TI_ENABLE_[CUDA/OPENGL/...]        Disable a backend upon startup. For example, set `export TI_ENABLE_CUDA=0` to disable the CUDA backend.    TI_VISIBLE_DEVICE        Specify which GPU to use for VULKAN: `export TI_VISIBLE_DEVICES=[gpuid]`.[Runtime Options]    TI_DEBUG        Turn on/off debug mode. For example, `export TI_DEBUG=1` activates debug mode.    TI_ENABLE_TORCH        Enable/disable the import of torch upon startup. For example, `export TI_ENABLE_TORCH=0` prohibits the use of torch.        The default value is 1.    TI_ENABLE_PADDLE        Enable/disable the import of paddle upon startup. For example, `export TI_ENABLE_PADDLE=0` prohibits the use of paddle.        The default value is 1.[Develop Options]    TI_CACHE_RUNTIME_BITCODE        Enable/disable the caching of compiled runtime bitcode in developer mode. For example, `export TI_CACHE_RUNTIME_BITCODE=1` enables the program to cache compiled runtime bitcode.        Turning off the setting can save startup time.    TI_TEST_THREADS        Specify the number of threads to run a test. For example, set `export TI_TEST_THREADS=4` to allocate four threads.        Alternatively, you can run `python tests/run_tests.py -t4`.[Logging Options]    TI_LOG_LEVEL        Set the logging level. For example, `export TI_LOG_LEVEL=trace` enables the TRACE level.CopyBackendsTo specify which architecture to use: ti.init(arch=ti.cuda) designates CUDA as the backend. This argument is equivalent to the environment variable TI_ARCH.To specify the pre-allocated memory size for CUDA: ti.init(device_memory_GB=0.5) allocates 0.5 GB size of memory.To specify which GPU to use for CUDA: export CUDA_VISIBLE_DEVICES=[gpuid].To specify which GPU to use for VULKAN: export TI_VISIBLE_DEVICE=[gpuid].To disable a backend (CUDA, METAL, OPENGL) upon startup: For example, export TI_ENABLE_CUDA=0 disables CUDA.noteIf you want to use CUDA and Taichi's GGUI system at the same time on a machine with multiple GPU cards, ensure that CUDA_VISIBLE_DEVICES matches TI_VISIBLE_DEVICE. In principle, CUDA_VISIBLE_DEVICES and TI_VISIBLE_DEVICE should point to the same GPU device identified with UUID. Use nvidia-smi -L to retrieve the details of your GPU devices.CompilationTo disable advanced optimization: ti.init(advanced_optimization=False), which helps save compile time and reduce possible errors.To disable fast math: ti.init(fast_math=False), which helps prevent possible undefined math behavior.To print the intermediate IR generated: ti.init(print_ir=True). Note that compiled kernels are cached by default. To force compilation and IR emission, use ti.init(print_ir=True, offline_cache=False).RuntimeTo restart the entire Taichi system (and erase all fields and kernels): ti.reset().To start a program in debug mode: ti.init(debug=True). Alternatively, you can set the environment variable TI_DEBUG or run your code via ti debug your_script.py.To disable the import of torch upon startup: export TI_ENABLE_TORCH=0.To disable the import of paddle upon startup: export TI_ENABLE_PADDLE=0.To set a custom seed for the random number generator used by ti.random(): ti.init(random_seed=seed). seed should be an integer. An example: ti.init(random_seed=int(time.time())).To set the default precision of floating-point numbers of Taichi runtime to ti.f64: ti.init(default_fp=ti.i64).To set the default precision of floating-point numbers of Taichi runtime to ti.i32: ti.init(default_ip=ti.i32).To disable the offline cache of compiled kernels: ti.init(offline_cache=False). See the Offline cache for more information.To enable the use of variables as indices to access vector/matrix elements in the Taichi scope: ti.init(dynamic_index=True).To turn on kernel profiling: ti.init(kernel_profiler=True). See the Profiler for more information.LoggingTo set the logging level: ti.init(log_level=ti.TRACE) or ti.set_logging_level(ti.TRACE) enables the TRACE level. The environment variable TI_LOG_LEVEL serves the same purpose.To eliminate verbose outputs: ti.init(verbose=False).DevelopTo trigger GDB when Taichi crashes: ti.init(gdb_trigger=True).To cache compiled runtime bitcode in dev mode: export TI_CACHE_RUNTIME_BITCODE=1, which saves startup time.To allocate four threads to run a test: export TI_TEST_THREADS=4 or python tests/run_tests.py -t4.noteIf ti.init is called twice, the configuration in the first call is discarded. For example:ti.init(debug=True)print(ti.cfg.debug)  # Trueti.init()print(ti.cfg.debug)  # FalseCopyGoing high precisionThe default setting fast_math=True may cause unexpected precision problems, which can be hard to debug. For example, the function ti.sqrt (and those that rely on it, such as ti.norm) may have a lower precision on the CUDA backend than on the CPU backends. See this issue for more information. To work around this problem, set the default precision to ti.f64 and turn off the fast_math option:ti.init(default_fp=ti.f64, fast_math=False)CopyIf you encounter other precision loss problems, set ti.init(fast_math=False) and see if it works. If not, report an issue to us.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?BackendsCompilationRuntimeLoggingDevelopGoing high precisionCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Operators | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesLanguage ReferenceSyntax SugarsGlobal SettingsOperatorsDifferences between Taichi and Python ProgramsSIMT IntrinsicsInternalsFAQGlossaryDoc Home>>References>>OperatorsVersion: v1.6.0On this pageOperatorsHere we present the supported operators in Taichi for both primitive types and
compound types such as matrices.Supported operators for primitive typesArithmetic operatorsOperationResult-aa negated+aa unchangeda + bsum of a and ba - bdifference of a and ba * bproduct of a and ba / bquotient of a and ba // bfloored quotient of a and ba % bremainder of a / ba ** ba to the power of bnoteThe % operator in Taichi follows the Python style instead of C style,
e.g.,# In Taichi-scope or Python-scope:print(2 % 3)   # 2print(-2 % 3)  # 1CopyFor C-style mod (%), please use ti.raw_mod. This function also receives floating points as arguments.ti.raw_mod(a, b) returns a - b * int(float(a) / b).print(ti.raw_mod(2, 3))      # 2print(ti.raw_mod(-2, 3))     # -2print(ti.raw_mod(3.5, 1.5))  # 0.5CopynotePython3 distinguishes / (true division) and // (floor division), e.g., 1.0 / 2.0 = 0.5, 1 / 2 = 0.5, 1 // 2 = 0,
4.2 // 2 = 2. Taichi follows the same design:True divisions on integral types first cast their
operands to the default floating point type.Floor divisions on floating point types first cast their
operands to the default integral type.To avoid such implicit casting, you can manually cast your operands to
desired types, using ti.cast. Please see
Default precisions for more details on
default numerical types.Taichi also provides ti.raw_div function which performs true division if one of the operands is floating point type
and performs floor division if both operands are integral types.print(ti.raw_div(5, 2))    # 2print(ti.raw_div(5, 2.0))  # 2.5CopyComparison operatorsOperationResulta == bif a is equal to b, then True, else Falsea != bif a is not equal to b, then True, else Falsea > bif a is strictly greater than b, then True, else Falsea < bif a is strictly less than b, then True, else Falsea >= bif a is greater than or equal to b, then True, else Falsea <= bif a is less than or equal to b, then True, else FalseLogical operatorsOperationResultnot aif a is False, then True, else Falsea or bif a is False, then b, else aa and bif a is False, then a, else bConditional operationsThe result of conditional expression a if cond else b is a if cond is True, or b otherwise.
a and b must have a same type.The conditional expression does short-circuit evaluation, which means the branch not chosen is not evaluated.a = ti.field(ti.i32, shape=(10,))for i in range(10):    a[i] = i@ti.kerneldef cond_expr(ind: ti.i32) -> ti.i32:    return a[ind] if ind < 10 else 0cond_expr(3)  # returns 3cond_expr(10)  # returns 0, a[10] is not evaluatedCopyFor element-wise conditional operations on Taichi vectors and matrices,
Taichi provides ti.select(cond, a, b) which does not do short-circuit evaluation.cond = ti.Vector([1, 0])a = ti.Vector([2, 3])b = ti.Vector([4, 5])ti.select(cond, a, b)  # ti.Vector([2, 5])CopyBitwise operatorsOperationResult~athe bits of a inverteda & bbitwise and of a and ba ^ bbitwise exclusive or of a and ba | bbitwise or of a and ba << bleft-shift a by b bitsa >> bright-shift a by b bitsnoteThe >> operation denotes the
Shift Arithmetic Right (SAR) operation.
For the Shift Logical Right (SHR) operation,
consider using ti.bit_shr(). For left shift operations, SAL and SHL are the
same.Trigonometric functionsti.sin(x)ti.cos(x)ti.tan(x)ti.asin(x)ti.acos(x)ti.atan2(x, y)ti.tanh(x)CopyOther arithmetic functionsti.sqrt(x)ti.rsqrt(x)  # A fast version for `1 / ti.sqrt(x)`.ti.exp(x)ti.log(x)ti.round(x, dtype=None)ti.floor(x, dtype=None)ti.ceil(x, dtype=None)ti.sum(x)ti.max(x, y, ...)ti.min(x, y, ...)ti.abs(x)  # Same as `abs(x)`ti.pow(x, y)  # Same as `pow(x, y)` and `x ** y`CopyThe dtype argument in round, floor and ceil functions specifies the data type of the returned value. The default None means the returned type is the same as input x.Builtin-alike functionsabs(x)  # Same as `ti.abs(x, y)`pow(x, y)  # Same as `ti.pow(x, y)` and `x ** y`.CopyRandom number generatorti.random(dtype=float)Copynoteti.random supports u32, i32, u64, i64, and all floating point types.
The range of the returned value is type-specific.TypeRangei32-2,147,483,648 to 2,147,483,647u320 to 4,294,967,295i64-9,223,372,036,854,775,808 to 9,223,372,036,854,775,807u640 to 18,446,744,073,709,551,615floating point0.0 to 1.0Supported atomic operationsIn Taichi, augmented assignments (e.g., x[i] += 1) are automatically
atomic.cautionWhen modifying global variables in parallel, make sure you use atomic
operations. For example, to sum up all the elements in x,@ti.kerneldef sum():    for i in x:        # Approach 1: OK        total[None] += x[i]        # Approach 2: OK        ti.atomic_add(total[None], x[i])        # Approach 3: Wrong result since the operation is not atomic.        total[None] = total[None] + x[i]CopynoteWhen atomic operations are applied to local values, the Taichi compiler
will try to demote these operations into their non-atomic counterparts.Apart from the augmented assignments, explicit atomic operations, such
as ti.atomic_add, also do read-modify-write atomically. These
operations additionally return the old value of the first argument.
For example,x[i] = 3y[i] = 4z[i] = ti.atomic_add(x[i], y[i])# now x[i] = 7, y[i] = 4, z[i] = 3CopyBelow is a list of all explicit atomic operations:OperationBehaviorti.atomic_add(x, y)atomically compute x + y, store the result in x, and return the old value of xti.atomic_sub(x, y)atomically compute x - y, store the result in x, and return the old value of xti.atomic_and(x, y)atomically compute x & y, store the result in x, and return the old value of xti.atomic_or(x, y)atomically compute x | y, store the result in x, and return the old value of xti.atomic_xor(x, y)atomically compute x ^ y, store the result in x, and return the old value of xti.atomic_max(x, y)atomically compute max(x, y), store the result in x, and return the old value of xti.atomic_min(x, y)atomically compute min(x, y), store the result in x, and return the old value of xnoteSupported atomic operations on each backend:typeCPUCUDAOpenGLMetalC sourcei32✔️✔️✔️✔️✔️f32✔️✔️✔️✔️✔️i64✔️✔️⭕❌✔️f64✔️✔️⭕❌✔️(⭕ Requiring extensions for the backend.)Supported operators for matricesThe previously mentioned operations on primitive types can also be applied on
compound types such as matrices.
In these cases, they are applied in an element-wise manner. For example:B = ti.Matrix([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])C = ti.Matrix([[3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])A = ti.sin(B)# is equivalent tofor i in ti.static(range(2)):    for j in ti.static(range(3)):        A[i, j] = ti.sin(B[i, j])A = B ** 2# is equivalent tofor i in ti.static(range(2)):    for j in ti.static(range(3)):        A[i, j] = B[i, j] ** 2A = B ** C# is equivalent tofor i in ti.static(range(2)):    for j in ti.static(range(3)):        A[i, j] = B[i, j] ** C[i, j]A += 2# is equivalent tofor i in ti.static(range(2)):    for j in ti.static(range(3)):        A[i, j] += 2A += B# is equivalent tofor i in ti.static(range(2)):    for j in ti.static(range(3)):        A[i, j] += B[i, j]CopyIn addition, the following methods are supported matrices operations:a = ti.Matrix([[2, 3], [4, 5]])a.transpose()   # the transposed matrix of `a`, will not effect the data in `a`.a.trace()       # the trace of matrix `a`, the returned scalar value can be computed as `a[0, 0] + a[1, 1] + ...`.a.determinant() # the determinant of matrix `a`.a.inverse()     # (ti.Matrix) the inverse of matrix `a`.a@a             # @ denotes matrix multiplicationCopynoteFor now, determinant() and inverse() only works in Taichi-scope, and the
size of the matrix must be 1x1, 2x2, 3x3 or 4x4.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Supported operators for primitive typesArithmetic operatorsComparison operatorsLogical operatorsConditional operationsBitwise operatorsTrigonometric functionsOther arithmetic functionsBuiltin-alike functionsRandom number generatorSupported atomic operationsSupported operators for matricesCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Differences between Taichi and Python Programs | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesLanguage ReferenceSyntax SugarsGlobal SettingsOperatorsDifferences between Taichi and Python ProgramsSIMT IntrinsicsInternalsFAQGlossaryDoc Home>>References>>Differences between Taichi and Python ProgramsVersion: v1.6.0On this pageDifferences between Taichi and Python ProgramsAlthough Taichi uses Python as the frontend, it follows a different set of rules in many aspects, including:Taichi only supports return statement outside non-static if/for/while scope in the programVariables defined inside an if/for/while block cannot be accessed outside the block.Taichi does not fully support some language features of Python.Set, list, dictionary and operator inComprehensionsOperator isReturn statement and return type annotationIf a Taichi kernel/function does not have a return statement, it must not have return type annotation.If a Taichi kernel has a return statement, it must have return type annotation.If a Taichi function has a return statement, return type annotation is recommended, and it will be mandatory in the future.@ti.kerneldef error_kernel_no_return_annotation():    return 0  # Error: Have return statement but have no return type annotation@ti.kerneldef error_kernel_no_return() -> ti.i32:  # Error: Have return type annotation but have no return statement    pass@ti.funcdef error_func_no_return() -> ti.i32:  # Error: Have return type annotation but have no return statement    passCopyThe return statement can not be in a scope of non-static if/for/while.@ti.kerneldef error_return_inside_non_static_if(a: ti.i32) -> ti.i32:    if a:        return 1  # Error: Return statement inside if scopeCopyThe compiler discards code after the first return statement.@ti.kerneldef discarded_after_first_return(a: ti.i32) -> ti.i32:    return 1    if a:  # Discarded        return 1  # Discardeddiscarded_after_first_return(0)  # OK: returns 1CopyIf there are compile-time evaluations in the code, make sure there is a return statement under all circumstances.
Otherwise, error occurs when a branch is chosen which does not have return statement.@ti.kerneldef return_inside_static_if(a: ti.template()) -> ti.i32:    if ti.static(a):        return 1    return 0return_inside_static_if(1)  # OK: Returns 1return_inside_static_if(0)  # OK: Returns 0@ti.kerneldef return_inside_static_if_no_return_outside(a: ti.template()) -> ti.i32:    if ti.static(a):        return 1return_inside_static_if_no_return_outside(1)  # OK: Returns 1return_inside_static_if_no_return_outside(0)  # Error: No return statement@ti.kerneldef ok_return_inside_static_for() -> ti.i32:    a = 0    for i in ti.static(range(10)):  # Static for        a += i        if ti.static(i == 8):  # Static if            return a  # OK: Returns 36CopyVariable scopingIn Python, a variable defined inside an if/for/while block can be accessed outside the block.
However, in Taichi, the variables can only be accessed within the block it is defined.@ti.kerneldef error_access_var_outside_for() -> ti.i32:    for i in range(10):        a = i    return a  # Error: variable "a" not found@ti.kerneldef error_access_var_outside_if(a: ti.i32) -> ti.i32:    if a:        b = 1    else:        b = 2    return b  # Error: variable "b" not found@ti.kerneldef ok_define_var_before_if(a: ti.i32) -> ti.i32:    b = 0    if a:        b = 1    else:        b = 2    return b  # OK: "b" is defined before "if"ok_define_var_before_if(0)  # Returns 2CopyUnsupported/partially supported Python language featuresSet, list, dictionary and operator inCurrently, Taichi does not support set.List and dictionary before assigning to a variable works as the python list and dictionary.
However, after assigning to a variable, the content of the list and the values (not keys) of the dictionary are converted to Taichi variables.Taichi does not have a runtime implementation of in currently. Therefore, operator in and not in only works in  static scope (inside ti.static()).@ti.kerneldef list_without_assign() -> ti.i32:    if ti.static(1 in [1, 2]):  # [1, 2]        return 1    return 0list_without_assign()  # Returns 1@ti.kerneldef list_assigned() -> ti.i32:    a = [1, 2]  # a: [Variable(1), Variable(2)]    if ti.static(1 in a):  # 1 is not in [Variable(1), Variable(2)]        return 1    return 0list_assigned()  # Returns 0@ti.kerneldef error_non_static_in():    if i in [1, 2]:  # Error: Cannot use `in` outside static scope        passCopyComprehensionsTaichi partially supports list comprehension and dictionary comprehension,
but does not support set comprehension.For list comprehensions and dictionary comprehensions, the ifs and fors in them are evaluated at compile time.
The iterators and conditions are implicitly in static scope.Operator isCurrently, Taichi does not support operator is and is not.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Return statement and return type annotationVariable scopingUnsupported/partially supported Python language featuresSet, list, dictionary and operator inComprehensionsOperator isCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Internal Designs | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsInternal DesignsLife of a Taichi KernelFAQGlossaryDoc Home>>Internals>>Internal DesignsVersion: v1.6.0On this pageInternal DesignsIntermediate representation (IR)Taichi's computation IR is designed to beStatic-single assignment;Hierarchical, instead of LLVM-style control-flow graph + basic blocks;Differentiable;Statically and strongly typed.For example, a simple Taichi kernelshow_ir.pyimport taichi as titi.init(print_ir=True)@ti.kerneldef foo():    for i in range(10):        if i < 4:            print(i)foo()Copymay be compiled intokernel {  $0 = offloaded range_for(0, 10) grid_dim=0 block_dim=32  body {    <i32> $1 = loop $0 index 0    <i32> $2 = const [4]    <i32> $3 = cmp_lt $1 $2    <i32> $4 = const [1]    <i32> $5 = bit_and $3 $4    $6 : if $5 {      print $1, "\n"    }  }}CopynoteUse ti.init(print_ir=True) to print IR of all instantiated kernels.noteSee Life of a Taichi kernel for more details about
the JIT compilation system of Taichi.Data structure organizationThe internal organization of Taichi's data structure is defined using the Structural Node
("SNode", /snōd/) tree system. The SNode system might be confusing for new developers:
it is important to distinguish three concepts: SNode containers,
SNode cells, and SNode components.A SNode container can have multiple SNode cells. The numbers of
cells are recommended to be powers of two.For example, S = ti.root.dense(ti.i, 128) creates an SNode S, and each S container has 128 S cells.A SNode cell can have multiple SNode components.For example, P = S.dense(ti.i, 4); Q = S.dense(ti.i, 4) inserts two components (one P container and one Q container) into each S cell.Note that each SNode component is a SNode container of a lower-level SNode.A hierarchical data structure in Taichi, dense or sparse, is essentially a tree with interleaved container and cell levels.
Note that containers of place SNodes do not have cells. Instead, they
directly contain numerical values.Consider the following example:# misc/listgen_demo.pyx = ti.field(ti.i32)y = ti.field(ti.i32)z = ti.field(ti.i32)S0 = ti.rootS1 = S0.pointer(ti.i, 4)S2 = S1.dense(ti.i, 2)S2.place(x, y) # S3: x; S4: yS5 = S1.dense(ti.i, 2)S5.place(z) # S6: zCopyThe whole data structure is an S0root container, containing1x S0root cell, which has only one component, which
isAn S1pointer container, containing4x S1pointer cells, each with two components,
which areAn S2dense container, containing2x S2dense cells, each with two
components, which areAn S3place_x container which directly
contains a x: ti.i32 valueAn S4place_y container which directly
contains a y: ti.i32 valueAn S5dense container, containing2x S5dense cells, each with one
component, which isAn S6place container which directly
contains a z: ti.i32 valueThe following figure shows the hierarchy of the data structure. The
numbers are indices of the containers and cells.Note that the S0root container and cell do not have an index.In summary, we will have the following containers:1x S0root container1x S1pointer container4x S2dense containers4x S5dense containers8x S3place_x containers, each directly containing an i32 value8x S4place_y containers, each directly containing an i32 value8x S6place_z containers, each directly containing an i32 value... and the following cells:1x S0root cell4x S1pointer cells8x S2dense cells8x S5dense cellsAgain, note that S3place_x, S4place_y and S6place_z containers do not
have corresponding cells.In struct compilers of supported backends, each SNode has two types: container type and
cell type. Again, components of a higher level SNode cell are
containers of a lower level SNode.Note that cells are never exposed to end-users.List generation generates lists of SNode containers (instead of
SNode cells).noteWe are on our way to remove usages of children, instances, and
elements in Taichi. These are very ambiguous terms and should be replaced with standardized terms: container, cell, and component.List generationStruct-fors in Taichi loop over all active elements of a (sparse) data
structure in parallel. Evenly distributing work onto processor cores
is challenging on sparse data structures: naively splitting an irregular
tree into pieces can easily lead to partitions with drastically
different numbers of leaf elements.Our strategy is to generate lists of active SNode containers, layer by
layer. The list generation computation happens on the same device as
normal computation kernels, depending on the arch argument when the
user calls ti.init().List generations flatten the data structure leaf elements into a 1D
list, circumventing the irregularity of incomplete trees. Then we
can simply invoke a regular parallel for over the 1D list.For example,# misc/listgen_demo.pyimport taichi as titi.init(print_ir=True)x = ti.field(ti.i32)S0 = ti.rootS1 = S0.dense(ti.i, 4)S2 = S1.bitmasked(ti.i, 4)S2.place(x)@ti.kerneldef func():    for i in x:        print(i)func()Copygives you the following IR:$0 = offloaded clear_list S1dense$1 = offloaded listgen S0root->S1dense$2 = offloaded clear_list S2bitmasked$3 = offloaded listgen S1dense->S2bitmasked$4 = offloaded struct_for(S2bitmasked) block_dim=0 {  <i32 x1> $5 = loop index 0  print i, $5}CopyNote that func leads to two list generations:(Tasks $0 and $1) based on the list of the (only) S0root container,
generate the list of the (only) S1dense container;(Tasks $2 and $3) based on the list of S1dense containers,
generate the list of S2bitmasked containers.The list of S0root SNode always has exactly one container, so we
never clear or re-generate this list. Although the list of S1dense always
has only one container, we still regenerate the list for uniformity.
The list of S2bitmasked has 4 containers.noteThe list of place (leaf) nodes (e.g., S3 in this example) is never
generated. Instead, we simply loop over the list of their parent nodes,
and for each parent node we enumerate the place nodes on-the-fly
(without actually generating a list).The motivation for this design is to amortize list generation overhead.
Generating one list element per leaf node (place SNode) element is too
expensive, likely much more expensive than the essential computation
happening on the leaf element. Therefore we only generate their parent
element list, so that the list generation cost is amortized over
multiple child elements of a second-to-last-level SNode element.In the example above, although we have 16 instances of x, we only
generate a list of 4 x S2bitmasked nodes (and 1 x S1dense node).StatisticsIn some cases, it is helpful to gather certain quantitative information
about internal events during Taichi program execution. The Statistics
class is designed for this purpose.Usage:#include "taichi/util/statistics.h"// add 1.0 to counter "codegen_offloaded_tasks"taichi::stat.add("codegen_offloaded_tasks");// add the number of statements in "ir" to counter "codegen_statements"taichi::stat.add("codegen_statements", irpass::analysis::count_statements(this->ir));CopyNote the keys are std::string and values are double.To print out all statistics in Python:ti.core.print_stat()CopyWhy Python frontendEmbedding Taichi in python has the following advantages:Easy to learn. Taichi has a very similar syntax to Python.Easy to run. No ahead-of-time compilation is needed.This design allows people to reuse existing python infrastructure:IDEs. A python IDE mostly works for Taichi with syntax
highlighting, syntax checking, and autocomplete.Package manager (pip). A developed Taichi application and be
easily submitted to PyPI and others can easily set it up with
pip.Existing packages. Interacting with other python components
(e.g. matplotlib and numpy) is just trivial.The built-in AST manipulation tools in python allow us to flexibly
manipulate and analyze Python ASTs,
as long as the kernel body function is parse-able by the Python parser.However, this design has drawbacks too:Taichi kernels must be parse-able by Python parsers. This means Taichi
syntax cannot go beyond Python syntax.For example, indexing is always needed when accessing elements
in Taichi fields, even if the fields is 0D. Use x[None] = 123
to set the value in x if x is 0D. This is because x = 123
will set x itself (instead of its containing value) to be the
constant 123 in Python syntax. For code consistency in Python-
and Taichi-scope, we have to use the more verbose x[None] = 123 syntax.Python has relatively low performance. This can cause a performance
issue when initializing large Taichi fields with pure python
scripts. A Taichi kernel should be used to initialize huge fields.Virtual indices v.s. physical indicesIn Taichi, virtual indices are used to locate elements in fields, and
physical indices are used to specify data layouts in memory.For example,In a[i, j, k], i, j, and k are virtual indices.In for i, j in x:, i and j are virtual indices.ti.i, ti.j, ti.k, ti.l, ... are physical indices.In struct-for statements, LoopIndexStmt::index is a physical
index.The mapping between virtual indices and physical indices for each
SNode is stored in SNode::physical_index_position. I.e.,
physical_index_position[i] answers the question: which physical
index does the i-th virtual index correspond to?Each SNode can have a different virtual-to-physical mapping.
physical_index_position[i] == -1 means the i-th virtual index does
not correspond to any physical index in this SNode.SNode s in handy dense fields (i.e.,
a = ti.field(ti.i32, shape=(128, 256, 512))) have trivial
virtual-to-physical mapping, e.g. physical_index_position[i] = i.However, more complex data layouts, such as column-major 2D fields can
lead to SNodes with physical_index_position[0] = 1 and
physical_index_position[1] = 0.a = ti.field(ti.f32, shape=(128, 32, 8))b = ti.field(ti.f32)ti.root.dense(ti.j, 32).dense(ti.i, 16).place(b)ti.lang.impl.get_runtime().materialize() # This is an internal api for dev, we don't make sure it is stable for user.mapping_a = a.snode().physical_index_position()assert mapping_a == {0: 0, 1: 1, 2: 2}mapping_b = b.snode().physical_index_position()assert mapping_b == {0: 1, 1: 0}# Note that b is column-major:# the virtual first index exposed to the user comes second in memory layout.CopyTaichi supports up to 12 (constexpr int taichi_max_num_indices = 12)
virtual indices and physical indices.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Intermediate representation (IR)Data structure organizationList generationStatisticsWhy Python frontendVirtual indices v.s. physical indicesCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Life of a Taichi Kernel | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsInternal DesignsLife of a Taichi KernelFAQGlossaryDoc Home>>Internals>>Life of a Taichi KernelVersion: v1.6.0On this pageLife of a Taichi KernelSometimes it is helpful to understand the life cycle of a Taichi kernel.
In short, compilation will only happen on the first invocation of an
instance of a kernel.The life cycle of a Taichi kernel has the following stages:Kernel registrationTemplate instantiation and cachingPython AST transformsTaichi IR compilation, optimization, and executable generationLaunchingLet's consider the following simple kernel:@ti.kerneldef add(field: ti.template(), delta: ti.i32):    for i in field:        field[i] += deltaCopyWe allocate two 1D fields to simplify discussion:x = ti.field(dtype=ti.f32, shape=128)y = ti.field(dtype=ti.f32, shape=16)CopyKernel registrationWhen the ti.kernel decorator is executed, a kernel named add is
registered. Specifically, the Python Abstract Syntax Tree (AST) of the
add function will be memorized. No compilation will happen until the
first invocation of add.Template instantiation and cachingadd(x, 42)CopyWhen add is called for the first time, the Taichi frontend compiler
will instantiate the kernel.When you have a second call with the same template signature
(explained later), e.g.,add(x, 1)CopyTaichi will directly reuse the previously compiled binary.Arguments hinted with ti.template() are template arguments, and will
incur template instantiation. For example,add(y, 42)Copywill lead to a new instantiation of add.noteTemplate signatures are what distinguish different instantiations of
a kernel template. The signature of add(x, 42) is (x, ti.i32), which
is the same as that of add(x, 1). Therefore, the latter can reuse the
previously compiled binary. The signature of add(y, 42) is
(y, ti.i32), a different value from the previous signature, hence a
new kernel will be instantiated and compiled.noteMany basic operations in the Taichi standard library are implemented
using Taichi kernels using metaprogramming tricks. Invoking them will
incur implicit kernel instantiations.Examples include x.to_numpy() and y.from_torch(torch_tensor). When
you invoke these functions, you will see kernel instantiations, as
Taichi kernels will be generated to offload the hard work to multiple
CPU cores/GPUs.As mentioned before, the second time you call the same operation, the
cached compiled kernel will be reused and no further compilation is
needed.Code transformation and optimizationsWhen a new instantiation happens, the Taichi frontend compiler (i.e.,
the ASTTransformer Python class) will transform the kernel body AST
into a Python script, which, when executed, emits a Taichi frontend AST.
Basically, some patches are applied to the Python AST so that the Taichi
frontend can recognize it.The Taichi AST lowering pass translates Taichi frontend IR into
hierarchical static single assignment (SSA) IR, which allows a series of
further IR passes to happen, such asLoop vectorizationType inference and checkingGeneral simplifications such as common subexpression elimination
(CSE), dead instruction elimination (DIE), constant folding, and
store forwardingAccess loweringData access optimizationsReverse-mode automatic differentiation (if using differentiable
programming)Parallelization and offloadingAtomic operation demotionThe just-in-time (JIT) compilation engineFinally, the optimized SSA IR is fed into backend compilers such as LLVM
or Apple Metal/OpenGL shader compilers. The backend compilers then
generate high-performance executable CPU/GPU programs.Kernel launchingTaichi kernels will be ultimately launched as multi-threaded CPU tasks
or GPU kernels.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Kernel registrationTemplate instantiation and cachingCode transformation and optimizationsThe just-in-time (JIT) compilation engineKernel launchingCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Frequently Asked Questions | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQFrequently Asked QuestionsInstallation TroubleshootingGlossaryDoc Home>>FAQ>>Frequently Asked QuestionsVersion: v1.6.0On this pageFrequently Asked QuestionsInstallationWhy does my pip complain package not found when installing Taichi?You may have a Python interpreter with an unsupported version. Currently, Taichi only supports Python 3.7/3.8/3.9/3.10 (64-bit) . For more information about installation-specific issues, please check Installation Troubleshooting.Parallel programmingOuter-most loops in Taichi kernels are by default parallel. How can I serialize one of them?A solution is to add an additional ghost loop with only one iteration outside the loop you want to serialize.for _ in range(1):  # This "ghost" loop will be "parallelized", but with only one thread. Therefore, the containing loop below is serialized.    for i in range(100):  # The loop you want to serialize        ...CopyDoes Taichi provide a barrier synchronization function similar to __syncthreads() or glMemoryBarrier()?You can call ti.sync(), which is similar to CUDA's cudaStreamSynchronize(), in Taichi to synchronize the parallel for loops.__syncthreads() is a block-level synchronization barrier, and Taichi provides a synonymous API ti.simt.block.sync(), which for now supports CUDA and Vulkan backends only. However, all block-level APIs are still experimental, and you should use this API only when it relates to SIMT operation synchronization and SharedArray reads and writes.Data structuresHow do I declare a field with a dynamic length?The dynamic SNode supports variable-length fields. It acts similarly to std::vector in C++ or list in Python.tipAn alternative solution is to allocate a large enough dense field, with a corresponding 0-D field
field_len[None] tracking its length. In practice, programs allocating memory using dynamic
SNodes may be less efficient than using dense SNodes, due to dynamic data structure
maintenance overheads.How can I swap elements between two fields in the Taichi scope? a,b = b,a does not work.Direct value assignments lead to semantic ambiguity. For example, a = b can mean data copy if a is pre-defined, or otherwise can serve to define and initialize a.You can swap two fields in the Taichi scope using a struct for:a = ti.field(ti.i32, 32)b = ti.field(ti.i32, 32)@ti.funcdef field_copy(src: ti.template(), dst: ti.template()):    for I in ti.grouped(src):        dst[I] = src[I]@ti.kerneldef test():    # copy b to a    field_copy(b, a)    print(a[0])test()CopyHow do I compute the minimum/maximum of a field?Use ti.atomic_min/atomic_max instead of ti.min/max. For example:x = ti.field(ti.f32, 32)@ti.kerneldef x_min() -> ti.f32:    ret: ti.f32 = x[0]    for i in x:        ret = ti.atomic_min(ret, x[i])    return retx_min()CopyDoes Taichi support bool type?Currently, Taichi does not support the bool type.How do I program on less structured data structures (such as graphs and tetrahedral meshes) in Taichi?These structures have to be decomposed into 1D Taichi fields. For example, when representing a graph, you can allocate two fields, one for the vertices and the other for the edges. You can then traverse the elements using for v in vertices or for v in range(n).OperationsIn Taichi v1.3.0, the matmul result of a vector and a transposed vector gives a scalar instead of a matrix.Taichi distinguishes vectors from matrices starting from v1.3.0, as explained in the release note. transpose() on a vector is no longer allowed. Use a.outer_product(b), instead of a @ b.transpose(), to find the outer product of two vectors.Developement relatedCan I enable auto compeletion for Taichi?Yes, Taichi's Python user-facing APIs should work natively with any language server for Python.Take VSCode as an example, you can install Python or Pylance extensions to get language support like signature help with type information, code completion etc.If it doesn't work out of box after installing the extension, please make sure the right Python interpreter is selected by:invoke command palette (Shift + Command + P (Mac) / Ctrl + Shift + P (Windows/Linux))find Python: Select Interpretermake sure you select the path to the Python interpreter you're using with a taichi package installedHow to install Taichi on a server without Internet access?Follow these steps to install Taichi on a server without Internet access.From a computer with Internet access, pip download Taichi, ensuring that this computer has the same operating system as the target server:pip download taichiCopyThis command downloads the wheel package of Taichi and all its dependencies.Copy the downloaded .whl packages to your local server and install each with the following command. Note that you must* complete all dependency installation before installing Taichi.python -m pip install xxxx.whlCopyIntegration with other libs/softwaresWhat is the most convenient way to load images into Taichi fields?One feasible solution is field.from_numpy(ti.tools.imread('filename.png')).Can Taichi interact with other Python packages such as matplotlib?Yes, Taichi supports many popular Python packages. Taichi provides helper functions such as from_numpy and to_numpy to transfer data between Taichi fields and NumPy arrays, so that you can also use your favorite Python packages (e.g., numpy, pytorch, matplotlib) together with Taichi as below:import taichi as tiimport numpy as npimport matplotlib.pyplot as pltpixels = ti.field(ti.f32, (512, 512))def render_pixels():    arr = np.random.rand(512, 512)    pixels.from_numpy(arr)   # load numpy data into taichi fieldsrender_pixels()arr = pixels.to_numpy()  # store taichi data into numpy arraysplt.imshow(arr)plt.show()import matplotlib.cm as cmcmap = cm.get_cmap('magma')gui = ti.GUI('Color map', (512, 512))while gui.running:    render_pixels()    arr = pixels.to_numpy()    gui.set_image(cmap(arr))    gui.show()CopyBesides, you can also pass numpy arrays or torch tensors into a Taichi kernel as arguments. See Interacting with external arrays for more details.Can I integrate Taichi and Houdini?The answer is an unequivocal Yes! Our contributors managed to embed taichi_elements, a multi-material continuum physics engine, into Houdini as an extension, combining Houdini's flexibility in preprocessing with Taichi's strength in high-performance computation.You can follow the instructions provided here.Precision relatedHow do I accurately initialize a vector or matrix with f64 precision when my default floating-point precision (default_fp) is f32?To better understand the question, look at the program below:import taichi as titi.init()@ti.kerneldef foo():    A = ti.Vector([0.2, 0.0], ti.f64)    print('A =', A)    B = ti.Vector([ti.f64(0.2), 0.0], ti.f64)    print('B =', B)foo()CopyYou get the following output:A = [0.200000002980, 0.000000000000]B = [0.200000000000, 0.000000000000]CopyYou may notice the value of A is slightly different from [0.2, 0]. This is because, by default, your float literals are converted to ti.f32, and 0.2 in ti.f32 precision becomes 0.200000002980. If you expect A and B to have ti.f64 precision, use ti.f64(0.2) to preserve more effective digits here so that 0.2 keeps its ti.f64 type.Alternatively, if you can afford having all floating-point operations in f64 precision, you can directly initialize Taichi with ti.init(..., default_fp=ti.f64).From Python to TaichiWhy does it always return an error when I pass a list from the Python scope to a Taichi kernel?A Taichi kernel cannot take a Python list directly. You need to use NumPy arrays as a bridge.For example, the following code snippet does not work:import taichi as tiimport numpy as npti.init()x = ti.field(ti.i32, shape=3)array = [10, 20, 30]@ti.kerneldef test(arr: list):    for i in range(3):        x[i] = arr[i]test(array)CopyYou need to import NumPy:import taichi as tiimport numpy as npti.init(arch=ti.cpu)x = ti.field(ti.i32, shape=3)array = np.array([10, 20, 30])@ti.kerneldef test(arr: ti.types.ndarray()):    for i in range(3):        x[i] = arr[i]test(array)CopyVisualizationDoes the Taichi's GUI system support color mapping when rendering simulation results?Taichi's GUI system can display colors when the field it accepts is a 3D vector field where each vector represents the RGB values of a pixel.To enable color mapping, convert ti.field into a NumPy array and call Matplotlib's colormap (cm), as shown in the following example:pixels = ti.Vector.field(3, shape=(w, h))gui = ti.GUI(f'Window title', (w, h))step = 0while gui.running: # Main loop    simulate_one_substep(pixels)    img = pixels.to_numpy()    img = cm.jet(img)    gui.set_image(img)    gui.show()CopyObjective-oriented programmingWhy does inheritance fail? I created a parent class and a child class, both decorated with @ti.data_oriented, and placed fields under @ti.kernel.The problem does not lie with inheritance. All Taichi fields must be allocated/placed in the Python scope. In other words, you need to define a field before calling @ti.kernel.For example, the following code snippet cannot run properly:@ti.data_orientedclass MyClass1():    def __init__(self):        self.testfield = ti.Vector.field(3, dtype=ti.f32)    @ti.kernel    def init_field(self):        ti.root.dense(ti.i, 10).place(self.testfield)CopyInstead, refrain from involving @ti.kernel when declaring a field via ti.root().place():@ti.data_orientedclass TriangleRasterizer:    def __init__(self, n):        self.n = n        self.A = ti.Vector.field(2, dtype=ti.f32)        self.B = ti.Vector.field(2, dtype=ti.f32)        self.C = ti.Vector.field(2, dtype=ti.f32)        self.c0 = ti.Vector.field(3, dtype=ti.f32)        self.c1 = ti.Vector.field(3, dtype=ti.f32)        self.c2 = ti.Vector.field(3, dtype=ti.f32)        self.vertices = ti.root.dense(ti.i, n).place(self.A, self.B, self.C)        self.colors = ti.root.dense(ti.i, n).place(self.c0, self.c1, self.c2)        # Tile-based culling        self.block_num_triangles = ti.field(dtype=ti.i32,                                            shape=(width // tile_size,                                                   height // tile_size))        self.block_indicies = ti.field(dtype=ti.i32,                                       shape=(width // tile_size,                                              height // tile_size, n))CopyFrom Taichi to PythonHow can I write data in Taichi fields to files? write() does not work.You cannot save data in Taichi fields directly, but there is a workaround. Taichi allows interaction with external arrays. Use to_numpy to convert a Taichi field to a NumPy array, as explained in this section. Then write the Numpy array to files via numpy.savetxt.A simple example:import taichi as tiimport numpy as npti.init(arch=ti.cpu)x = ti.field(dtype=ti.f32, shape= 10)y = ti.Vector.field(n=2, dtype=ti.i32, shape=10)@ti.kerneldef init():    for i in x:        x[i] = i * 0.5 + 1.0    for i in y:        y[i] = ti.Vector([i,i])init()np.savetxt('x.txt', x.to_numpy())np.savetxt('y.txt', y.to_numpy())CopyAnd data in fields x and y can be found in files x.txt and y.txt, respectively.Why an image obtained using field.to_numpy() is rotated when displayed using matplotlib's plt.imshow()?Taichi fields adopt a different coordinate system from NumPy's arrays for storing images. In a Taichi field, [0,0] denotes the pixel at the lower left corner of the image; the first axis extends to the right of the image; the second axis extends to the top.This is different from the usual convention taken by popular third-party libs like matplotlib or opencv, where [0, 0] denotes the pixel at the top left corner, the first axis extends down to the bottom of the image, and the second axis extends to the right.Therefore, to display a NumPy array using matplotlb's imshow(), you must rotate it 90 degrees clockwise.MiscellaneousHow does Taichi compare with Python packages designed for data science or machine learning?Popular packages designed for data science or machine learning include NumPy, JAX, PyTorch, and TensorFlow. A major difference between them and Taichi lies in the granularity of math operations.A common feature shared by the other packages is that they treat a single data array as the smallest unit of operations. Take PyTorch as an example. PyTorch processes a tensor as a whole and thus prefers such operations as the addition/subtraction/multiplication/division of tensors and matrix multiplication. The operators are parallelized internally, but the implementation process is invisible to users. As a result, users have to combine operators in various ways if they want to manipulate elements in tensors.Unlike them, Taichi makes element-level operations transparent and directly manipulates each iteration of the loops. This is why Taichi outperforms the other packages in scientific computing. In this sense, it compares more to C++ and CUDA.How does Taichi compare with Cython?Cython is a superset of the Python language for quickly generating C/C++ extensions. It is a frequently-used tool to improve Python code performance thanks to its support for C data types and static typing. In fact, many modules in the official NumPy and SciPy code are written and compiled in Cython.On the flip side, the mixture of Python and C values compromises Cython's readability. In addition, though Cython supports parallel computing to a certain degree (via multi-threading), it cannot offload computation to GPU backends.Compared with Cython, Taichi is more friendly to Non-C users because it can achieve significant performance improvement with pure valid Python code. Supporting a wide range of backends, Taichi is subject to much fewer limits when performing parallel programming. In addition, unlike Cython, Taichi does not require the OpenMP API or an extra parallelism module to accelerate your program. Just specify a backend and wrap the loop with the decorator @ti.kernel; then, you can leave the job to Taichi.How does Taichi compare with Numba?As its name indicates, Numba is tailored for NumPy. Numba is recommended if your functions involve vectorization of NumPy arrays. Compared with Numba, Taichi enjoys the following advantages:Taichi provides advanced features, including quantized data types, dataclasses and sparse data structures, and allows you to adjust memory layout flexibly. These features are especially helpful when a program handles massive amounts of data. However, Numba only performs best when dealing with dense NumPy arrays.Taichi can run on different GPU backends, making large-scale parallel programming (such as particle simulation or rendering) much more efficient. But it would be hard even to imagine writing a renderer in Numba.How does Taichi compare with ctypes?ctypes allows you to call C/C++ compiled code from Python and run C++/CUDA programs in Python through a C-compatible API. It is a convenient option to access a vast collection of libraries in Python while achieving some improvement in performance. However, ctypes elevates the usage barrier: To write a satisfactory program, you need to command C, Python, CMake, CUDA, and even more languages. Moreover, ctypes may not fit in well with some performance-critical scenarios where you try to call large C libraries in Python, given the runtime overhead it incurs.In contrast, it is much more reassuring to keep everything in Python. Taichi accelerates the performance of native Python code through automatic parallelization without involving the libraries out of the Python ecosystem. It also enables offline cache, which drastically reduces the launch overhead of Taichi kernels after the first call.How does Taichi compare with PyPy?Similar to Taichi, PyPy also accelerates Python code via just-in-time (JIT) compilation. PyPy is attractive because users can keep Python scripts as they are without even moderate modification. On the other hand, its strict conformity with Python rules leaves limited room for optimization.If you expect a greater leap in performance, Taichi can achieve the end. But you need to familiarize yourself with Taichi's syntax and assumptions, which differ from Python's slightly.Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?InstallationWhy does my pip complain package not found when installing Taichi?Parallel programmingOuter-most loops in Taichi kernels are by default parallel. How can I serialize one of them?Does Taichi provide a barrier synchronization function similar to __syncthreads() or glMemoryBarrier()?Data structuresHow do I declare a field with a dynamic length?How can I swap elements between two fields in the Taichi scope? a,b = b,a does not work.How do I compute the minimum/maximum of a field?Does Taichi support bool type?How do I program on less structured data structures (such as graphs and tetrahedral meshes) in Taichi?OperationsIn Taichi v1.3.0, the matmul result of a vector and a transposed vector gives a scalar instead of a matrix.Developement relatedCan I enable auto compeletion for Taichi?How to install Taichi on a server without Internet access?Integration with other libs/softwaresWhat is the most convenient way to load images into Taichi fields?Can Taichi interact with other Python packages such as matplotlib?Can I integrate Taichi and Houdini?Precision relatedHow do I accurately initialize a vector or matrix with f64 precision when my default floating-point precision (default_fp) is f32?From Python to TaichiWhy does it always return an error when I pass a list from the Python scope to a Taichi kernel?VisualizationDoes the Taichi's GUI system support color mapping when rendering simulation results?Objective-oriented programmingWhy does inheritance fail? I created a parent class and a child class, both decorated with @ti.data_oriented, and placed fields under @ti.kernel.From Taichi to PythonHow can I write data in Taichi fields to files? write() does not work.Why an image obtained using field.to_numpy() is rotated when displayed using matplotlib's plt.imshow()?MiscellaneousHow does Taichi compare with Python packages designed for data science or machine learning?How does Taichi compare with Cython?How does Taichi compare with Numba?How does Taichi compare with ctypes?How does Taichi compare with PyPy?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Installation Troubleshooting | Taichi Docs
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)SearchWhy TaichiGet StartedKernels & FunctionsType SystemData ContainersDifferentiable ProgrammingAdvanced ProgrammingVisualizationPerformanceDebuggingDeploymentTaichi Runtime C-APIMath LibraryContributionReferencesInternalsFAQFrequently Asked QuestionsInstallation TroubleshootingGlossaryDoc Home>>FAQ>>Installation TroubleshootingVersion: v1.6.0On this pageInstallation TroubleshootingLinux issuesIf Taichi crashes and reports
/usr/lib/libstdc++.so.6: version `CXXABI_1.3.11' not found:You might be using Ubuntu 16.04. Please try the solution in this
thread:sudo add-apt-repository ppa:ubuntu-toolchain-r/test -ysudo apt-get updatesudo apt-get install libstdc++6CopyWindows issuesIf Taichi crashes and reports ImportError on Windows. Please
consider installing Microsoft Visual C++
Redistributable.Python issuesIf pip could not find a satisfying package,
i.e.,ERROR: Could not find a version that satisfies the requirement taichi (from versions: none)ERROR: No matching distribution found for taichiCopyMake sure you're using Python version 3.7/3.8/3.9/3.10:python3 -c "import sys;print(sys.version[:sys.version.find('.', 2)])"# 3.7, 3.8, 3.9, or 3.10CopyMake sure your Python executable is 64-bit:python3 -c "print(__import__('platform').architecture()[0])"# 64bitCopyCUDA issuesIf Taichi crashes with the following errors:[Taichi] mode=release[Taichi] version 0.6.0, supported archs: [cpu, cuda, opengl], commit 14094f25, python 3.8.2[W 05/14/20 10:46:49.549] [cuda_driver.h:call_with_warning@60] CUDA Error CUDA_ERROR_INVALID_DEVICE: invalid device ordinal while calling mem_advise (cuMemAdvise)[E 05/14/20 10:46:49.911] Received signal 7 (Bus error)CopyThis might be because that your NVIDIA GPU is pre-Pascal, and it
has limited support for Unified
Memory.Possible solution: add export TI_USE_UNIFIED_MEMORY=0 to
your ~/.bashrc. This disables unified memory usage in the CUDA
backend.If Taichi exits with message "Out of CUDA pre-allocated memory", e.g.,import taichi as titi.init(arch=ti.cuda)x = ti.field(dtype=ti.i16)ti.root.pointer(ti.i, 1024).dense(ti.i, 1024 * 1024).place(x)# A sparse array. Each dense block is 2MB in size.# Populate 1024 * 2MB = 2GB memorydef populate():  for k in range(1024):    x[k * 1024 * 1024] = 1populate()Copy... may give you ...[Taichi] Starting on arch=cudaTaichi JIT:0: allocate_from_buffer: block: [0,0,0], thread: [0,0,0] Assertion `Out of CUDA pre-allocated memory.Consider using ti.init(device_memory_fraction=0.9) or ti.init(device_memory_GB=4) to allocate more GPU memory` failed.CopyThis usually happens when you are using sparse data structures that need dynamic GPU memory allocation.
On platforms without CUDA unified memory support (e.g., Windows),
Taichi only pre-allocates 1 GB of GPU memory for dynamically allocated data structures.
To fix this, simply pre-allocate more GPU memory:Set ti.init(..., device_memory_fraction=0.9) to allocate 90% of GPU memory. Replace "90%" with any other fraction depending on your hardware.Set ti.init(..., device_memory_GB=4) to allocate 4 GB GPU memory. Feel free to use any number bigger than 1.Setting environment variables TI_DEVICE_MEMORY_FRACTION=0.9 and TI_DEVICE_MEMORY_GB=4 would also work.Note that on Linux, Taichi automatically grows the memory pool using CUDA unified memory mechanisms.If you find other CUDA problems:Possible solution: add export TI_ENABLE_CUDA=0 to your
~/.bashrc. This disables the CUDA backend completely and
Taichi will fall back on other GPU backends such as OpenGL.OpenGL issuesIf Taichi crashes with a stack backtrace containing a line of
glfwCreateWindow (see
#958):[Taichi] mode=release[E 05/12/20 18.25:00.129] Received signal 11 (Segmentation Fault)************************************ Taichi Compiler Stack Traceback ************************************... (many lines, omitted)/lib/python3.8/site-packages/taichi/core/../lib/taichi_python.so: _glfwPlatformCreateWindow/lib/python3.8/site-packages/taichi/core/../lib/taichi_python.so: glfwCreateWindow/lib/python3.8/site-packages/taichi/core/../lib/taichi_python.so: taichi::lang::opengl::initialize_opengl(bool)... (many lines, omitted)Copyit is likely because you are running Taichi on a (virtual) machine
with an old OpenGL API. Taichi requires OpenGL 4.3+ to work.Possible solution: add export TI_ENABLE_OPENGL=0 to your
~/.bashrc even if you initialize Taichi with other backends
than OpenGL. This disables the OpenGL backend detection to avoid
incompatibilities.Installation interruptedDuring the installation, the downloading process is interrupted because of HTTPSConnection error. You can try installing Taichi from a mirror source.pip install taichi -i https://pypi.douban.com/simpleCopyOther issuesIf none of those above address your problem, please report this by
opening an
issue
on GitHub. This would help us improve user experiences and
compatibility, many thanks!Edit this pageLast updated on 5/12/2023 by Taichi GardenerWas this helpful?Linux issuesWindows issuesPython issuesCUDA issuesOpenGL issuesInstallation interruptedOther issuesCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Taichi Language’s API reference — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)
Taichi Language’s API reference#
You have arrived at Taichi Lang’s API reference page, if you are looking for more structured or
introductory articles about Taichi Lang, please refer to Docs Section.
Python API
taichi: The main taichi module
taichi.ad: Taichi Autodiff system
taichi.aot: Taichi AOT system
taichi.linalg: Taichi Linear algebra library
taichi.math: Taichi math library
taichi.profiler: Taichi profiler tools
taichi.tools: Miscellaneous tools shipped with Taichi
taichi.types: Taichi types
taichi.types.quant: Taichi quantized types
taichi.ui: Taichi UI components (GUI and GGUI)
Was this helpful?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Taichi Language’s API reference — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatdevelopdevelopv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)
Taichi Language’s API reference#
You have arrived at Taichi Lang’s API reference page, if you are looking for more structured or
introductory articles about Taichi Lang, please refer to Docs Section.
Python API
taichi: The main taichi module
taichi.ad: Taichi Autodiff system
taichi.aot: Taichi AOT system
taichi.linalg: Taichi Linear algebra library
taichi.math: Taichi math library
taichi.profiler: Taichi profiler tools
taichi.tools: Miscellaneous tools shipped with Taichi
taichi.types: Taichi types
taichi.types.quant: Taichi quantized types
taichi.ui: Taichi UI components (GUI and GGUI)
Was this helpful?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Taichi Language’s API reference — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)
Taichi Language’s API reference#
You have arrived at Taichi Lang’s API reference page, if you are looking for more structured or
introductory articles about Taichi Lang, please refer to Docs Section.
Python API
taichi: The main taichi module
taichi.ad: Taichi Autodiff system
taichi.aot: Taichi AOT system
taichi.linalg: Taichi Linear algebra library
taichi.math: Taichi math library
taichi.profiler: Taichi profiler tools
taichi.tools: Miscellaneous tools shipped with Taichi
taichi.types: Taichi types
taichi.types.quant: Taichi quantized types
taichi.ui: Taichi UI components (GUI and GGUI)
Was this helpful?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
Taichi Language’s API reference — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.5.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)
Taichi Language’s API reference#
You have arrived at Taichi Lang’s API reference page, if you are looking for more structured or
introductory articles about Taichi Lang, please refer to Docs Section.
Python API
taichi: The main taichi module
taichi.ad: Taichi Autodiff system
taichi.aot: Taichi AOT system
taichi.linalg: Taichi Linear algebra library
taichi.math: Taichi math library
taichi.profiler: Taichi profiler tools
taichi.tools: Miscellaneous tools shipped with Taichi
taichi.types: Taichi types
taichi.types.quant: Taichi quantized types
taichi.ui: Taichi UI components (GUI and GGUI)
Was this helpful?Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
taichi — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)On this page
taichi#
class taichi.BitpackedFields(max_num_bits)#
Taichi bitpacked fields, where fields with quantized types are packed together.
Parameters:
max_num_bits (int) – Maximum number of bits all fields inside can occupy in total. Only 32 or 64 is allowed.
place(self, *args, shared_exponent=False)#
Places a list of fields with quantized types inside.
Parameters:
*args (List[Field]) – A list of fields with quantized types to place.
shared_exponent (bool) – Whether the fields have a shared exponent.
taichi.CRITICAL = critical#
The str ‘critical’, used for the critical logging level.
taichi.DEBUG = debug#
The str ‘debug’, used for the debug logging level.
class taichi.DeviceCapability#
spirv_has_atomic_float = spirv_has_atomic_float#
spirv_has_atomic_float16 = spirv_has_atomic_float16#
spirv_has_atomic_float16_add = spirv_has_atomic_float16_add#
spirv_has_atomic_float16_minmax = spirv_has_atomic_float16_minmax#
spirv_has_atomic_float64 = spirv_has_atomic_float64#
spirv_has_atomic_float64_add = spirv_has_atomic_float64_add#
spirv_has_atomic_float64_minmax = spirv_has_atomic_float64_minmax#
spirv_has_atomic_float_add = spirv_has_atomic_float_add#
spirv_has_atomic_float_minmax = spirv_has_atomic_float_minmax#
spirv_has_atomic_int64 = spirv_has_atomic_int64#
spirv_has_float16 = spirv_has_float16#
spirv_has_float64 = spirv_has_float64#
spirv_has_int16 = spirv_has_int16#
spirv_has_int64 = spirv_has_int64#
spirv_has_int8 = spirv_has_int8#
spirv_has_no_integer_wrap_decoration = spirv_has_no_integer_wrap_decoration#
spirv_has_non_semantic_info = spirv_has_non_semantic_info#
spirv_has_physical_storage_buffer = spirv_has_physical_storage_buffer#
spirv_has_subgroup_arithmetic = spirv_has_subgroup_arithmetic#
spirv_has_subgroup_ballot = spirv_has_subgroup_ballot#
spirv_has_subgroup_basic = spirv_has_subgroup_basic#
spirv_has_subgroup_vote = spirv_has_subgroup_vote#
spirv_has_variable_ptr = spirv_has_variable_ptr#
spirv_version_1_3 = spirv_version=66304#
spirv_version_1_4 = spirv_version=66560#
spirv_version_1_5 = spirv_version=66816#
taichi.ERROR = error#
The str ‘error’, used for the error logging level.
class taichi.Field(_vars)#
Taichi field class.
A field is constructed by a list of field members.
For example, a scalar field has 1 field member, while a 3x3 matrix field has 9 field members.
A field member is a Python Expr wrapping a C++ FieldExpression.
Parameters:
vars (List[Expr]) – Field members.
copy_from(self, other)#
Copies all elements from another field.
The shape of the other field needs to be the same as self.
Parameters:
other (Field) – The source field.
from_paddle(self, arr)#
Loads all elements from a paddle tensor.
The shape of the paddle tensor needs to be the same as self.
Parameters:
arr (paddle.Tensor) – The source paddle tensor.
from_torch(self, arr)#
Loads all elements from a torch tensor.
The shape of the torch tensor needs to be the same as self.
Parameters:
arr (torch.tensor) – The source torch tensor.
parent(self, n=1)#
Gets an ancestor of the representative SNode in the SNode tree.
Parameters:
n (int) – the number of levels going up from the representative SNode.
Returns:
The n-th parent of the representative SNode.
Return type:
SNode
class taichi.FieldsBuilder#
A builder that constructs a SNodeTree instance.
Example:
x = ti.field(ti.i32)
y = ti.field(ti.f32)
fb = ti.FieldsBuilder()
fb.dense(ti.ij, 8).place(x)
fb.pointer(ti.ij, 8).dense(ti.ij, 4).place(y)
# After this line, `x` and `y` are placed. No more fields can be placed
# into `fb`.
#
# The tree looks like the following:
# (implicit root)
#  |
#  +-- dense +-- place(x)
#  |
#  +-- pointer +-- dense +-- place(y)
fb.finalize()
bitmasked(self, indices: Sequence[_Axis] | _Axis, dimensions: Sequence[int] | int)#
Same as taichi.lang.snode.SNode.bitmasked()
deactivate_all(self)#
Same as taichi.lang.snode.SNode.deactivate_all()
dense(self, indices: Sequence[_Axis] | _Axis, dimensions: Sequence[int] | int)#
Same as taichi.lang.snode.SNode.dense()
dynamic(self, index: Sequence[_Axis] | _Axis, dimension: Sequence[int] | int, chunk_size: int | None = None)#
Same as taichi.lang.snode.SNode.dynamic()
finalize(self, raise_warning=True)#
Constructs the SNodeTree and finalizes this builder.
Parameters:
raise_warning (bool) – Raise warning or not.
lazy_dual(self)#
Same as taichi.lang.snode.SNode.lazy_dual()
lazy_grad(self)#
Same as taichi.lang.snode.SNode.lazy_grad()
place(self, *args: Any, offset: Sequence[int] | int | None = None)#
Same as taichi.lang.snode.SNode.place()
pointer(self, indices: Sequence[_Axis] | _Axis, dimensions: Sequence[int] | int)#
Same as taichi.lang.snode.SNode.pointer()
quant_array(self, indices: Sequence[_Axis] | _Axis, dimensions: Sequence[int] | int, max_num_bits: int)#
Same as taichi.lang.snode.SNode.quant_array()
taichi.Format#
class taichi.GUI(name='Taichi', res=512, background_color=0, show_gui=True, fullscreen=False, fast_gui=False)#
Taichi Graphical User Interface class.
Parameters:
name (str, optional) – The name of the GUI to be constructed.
Default is ‘Taichi’.
res (Union[int, List[int]], optional) – The resolution of created
GUI. Default is 512*512. If res is scalar, then width will be equal to height.
background_color (int, optional) – The background color of created GUI.
Default is 0x000000.
show_gui (bool, optional) – Specify whether to render the GUI. Default is True.
fullscreen (bool, optional) – Specify whether to render the GUI in
fullscreen mode. Default is False.
fast_gui (bool, optional) – Specify whether to use fast gui mode of
Taichi. Default is False.
Returns:
The created taichi GUI object.
Return type:
GUI
class Event#
Class for holding a gui event.
An event is represented by:
type (PRESS, MOTION, RELEASE)
modifier (modifier keys like ctrl, shift, etc)
pos (mouse position)
key (event key)
delta (for holding mouse wheel)
class EventFilter(*e_filter)#
A set to store detected user events.
match(self, e)#
Check if a specified event e is among the detected events.
class WidgetValue(gui, wid)#
Class for maintaining id of gui widgets.
ALT = Alt#
BACKSPACE = BackSpace#
CAPSLOCK = Caps_Lock#
CTRL = Control#
DOWN = Down#
ESCAPE = Escape#
EXIT = WMClose#
LEFT = Left#
LMB = LMB#
MMB = MMB#
MOTION#
MOVE = Motion#
PRESS#
RELEASE#
RETURN = Return#
RIGHT = Right#
RMB = RMB#
SHIFT = Shift#
SPACE =#
TAB = Tab#
UP = Up#
WHEEL = Wheel#
arrow(self, orig, direction, radius=1, color=16777215, **kwargs)#
Draws a single arrow on canvas.
Parameters:
orig (List[Number]) – The position where arrow starts. Shape must be 2.
direction (List[Number]) – The direction where arrow points to. Shape must be 2.
radius (Number, optional) – The width of arrow. Default is 1.
color (int, optional) – The color of arrow. Default is 0xFFFFFF.
arrow_field(self, direction, radius=1, color=16777215, bound=0.5, **kwargs)#
Draw a field of arrows on canvas.
Parameters:
direction (np.array) – The pattern and direction of the field of arrows.
color (Union[int, np.array], optional) – The color or colors of arrows.
Default is 0xFFFFFF.
bound (Number, optional) – The boundary of the field. Default is 0.5.
arrows(self, orig, direction, radius=1, color=16777215, **kwargs)#
Draw a list arrows on canvas.
Parameters:
orig (numpy.array) – The positions where arrows start.
direction (numpy.array) – The directions where arrows point to.
radius (Union[Number, np.array], optional) – The width of arrows. Default is 1.
color (Union[int, np.array], optional) – The color or colors of arrows. Default is 0xffffff.
button(self, text, event_name=None)#
Create a button object on canvas to be manipulated with.
Parameters:
text (str) – The title of button.
event_name (str, optional) – The event name associated with button.
Default is WidgetButton_{text}
Returns:
The event name associated with created button.
circle(self, pos, color=16777215, radius=1)#
Draws a circle on canvas.
Parameters:
pos (Union[List[int], numpy.array]) – The position of the circle.
color (int, Optional) – The color of the circle. Default is 0xFFFFFF.
radius (Number, Optional) – The radius of the circle in pixel. Default is 1.
circles(self, pos, radius=1, color=16777215, palette=None, palette_indices=None)#
Draws a list of circles on canvas.
Parameters:
pos (numpy.array) – The positions of the circles.
radius (Union[Number, numpy.array], optional) – The radius of the circles in pixel.                 Can be either a number, which will be applied to all circles, or a 1D NumPy array of the same length as pos.                 The default is 1.
color (int, optional) – The color of the circles. Default is 0xFFFFFF.
palette (list[int], optional) – The List of colors from which to
choose to draw. Default is None.
palette_indices (Union[list[int], ti.field, numpy.array], optional) – The List of indices that choose color from palette for each
circle. Shape must match pos. Default is None.
clear(self, color=None)#
Clears the canvas with the color provided.
Parameters:
color (int, optional) – Specify the color to clear the canvas. Default
is the background color of GUI.
close(self)#
Close this GUI.
Example:
>>> while gui.running:
>>>     if gui.get_event(gui.PRESS, ti.GUI.ESCAPE):
>>>         gui.close()
>>>     gui.show()
contour(self, scalar_field, normalize=False)#
Plot a contour view of a scalar field.
The input scalar_field will be converted to a Numpy array first, and then plotted
by the Matplotlib colormap ‘Plasma’. Notice this method will automatically perform
a bilinear interpolation on the field if the size of the field does not match with
the GUI window size.
Parameters:
scalar_field (ti.field) – The scalar field being plotted.
normalize (bool, Optional) – Display the normalized scalar field if set to True.
False. (Default is) – 
cook_image(self, img)#
Converts an img to range [0, 1] for display.
The input image is stored in a numpy.ndarray, if it’s dtype
is int it will be rescaled and mapped into range [0, 1]. If
the dtype is float it will be directly casted to 32-bit float type.
static get_bool_environ(key, default)#
Get an environment variable and cast it to bool.
Parameters:
key (str) – The environment variable key.
default (bool) – The default value.
Returns:
The environment variable value cast to bool.             If the value is not found, directly return argument ‘default’.
get_cursor_pos(self)#
Returns the current position of mouse as a pair of floats
in the range [0, 1] x [0, 1].
The origin of the coordinates system is located at the lower left
corner, with +x direction points to the right, and +y direcntion
points upward.
Returns:
The current position of mouse.
get_event(self, *e_filter)#
Checks if the specified event is triggered.
Parameters:
*e_filter (ti.GUI.EVENT) – The specific event to be checked.
Returns:
whether or not the specified event is triggered.
Return type:
bool
get_events(self, *e_filter)#
Gets a list of events that are triggered.
Parameters:
*e_filter (List[ti.GUI.EVENT]) – The type of events to be filtered.
Returns:
A list of events that are triggered.
Return type:
EVENT
get_image(self)#
Return the window content as an numpy.ndarray.
Returns:
The image data in numpy contiguous array type.
Return type:
numpy.array
get_key_event(self)#
Gets keyboard triggered event.
Returns:
The keyboard triggered event.
Return type:
EVENT
has_key_event(self)#
Check if there is any key event registered.
Returns:
whether or not there is any key event registered.
Return type:
bool
is_pressed(self, *keys)#
Checks if any key among a set of specified keys is pressed.
Parameters:
*keys (Union[str, List[str]]) – The keys to be listened to.
Returns:
whether or not any key among the specified keys is pressed.
Return type:
bool
label(self, text)#
Creates a label object on canvas.
Parameters:
text (str) – The title of label.
Returns:
The created label object.
Return type:
WidgetValue
line(self, begin, end, radius=1, color=16777215)#
Draws a single line on canvas.
Parameters:
begin (List[Number]) – The position of one end of line. Shape must be 2.
end (List[Number]) – The position of the other end of line. Shape must be 2.
radius (Number, optional) – The width of line. Default is 1.
color (int, optional) – The color of line. Default is 0xFFFFFF.
lines(self, begin, end, radius=1, color=16777215)#
Draw a list of lines on canvas.
Parameters:
begin (numpy.array) – The positions of one end of lines.
end (numpy.array) – The positions of the other end of lines.
radius (Union[Number, numpy.array], optional) – The width of lines.
Can be either a single width or a list of width whose shape matches
the shape of begin & end. Default is 1.
color (Union[int, numpy.array], optional) – The color or colors of lines.
Can be either a single color or a list of colors whose shape matches
the shape of begin & end. Default is 0xFFFFFF.
point_field(self, radius, color=16777215, bound=0.5)#
Draws a field of points on canvas.
Parameters:
radius (np.array) – The pattern and radius of the field of points.
color (Union[int, np.array], optional) – The color or colors of points.
Default is 0xFFFFFF.
bound (Number, optional) – The boundary of the field. Default is 0.5.
rect(self, topleft, bottomright, radius=1, color=16777215)#
Draws a single rectangle on canvas.
Parameters:
topleft (List[Number]) – The position of the topleft corner of rectangle.
Shape must be 2.
bottomright (List[Number]) – The position of the bottomright corner
of rectangle. Shape must be 2.
radius (Number, optional) – The width of rectangle’s sides. Default is 1.
color (int, optional) – The color of rectangle. Default is 0xFFFFFF.
set_image(self, img)#
Sets an image to display on the window.
The image pixels are set from the values of img[i, j], where i indicates
the horizontal coordinates (from left to right) and j the vertical coordinates
(from bottom to top).
If the window size is (x, y), then img must be one of:
ti.field(shape=(x, y)), a gray-scale image
ti.field(shape=(x, y, 3)), where 3 is for (r, g, b) channels
ti.field(shape=(x, y, 2)), where 2 is for (r, g) channels
ti.Vector.field(3, shape=(x, y)) (r, g, b) channels on each component
ti.Vector.field(2, shape=(x, y)) (r, g) channels on each component
np.ndarray(shape=(x, y))
np.ndarray(shape=(x, y, 3))
np.ndarray(shape=(x, y, 2))
The data type of img must be one of:
uint8, range [0, 255]
uint16, range [0, 65535]
uint32, range [0, 4294967295]
float32, range [0, 1]
float64, range [0, 1]
Parameters:
img (Union[taichi.field, numpy.array]) – The color array                 representing the image to be drawn. Support greyscale, RG, RGB,                 and RGBA color representations. Its shape must match GUI resolution.
show(self, file=None)#
Shows the frame content in the gui window, or save the content to an
image file.
Parameters:
file (str, optional) – output filename. The default is None, and
the frame content is displayed in the gui window. If it’s a valid
image filename the frame will be saved as the specified image.
slider(self, text, minimum, maximum, step=1)#
Creates a slider object on canvas to be manipulated with.
Parameters:
text (str) – The title of slider.
minimum (int, float) – The minimum value of slider.
maximum (int, float) – The maximum value of slider.
step (int, float) – The changing step of slider. Optional and default to 1.
Returns:
The created slider object.
Return type:
WidgetValue
text(self, content, pos, font_size=15, color=16777215)#
Draws texts on canvas.
Parameters:
content (str) – The text to be drawn on canvas.
pos (List[Number]) – The position where the text is to be put.
font_size (Number, optional) – The font size of the text.
color (int, optional) – The color of the text. Default is 0xFFFFFF.
triangle(self, a, b, c, color=16777215)#
Draws a single triangle on canvas.
Parameters:
a (List[Number]) – The position of the first point of triangle. Shape must be 2.
b (List[Number]) – The position of the second point of triangle. Shape must be 2.
c (List[Number]) – The position of the third point of triangle. Shape must be 2.
color (int, optional) – The color of the triangle. Default is 0xFFFFFF.
triangles(self, a, b, c, color=16777215)#
Draws a list of triangles on canvas.
Parameters:
a (numpy.array) – The positions of the first points of triangles.
b (numpy.array) – The positions of the second points of triangles.
c (numpy.array) – The positions of the third points of triangles.
color (Union[int, numpy.array], optional) – The color or colors of triangles.
Can be either a single color or a list of colors whose shape matches
the shape of a & b & c. Default is 0xFFFFFF.
vector_field(self, vector_field, arrow_spacing=5, color=16777215)#
Display a vector field on canvas.
Parameters:
vector_field (ti.Vector.field) – The vector field being displayed.
arrow_spacing (int, optional) – The spacing between vectors.
color (Union[int, np.array], optional) – The color of vectors.
taichi.INFO = info#
The str ‘info’, used for the info logging level.
taichi.Layout#
class taichi.Matrix(arr, dt=None)#
Bases: taichi.lang.common_ops.TaichiOperations
The matrix class.
A matrix is a 2-D rectangular array with scalar entries, it’s row-majored, and is
aligned continuously. We recommend only use matrix with no more than 32 elements for
efficiency considerations.
Note: in taichi a matrix is strictly two-dimensional and only stores scalars.
Parameters:
arr (Union[list, tuple, np.ndarray]) – the initial values of a matrix.
dt (primitive_types) – the element data type.
ndim (int optional) – the number of dimensions of the matrix; forced reshape if given.
Example:
use a 2d list to initialize a matrix
>>> @ti.kernel
>>> def test():
>>>     n = 5
>>>     M = ti.Matrix([[0] * n for _ in range(n)], ti.i32)
>>>     print(M)  # a 5x5 matrix with integer elements
get the number of rows and columns via the `n`, `m` property:
>>> M = ti.Matrix([[0, 1], [2, 3], [4, 5]], ti.i32)
>>> M.n  # number of rows
3
>>> M.m  # number of cols
>>> 2
you can even initialize a matrix with an empty list:
>>> M = ti.Matrix([[], []], ti.i32)
>>> M.n
2
>>> M.m
0
all(self)#
Test whether all element not equal zero.
Returns:
True if all elements are not equal zero, False otherwise.
Return type:
bool
Example:
>>> v = ti.Vector([0, 0, 1])
>>> v.all()
False
any(self)#
Test whether any element not equal zero.
Returns:
True if any element is not equal zero, False otherwise.
Return type:
bool
Example:
>>> v = ti.Vector([0, 0, 1])
>>> v.any()
True
cast(self, dtype)#
Cast the matrix elements to a specified data type.
Parameters:
dtype (primitive_types) – data type of the
returned matrix.
Returns:
A new matrix with the specified data dtype.
Return type:
taichi.Matrix
Example:
>>> A = ti.Matrix([0, 1, 2], ti.i32)
>>> B = A.cast(ti.f32)
>>> B
[0.0, 1.0, 2.0]
static cols(cols)#
Constructs a Matrix instance by concatenating Vectors/lists column by column.
Parameters:
cols (List) – A list of Vector (1-D Matrix) or a list of list.
Returns:
A matrix.
Return type:
Matrix
Example:
>>> @ti.kernel
>>> def test():
>>>     v1 = ti.Vector([1, 2, 3])
>>>     v2 = ti.Vector([4, 5, 6])
>>>     m = ti.Matrix.cols([v1, v2])
>>>     print(m)
>>>
>>> test()
[[1, 4], [2, 5], [3, 6]]
cross(self, other)#
Performs the cross product with the input vector (1-D Matrix).
Both two vectors must have the same dimension <= 3.
For two 2d vectors (x1, y1) and (x2, y2), the return value is the
scalar x1*y2 - x2*y1.
For two 3d vectors v and w, the return value is the 3d vector
v x w.
Parameters:
other (Matrix) – The input Vector.
Returns:
The cross product of the two Vectors.
Return type:
Matrix
determinant(a)#
Returns the determinant of this matrix.
Note
The matrix dimension should be less than or equal to 4.
Returns:
The determinant of this matrix.
Return type:
dtype
Raises:
Exception – Determinants of matrices with sizes >= 5 are not supported.
static diag(dim, val)#
Returns a diagonal square matrix with the diagonals filled
with val.
Parameters:
dim (int) – the dimension of the wanted square matrix.
val (TypeVar) – value for the diagonal elements.
Returns:
The wanted diagonal matrix.
Return type:
Matrix
Example:
>>> m = ti.Matrix.diag(3, 1)
[[1, 0, 0],
 [0, 1, 0],
 [0, 0, 1]]
dot(self, other)#
Performs the dot product of two vectors.
To call this method, both multiplicatives must be vectors.
Parameters:
other (Matrix) – The input Vector.
Returns:
The dot product result (scalar) of the two Vectors.
Return type:
DataType
Example:
>>> v1 = ti.Vector([1, 2, 3])
>>> v2 = ti.Vector([3, 4, 5])
>>> v1.dot(v2)
26
classmethod field(cls, n, m, dtype, shape=None, order=None, name='', offset=None, needs_grad=False, needs_dual=False, layout=Layout.AOS, ndim=None)#
Construct a data container to hold all elements of the Matrix.
Parameters:
n (int) – The desired number of rows of the Matrix.
m (int) – The desired number of columns of the Matrix.
dtype (DataType, optional) – The desired data type of the Matrix.
shape (Union[int, tuple of int], optional) – The desired shape of the Matrix.
order (str, optional) – order of the shape laid out in memory.
name (string, optional) – The custom name of the field.
offset (Union[int, tuple of int], optional) – The coordinate offset
of all elements in a field.
needs_grad (bool, optional) – Whether the Matrix need grad field (reverse mode autodiff).
needs_dual (bool, optional) – Whether the Matrix need dual field (forward mode autodiff).
layout (Layout, optional) – The field layout, either Array Of
Structure (AOS) or Structure Of Array (SOA).
Returns:
A matrix.
Return type:
Matrix
fill(self, val)#
Fills the matrix with a specified value.
Parameters:
val (Union[int, float]) – Value to fill.
Example:
>>> A = ti.Matrix([0, 1, 2, 3])
>>> A.fill(-1)
>>> A
[-1, -1, -1, -1]
get_shape(self)#
static identity(dt, n)#
Constructs an identity Matrix with shape (n, n).
Parameters:
dt (DataType) – The desired data type.
n (int) – The number of rows/columns.
Returns:
An n x n identity matrix.
Return type:
Matrix
inverse(self)#
Returns the inverse of this matrix.
Note
The matrix dimension should be less than or equal to 4.
Returns:
The inverse of a matrix.
Return type:
Matrix
Raises:
Exception – Inversions of matrices with sizes >= 5 are not supported.
max(self)#
Returns the maximum element value.
min(self)#
Returns the minimum element value.
classmethod ndarray(cls, n, m, dtype, shape)#
Defines a Taichi ndarray with matrix elements.
This function must be called in Python scope, and after ti.init is called.
Parameters:
n (int) – Number of rows of the matrix.
m (int) – Number of columns of the matrix.
dtype (DataType) – Data type of each value.
shape (Union[int, tuple[int]]) – Shape of the ndarray.
Example:
The code below shows how a Taichi ndarray with matrix elements             can be declared and defined::
    >>> x = ti.Matrix.ndarray(4, 5, ti.f32, shape=(16, 8))
norm(self, eps=0)#
Returns the square root of the sum of the absolute squares
of its elements.
Parameters:
eps (Number) – a safe-guard value for sqrt, usually 0.
Example:
>>> a = ti.Vector([3, 4])
>>> a.norm()
5
Returns:
The square root of the sum of the absolute squares of its elements.
norm_inv(self, eps=0)#
The inverse of the matrix norm().
Parameters:
eps (float) – a safe-guard value for sqrt, usually 0.
Returns:
The inverse of the matrix/vector norm.
norm_sqr(self)#
Returns the sum of the absolute squares of its elements.
normalized(self, eps=0)#
Normalize a vector, i.e. matrices with the second dimension being
equal to one.
The normalization of a vector v is a vector of length 1
and has the same direction with v. It’s equal to v/|v|.
Parameters:
eps (float) – a safe-guard value for sqrt, usually 0.
Example:
>>> a = ti.Vector([3, 4], ti.f32)
>>> a.normalized()
[0.6, 0.8]
static one(dt, n, m=None)#
Constructs a Matrix filled with ones.
Parameters:
dt (DataType) – The desired data type.
n (int) – The first dimension (row) of the matrix.
m (int, optional) – The second dimension (column) of the matrix.
Returns:
A Matrix instance filled with ones.
Return type:
Matrix
outer_product(self, other)#
Performs the outer product with the input Vector (1-D Matrix).
The outer_product of two vectors v = (x1, x2, …, xn),
w = (y1, y2, …, yn) is a n times n square matrix, and its (i, j)
entry is equal to xi*yj.
Parameters:
other (Matrix) – The input Vector.
Returns:
The outer product of the two Vectors.
Return type:
Matrix
static rows(rows)#
Constructs a matrix by concatenating a list of
vectors/lists row by row. Must be called in Taichi scope.
Parameters:
rows (List) – A list of Vector (1-D Matrix) or a list of list.
Returns:
A matrix.
Return type:
Matrix
Example:
>>> @ti.kernel
>>> def test():
>>>     v1 = ti.Vector([1, 2, 3])
>>>     v2 = ti.Vector([4, 5, 6])
>>>     m = ti.Matrix.rows([v1, v2])
>>>     print(m)
>>>
>>> test()
[[1, 2, 3], [4, 5, 6]]
sum(self)#
Return the sum of all elements.
Example:
>>> m = ti.Matrix([[1, 2], [3, 4]])
>>> m.sum()
10
to_list(self)#
Return this matrix as a 1D list.
This is similar to numpy.ndarray’s flatten and ravel methods,
the difference is that this function always returns a new list.
to_numpy(self)#
Converts this matrix to a numpy array.
Returns:
The result numpy array.
Return type:
numpy.ndarray
Example:
>>> A = ti.Matrix([[0], [1], [2], [3]])
>>> A.to_numpy()
>>> A
array([[0], [1], [2], [3]])
trace(self)#
The sum of a matrix diagonal elements.
To call this method the matrix must be square-like.
Returns:
The sum of a matrix diagonal elements.
Example:
>>> m = ti.Matrix([[1, 2], [3, 4]])
>>> m.trace()
5
transpose(self)#
Returns the transpose of a matrix.
Returns:
The transpose of this matrix.
Return type:
Matrix
Example:
>>> A = ti.Matrix([[0, 1], [2, 3]])
>>> A.transpose()
[[0, 2], [1, 3]]
static unit(n, i, dt=None)#
Constructs a n-D vector with the i-th entry being equal to one and
the remaining entries are all zeros.
Parameters:
n (int) – The length of the vector.
i (int) – The index of the entry that will be filled with one.
dt (primitive_types, optional) – The desired data type.
Returns:
The returned vector.
Return type:
Matrix
Example:
>>> A = ti.Matrix.unit(3, 1)
>>> A
[0, 1, 0]
static zero(dt, n, m=None)#
Constructs a Matrix filled with zeros.
Parameters:
dt (DataType) – The desired data type.
n (int) – The first dimension (row) of the matrix.
m (int, optional) – The second dimension (column) of the matrix.
Returns:
A Matrix instance filled with zeros.
Return type:
Matrix
class taichi.MatrixField(_vars, n, m, ndim=2)#
Bases: taichi.lang.field.Field
Taichi matrix field with SNode implementation.
Parameters:
vars (List[Expr]) – Field members.
n (Int) – Number of rows.
m (Int) – Number of columns.
ndim (Int) – Number of dimensions; forced reshape if given.
fill(self, val)#
Fills this matrix field with specified values.
Parameters:
val (Union[Number, Expr, List, Tuple, Matrix]) – Values to fill,
should have consistent dimension consistent with self.
from_numpy(self, arr)#
Copies an numpy.ndarray into this field.
Example:
>>> m = ti.Matrix.field(2, 2, ti.f32, shape=(3, 3))
>>> arr = numpy.ones((3, 3, 2, 2))
>>> m.from_numpy(arr)
get_scalar_field(self, *indices)#
Creates a ScalarField using a specific field member.
Parameters:
indices (Tuple[Int]) – Specified indices of the field member.
Returns:
The result ScalarField.
Return type:
ScalarField
to_numpy(self, keep_dims=False, dtype=None)#
Converts the field instance to a NumPy array.
Parameters:
keep_dims (bool, optional) – Whether to keep the dimension after conversion.
When keep_dims=True, on an n-D matrix field, the numpy array always has n+2 dims, even for 1x1, 1xn, nx1 matrix fields.
When keep_dims=False, the resulting numpy array should skip the matrix dims with size 1.
For example, a 4x1 or 1x4 matrix field with 5x6x7 elements results in an array of shape 5x6x7x4.
dtype (DataType, optional) – The desired data type of returned numpy array.
Returns:
The result NumPy array.
Return type:
numpy.ndarray
to_paddle(self, place=None, keep_dims=False)#
Converts the field instance to a Paddle tensor.
Parameters:
place (paddle.CPUPlace()/CUDAPlace(n), optional) – The desired place of returned tensor.
keep_dims (bool, optional) – Whether to keep the dimension after conversion.
See to_numpy() for more detailed explanation.
Returns:
The result paddle tensor.
Return type:
paddle.Tensor
to_torch(self, device=None, keep_dims=False)#
Converts the field instance to a PyTorch tensor.
Parameters:
device (torch.device, optional) – The desired device of returned tensor.
keep_dims (bool, optional) – Whether to keep the dimension after conversion.
See to_numpy() for more detailed explanation.
Returns:
The result torch tensor.
Return type:
torch.tensor
class taichi.MatrixNdarray(n, m, dtype, shape)#
Bases: taichi.lang._ndarray.Ndarray
Taichi ndarray with matrix elements.
Parameters:
n (int) – Number of rows of the matrix.
m (int) – Number of columns of the matrix.
dtype (DataType) – Data type of each value.
shape (Union[int, tuple[int]]) – Shape of the ndarray.
Example:
>>> arr = ti.MatrixNdarray(2, 2, ti.f32, shape=(3, 3))
copy_from(self, other)#
Copies all elements from another ndarray.
The shape of the other ndarray needs to be the same as self.
Parameters:
other (Ndarray) – The source ndarray.
fill(self, val)#
Fills ndarray with a specific scalar value.
Parameters:
val (Union[int, float]) – Value to fill.
from_numpy(self, arr)#
Copies the data of a numpy.ndarray into this array.
Example:
>>> m = ti.MatrixNdarray(2, 2, ti.f32, shape=(2, 1), layout=0)
>>> arr = np.ones((2, 1, 2, 2))
>>> m.from_numpy(arr)
get_type(self)#
to_numpy(self)#
Converts this ndarray to a numpy.ndarray.
Example:
>>> arr = ti.MatrixNdarray(2, 2, ti.f32, shape=(2, 1))
>>> arr.to_numpy()
[[[[0. 0.]
   [0. 0.]]]
 [[[0. 0.]
   [0. 0.]]]]
class taichi.Mesh#
The Mesh type class.
MeshTaichi offers first-class support for triangular/tetrahedral meshes
and allows efficient computation on these irregular data structures,
only available for backends supporting ti.extension.mesh.
See more details in https://github.com/taichi-dev/meshtaichi
static generate_meta(data)#
static load_meta(filename)#
class taichi.MeshInstance#
add_mesh_attribute(self, element_type, snode, reorder_type)#
get_position_as_numpy(self)#
Get the vertex position of current mesh to numpy array.
Returns:
[x, y, z] with float-format.
Return type:
3d numpy array
get_relation_access(self, from_index, to_element_type, neighbor_idx_ptr)#
get_relation_size(self, from_index, to_element_type)#
set_index_mapping(self, element_type: MeshElementType, conv_type: ConvType, mapping: taichi.lang.field.ScalarField)#
set_num_patches(self, num_patches: int)#
set_owned_offset(self, element_type: MeshElementType, owned_offset: taichi.lang.field.ScalarField)#
set_patch_max_element_num(self, element_type: MeshElementType, max_element_num: int)#
set_relation_dynamic(self, rel_type: MeshRelationType, value: taichi.lang.field.ScalarField, patch_offset: taichi.lang.field.ScalarField, offset: taichi.lang.field.ScalarField)#
set_relation_fixed(self, rel_type: MeshRelationType, value: taichi.lang.field.ScalarField)#
set_total_offset(self, element_type: MeshElementType, total_offset: taichi.lang.field.ScalarField)#
class taichi.Ndarray#
Taichi ndarray class.
Parameters:
dtype (DataType) – Data type of each value.
shape (Tuple[int]) – Shape of the Ndarray.
copy_from(self, other)#
Copies all elements from another ndarray.
The shape of the other ndarray needs to be the same as self.
Parameters:
other (Ndarray) – The source ndarray.
fill(self, val)#
Fills ndarray with a specific scalar value.
Parameters:
val (Union[int, float]) – Value to fill.
get_type(self)#
class taichi.SNode(ptr)#
A Python-side SNode wrapper.
For more information on Taichi’s SNode system, please check out
these references:
https://docs.taichi-lang.org/docs/sparse
https://yuanming.taichi.graphics/publication/2019-taichi/taichi-lang.pdf
Arg:ptr (pointer): The C++ side SNode pointer.
bitmasked(self, axes, dimensions)#
Adds a bitmasked SNode as a child component of self.
Parameters:
axes (List[Axis]) – Axes to activate.
dimensions (Union[List[int], int]) – Shape of each axis.
Returns:
The added SNode instance.
deactivate_all(self)#
Recursively deactivate all children components of self.
dense(self, axes, dimensions)#
Adds a dense SNode as a child component of self.
Parameters:
axes (List[Axis]) – Axes to activate.
dimensions (Union[List[int], int]) – Shape of each axis.
Returns:
The added SNode instance.
dynamic(self, axis, dimension, chunk_size=None)#
Adds a dynamic SNode as a child component of self.
Parameters:
axis (List[Axis]) – Axis to activate, must be 1.
dimension (int) – Shape of the axis.
chunk_size (int) – Chunk size.
Returns:
The added SNode instance.
lazy_dual(self)#
Automatically place the dual fields following the layout of their primal fields.
lazy_grad(self)#
Automatically place the adjoint fields following the layout of their primal fields.
Users don’t need to specify needs_grad when they define scalar/vector/matrix fields (primal fields) using autodiff.
When all the primal fields are defined, using taichi.root.lazy_grad() could automatically generate
their corresponding adjoint fields (gradient field).
To know more details about primal, adjoint fields and lazy_grad(),
please see Page 4 and Page 13-14 of DiffTaichi Paper: https://arxiv.org/pdf/1910.00935.pdf
parent(self, n=1)#
Gets an ancestor of self in the SNode tree.
Parameters:
n (int) – the number of levels going up from self.
Returns:
The n-th parent of self.
Return type:
Union[None, _Root, SNode]
place(self, *args, offset=None)#
Places a list of Taichi fields under the self container.
Parameters:
*args (List[ti.field]) – A list of Taichi fields to place.
offset (Union[Number, tuple[Number]]) – Offset of the field domain.
Returns:
The self container.
pointer(self, axes, dimensions)#
Adds a pointer SNode as a child component of self.
Parameters:
axes (List[Axis]) – Axes to activate.
dimensions (Union[List[int], int]) – Shape of each axis.
Returns:
The added SNode instance.
quant_array(self, axes, dimensions, max_num_bits)#
Adds a quant_array SNode as a child component of self.
Parameters:
axes (List[Axis]) – Axes to activate.
dimensions (Union[List[int], int]) – Shape of each axis.
max_num_bits (int) – Maximum number of bits it can hold.
Returns:
The added SNode instance.
class taichi.ScalarField(var)#
Bases: Field
Taichi scalar field with SNode implementation.
Parameters:
var (Expr) – Field member.
copy_from(self, other)#
Copies all elements from another field.
The shape of the other field needs to be the same as self.
Parameters:
other (Field) – The source field.
fill(self, val)#
Fills this scalar field with a specified value.
from_numpy(self, arr)#
Copies the data from a numpy.ndarray into this field.
from_paddle(self, arr)#
Loads all elements from a paddle tensor.
The shape of the paddle tensor needs to be the same as self.
Parameters:
arr (paddle.Tensor) – The source paddle tensor.
from_torch(self, arr)#
Loads all elements from a torch tensor.
The shape of the torch tensor needs to be the same as self.
Parameters:
arr (torch.tensor) – The source torch tensor.
parent(self, n=1)#
Gets an ancestor of the representative SNode in the SNode tree.
Parameters:
n (int) – the number of levels going up from the representative SNode.
Returns:
The n-th parent of the representative SNode.
Return type:
SNode
to_numpy(self, dtype=None)#
Converts this field to a numpy.ndarray.
to_paddle(self, place=None)#
Converts this field to a paddle.Tensor.
to_torch(self, device=None)#
Converts this field to a torch.tensor.
class taichi.ScalarNdarray(dtype, arr_shape)#
Bases: Ndarray
Taichi ndarray with scalar elements.
Parameters:
dtype (DataType) – Data type of each value.
shape (Tuple[int]) – Shape of the ndarray.
copy_from(self, other)#
Copies all elements from another ndarray.
The shape of the other ndarray needs to be the same as self.
Parameters:
other (Ndarray) – The source ndarray.
fill(self, val)#
Fills ndarray with a specific scalar value.
Parameters:
val (Union[int, float]) – Value to fill.
from_numpy(self, arr)#
get_type(self)#
to_numpy(self)#
class taichi.Struct(*args, **kwargs)#
The Struct type class.
A struct is a dictionary-like data structure that stores members as
(key, value) pairs. Valid data members of a struct can be scalars,
matrices or other dictionary-like structures.
Parameters:
entries (Dict[str, Union[Dict, Expr, Matrix, Struct]]) – keys and values for struct members. Entries can optionally
include a dictionary of functions with the key ‘__struct_methods’
which will be attached to the struct for executing on the struct data.
Returns:
An instance of this struct.
Example:
>>> vec3 = ti.types.vector(3, ti.f32)
>>> a = ti.Struct(v=vec3([0, 0, 0]), t=1.0)
>>> print(a.items)
dict_items([('v', [0. 0. 0.]), ('t', 1.0)])
>>>
>>> B = ti.Struct(v=vec3([0., 0., 0.]), t=1.0, A=a)
>>> print(B.items)
dict_items([('v', [0. 0. 0.]), ('t', 1.0), ('A', {'v': [[0.], [0.], [0.]], 't': 1.0})])
classmethod field(cls, members, methods={}, shape=None, name='<Struct>', offset=None, needs_grad=False, needs_dual=False, layout=Layout.AOS)#
Creates a StructField with each element
has this struct as its type.
Parameters:
members (dict) – a dict, each item is like name: type.
methods (dict) – a dict of methods that should be included with
the field.  Each struct item of the field will have the
methods as instance functions.
shape (Tuple[int]) – width and height of the field.
offset (Tuple[int]) – offset of the indices of the created field.
For example if offset=(-10, -10) the indices of the field
will start at (-10, -10), not (0, 0).
needs_grad (bool) – enabling grad field (reverse mode autodiff) or not.
needs_dual (bool) – enabling dual field (forward mode autodiff) or not.
layout – AOS or SOA.
Example
>>> vec3 = ti.types.vector(3, ti.f32)
>>> sphere = {"center": vec3, "radius": float}
>>> F = ti.Struct.field(sphere, shape=(3, 3))
>>> F
{'center': array([[[0., 0., 0.],
    [0., 0., 0.],
    [0., 0., 0.]],
[[0., 0., 0.],[0., 0., 0.],
[0., 0., 0.]],
[[0., 0., 0.],[0., 0., 0.],
[0., 0., 0.]]], dtype=float32), ‘radius’: array([[0., 0., 0.],
[0., 0., 0.],
[0., 0., 0.]], dtype=float32)}
to_dict(self, include_methods=False, include_ndim=False)#
Converts the Struct to a dictionary.
Parameters:
include_methods (bool) – Whether any struct methods should be included
in the result dictionary under the key ‘__struct_methods’.
Returns:
The result dictionary.
Return type:
Dict
class taichi.StructField(field_dict, struct_methods, name=None, is_primal=True)#
Bases: taichi.lang.field.Field
Taichi struct field with SNode implementation.
Instead of directly constraining Expr entries, the StructField object
directly hosts members as Field instances to support nested structs.
Parameters:
field_dict (Dict[str, Field]) – Struct field members.
struct_methods (Dict[str, callable]) – Dictionary of functions to apply
to each struct instance in the field.
name (string, optional) – The custom name of the field.
copy_from(self, other)#
Copies all elements from another field.
The shape of the other field needs to be the same as self.
Parameters:
other (Field) – The source field.
fill(self, val)#
Fills this struct field with a specified value.
Parameters:
val (Union[int, float]) – Value to fill.
from_numpy(self, array_dict)#
Copies the data from a set of numpy.array into this field.
The argument array_dict must be a dictionay-like object, it
contains all the keys in this field and the copying process
between corresponding items can be performed.
from_paddle(self, array_dict)#
Copies the data from a set of paddle.Tensor into this field.
The argument array_dict must be a dictionay-like object, it
contains all the keys in this field and the copying process
between corresponding items can be performed.
from_torch(self, array_dict)#
Copies the data from a set of torch.tensor into this field.
The argument array_dict must be a dictionay-like object, it
contains all the keys in this field and the copying process
between corresponding items can be performed.
get_member_field(self, key)#
Creates a ScalarField using a specific field member.
Parameters:
key (str) – Specified key of the field member.
Returns:
The result ScalarField.
Return type:
ScalarField
parent(self, n=1)#
Gets an ancestor of the representative SNode in the SNode tree.
Parameters:
n (int) – the number of levels going up from the representative SNode.
Returns:
The n-th parent of the representative SNode.
Return type:
SNode
to_numpy(self)#
Converts the Struct field instance to a dictionary of NumPy arrays.
The dictionary may be nested when converting nested structs.
Returns:
The result NumPy array.
Return type:
Dict[str, Union[numpy.ndarray, Dict]]
to_paddle(self, place=None)#
Converts the Struct field instance to a dictionary of Paddle tensors.
The dictionary may be nested when converting nested structs.
Parameters:
place (paddle.CPUPlace()/CUDAPlace(n), optional) – The
desired place of returned tensor.
Returns:
The resultPaddle tensor.
Return type:
Dict[str, Union[paddle.Tensor, Dict]]
to_torch(self, device=None)#
Converts the Struct field instance to a dictionary of PyTorch tensors.
The dictionary may be nested when converting nested structs.
Parameters:
device (torch.device, optional) – The
desired device of returned tensor.
Returns:
The resultPyTorch tensor.
Return type:
Dict[str, Union[torch.Tensor, Dict]]
taichi.TRACE = trace#
The str ‘trace’, used for the debug logging level.
exception taichi.TaichiAssertionError#
Bases: TaichiRuntimeError, AssertionError
Thrown when assertion fails at runtime.
class args#
with_traceback()#
Exception.with_traceback(tb) –
set self.__traceback__ to tb and return self.
exception taichi.TaichiCompilationError#
Bases: Exception
Base class for all compilation exceptions.
class args#
with_traceback()#
Exception.with_traceback(tb) –
set self.__traceback__ to tb and return self.
exception taichi.TaichiNameError#
Bases: TaichiCompilationError, NameError
Thrown when an undefine name is found during compilation.
class args#
with_traceback()#
Exception.with_traceback(tb) –
set self.__traceback__ to tb and return self.
exception taichi.TaichiRuntimeError#
Bases: RuntimeError
Thrown when the compiled program cannot be executed due to unspecified reasons.
class args#
with_traceback()#
Exception.with_traceback(tb) –
set self.__traceback__ to tb and return self.
exception taichi.TaichiRuntimeTypeError#
Bases: TaichiRuntimeError, TypeError
Thrown when the compiled program cannot be executed due to unspecified reasons.
class args#
static get(pos, needed, provided)#
with_traceback()#
Exception.with_traceback(tb) –
set self.__traceback__ to tb and return self.
exception taichi.TaichiSyntaxError#
Bases: TaichiCompilationError, SyntaxError
Thrown when a syntax error is found during compilation.
class args#
class filename#
exception filename
class lineno#
exception lineno
class msg#
exception msg
class offset#
exception offset
class print_file_and_line#
exception print_file_and_line
class text#
exception text
with_traceback()#
Exception.with_traceback(tb) –
set self.__traceback__ to tb and return self.
exception taichi.TaichiTypeError#
Bases: TaichiCompilationError, TypeError
Thrown when a type mismatch is found during compilation.
class args#
with_traceback()#
Exception.with_traceback(tb) –
set self.__traceback__ to tb and return self.
class taichi.Texture(fmt, arr_shape)#
Taichi Texture class.
Parameters:
fmt (ti.Format) – Color format of the texture.
shape (Tuple[int]) – Shape of the Texture.
from_field(self, field)#
Loads a field to texture.
Parameters:
field (ti.Field) – Source field to load from.
from_image(self, image)#
Loads a PIL image to texture. This method is only allowed a 2D texture with ti.Format.rgba8.
Parameters:
image (PIL.Image.Image) – Source PIL image to load from.
from_ndarray(self, ndarray)#
Loads an ndarray to texture.
Parameters:
ndarray (ti.Ndarray) – Source ndarray to load from.
to_image(self)#
Saves a texture to a PIL image in RGB mode. This method is only allowed a 2D texture with ti.Format.rgba8.
Returns:
a PIL image in RGB mode, with the same size as source texture.
Return type:
img (PIL.Image.Image)
class taichi.Vector(arr, dt=None, **kwargs)#
Bases: Matrix
The matrix class.
A matrix is a 2-D rectangular array with scalar entries, it’s row-majored, and is
aligned continuously. We recommend only use matrix with no more than 32 elements for
efficiency considerations.
Note: in taichi a matrix is strictly two-dimensional and only stores scalars.
Parameters:
arr (Union[list, tuple, np.ndarray]) – the initial values of a matrix.
dt (primitive_types) – the element data type.
ndim (int optional) – the number of dimensions of the matrix; forced reshape if given.
Example:
use a 2d list to initialize a matrix
>>> @ti.kernel
>>> def test():
>>>     n = 5
>>>     M = ti.Matrix([[0] * n for _ in range(n)], ti.i32)
>>>     print(M)  # a 5x5 matrix with integer elements
get the number of rows and columns via the `n`, `m` property:
>>> M = ti.Matrix([[0, 1], [2, 3], [4, 5]], ti.i32)
>>> M.n  # number of rows
3
>>> M.m  # number of cols
>>> 2
you can even initialize a matrix with an empty list:
>>> M = ti.Matrix([[], []], ti.i32)
>>> M.n
2
>>> M.m
0
all(self)#
Test whether all element not equal zero.
Returns:
True if all elements are not equal zero, False otherwise.
Return type:
bool
Example:
>>> v = ti.Vector([0, 0, 1])
>>> v.all()
False
any(self)#
Test whether any element not equal zero.
Returns:
True if any element is not equal zero, False otherwise.
Return type:
bool
Example:
>>> v = ti.Vector([0, 0, 1])
>>> v.any()
True
cast(self, dtype)#
Cast the matrix elements to a specified data type.
Parameters:
dtype (primitive_types) – data type of the
returned matrix.
Returns:
A new matrix with the specified data dtype.
Return type:
taichi.Matrix
Example:
>>> A = ti.Matrix([0, 1, 2], ti.i32)
>>> B = A.cast(ti.f32)
>>> B
[0.0, 1.0, 2.0]
static cols(cols)#
Constructs a Matrix instance by concatenating Vectors/lists column by column.
Parameters:
cols (List) – A list of Vector (1-D Matrix) or a list of list.
Returns:
A matrix.
Return type:
Matrix
Example:
>>> @ti.kernel
>>> def test():
>>>     v1 = ti.Vector([1, 2, 3])
>>>     v2 = ti.Vector([4, 5, 6])
>>>     m = ti.Matrix.cols([v1, v2])
>>>     print(m)
>>>
>>> test()
[[1, 4], [2, 5], [3, 6]]
cross(self, other)#
Performs the cross product with the input vector (1-D Matrix).
Both two vectors must have the same dimension <= 3.
For two 2d vectors (x1, y1) and (x2, y2), the return value is the
scalar x1*y2 - x2*y1.
For two 3d vectors v and w, the return value is the 3d vector
v x w.
Parameters:
other (Matrix) – The input Vector.
Returns:
The cross product of the two Vectors.
Return type:
Matrix
determinant(a)#
Returns the determinant of this matrix.
Note
The matrix dimension should be less than or equal to 4.
Returns:
The determinant of this matrix.
Return type:
dtype
Raises:
Exception – Determinants of matrices with sizes >= 5 are not supported.
static diag(dim, val)#
Returns a diagonal square matrix with the diagonals filled
with val.
Parameters:
dim (int) – the dimension of the wanted square matrix.
val (TypeVar) – value for the diagonal elements.
Returns:
The wanted diagonal matrix.
Return type:
Matrix
Example:
>>> m = ti.Matrix.diag(3, 1)
[[1, 0, 0],
 [0, 1, 0],
 [0, 0, 1]]
dot(self, other)#
Performs the dot product of two vectors.
To call this method, both multiplicatives must be vectors.
Parameters:
other (Matrix) – The input Vector.
Returns:
The dot product result (scalar) of the two Vectors.
Return type:
DataType
Example:
>>> v1 = ti.Vector([1, 2, 3])
>>> v2 = ti.Vector([3, 4, 5])
>>> v1.dot(v2)
26
classmethod field(cls, n, dtype, *args, **kwargs)#
ti.Vector.field
fill(self, val)#
Fills the matrix with a specified value.
Parameters:
val (Union[int, float]) – Value to fill.
Example:
>>> A = ti.Matrix([0, 1, 2, 3])
>>> A.fill(-1)
>>> A
[-1, -1, -1, -1]
get_shape(self)#
static identity(dt, n)#
Constructs an identity Matrix with shape (n, n).
Parameters:
dt (DataType) – The desired data type.
n (int) – The number of rows/columns.
Returns:
An n x n identity matrix.
Return type:
Matrix
inverse(self)#
Returns the inverse of this matrix.
Note
The matrix dimension should be less than or equal to 4.
Returns:
The inverse of a matrix.
Return type:
Matrix
Raises:
Exception – Inversions of matrices with sizes >= 5 are not supported.
max(self)#
Returns the maximum element value.
min(self)#
Returns the minimum element value.
classmethod ndarray(cls, n, dtype, shape)#
Defines a Taichi ndarray with vector elements.
Parameters:
n (int) – Size of the vector.
dtype (DataType) – Data type of each value.
shape (Union[int, tuple[int]]) – Shape of the ndarray.
Example
The code below shows how a Taichi ndarray with vector elements can be declared and defined:
>>> x = ti.Vector.ndarray(3, ti.f32, shape=(16, 8))
norm(self, eps=0)#
Returns the square root of the sum of the absolute squares
of its elements.
Parameters:
eps (Number) – a safe-guard value for sqrt, usually 0.
Example:
>>> a = ti.Vector([3, 4])
>>> a.norm()
5
Returns:
The square root of the sum of the absolute squares of its elements.
norm_inv(self, eps=0)#
The inverse of the matrix norm().
Parameters:
eps (float) – a safe-guard value for sqrt, usually 0.
Returns:
The inverse of the matrix/vector norm.
norm_sqr(self)#
Returns the sum of the absolute squares of its elements.
normalized(self, eps=0)#
Normalize a vector, i.e. matrices with the second dimension being
equal to one.
The normalization of a vector v is a vector of length 1
and has the same direction with v. It’s equal to v/|v|.
Parameters:
eps (float) – a safe-guard value for sqrt, usually 0.
Example:
>>> a = ti.Vector([3, 4], ti.f32)
>>> a.normalized()
[0.6, 0.8]
static one(dt, n, m=None)#
Constructs a Matrix filled with ones.
Parameters:
dt (DataType) – The desired data type.
n (int) – The first dimension (row) of the matrix.
m (int, optional) – The second dimension (column) of the matrix.
Returns:
A Matrix instance filled with ones.
Return type:
Matrix
outer_product(self, other)#
Performs the outer product with the input Vector (1-D Matrix).
The outer_product of two vectors v = (x1, x2, …, xn),
w = (y1, y2, …, yn) is a n times n square matrix, and its (i, j)
entry is equal to xi*yj.
Parameters:
other (Matrix) – The input Vector.
Returns:
The outer product of the two Vectors.
Return type:
Matrix
static rows(rows)#
Constructs a matrix by concatenating a list of
vectors/lists row by row. Must be called in Taichi scope.
Parameters:
rows (List) – A list of Vector (1-D Matrix) or a list of list.
Returns:
A matrix.
Return type:
Matrix
Example:
>>> @ti.kernel
>>> def test():
>>>     v1 = ti.Vector([1, 2, 3])
>>>     v2 = ti.Vector([4, 5, 6])
>>>     m = ti.Matrix.rows([v1, v2])
>>>     print(m)
>>>
>>> test()
[[1, 2, 3], [4, 5, 6]]
sum(self)#
Return the sum of all elements.
Example:
>>> m = ti.Matrix([[1, 2], [3, 4]])
>>> m.sum()
10
to_list(self)#
Return this matrix as a 1D list.
This is similar to numpy.ndarray’s flatten and ravel methods,
the difference is that this function always returns a new list.
to_numpy(self)#
Converts this matrix to a numpy array.
Returns:
The result numpy array.
Return type:
numpy.ndarray
Example:
>>> A = ti.Matrix([[0], [1], [2], [3]])
>>> A.to_numpy()
>>> A
array([[0], [1], [2], [3]])
trace(self)#
The sum of a matrix diagonal elements.
To call this method the matrix must be square-like.
Returns:
The sum of a matrix diagonal elements.
Example:
>>> m = ti.Matrix([[1, 2], [3, 4]])
>>> m.trace()
5
transpose(self)#
Returns the transpose of a matrix.
Returns:
The transpose of this matrix.
Return type:
Matrix
Example:
>>> A = ti.Matrix([[0, 1], [2, 3]])
>>> A.transpose()
[[0, 2], [1, 3]]
static unit(n, i, dt=None)#
Constructs a n-D vector with the i-th entry being equal to one and
the remaining entries are all zeros.
Parameters:
n (int) – The length of the vector.
i (int) – The index of the entry that will be filled with one.
dt (primitive_types, optional) – The desired data type.
Returns:
The returned vector.
Return type:
Matrix
Example:
>>> A = ti.Matrix.unit(3, 1)
>>> A
[0, 1, 0]
static zero(dt, n, m=None)#
Constructs a Matrix filled with zeros.
Parameters:
dt (DataType) – The desired data type.
n (int) – The first dimension (row) of the matrix.
m (int, optional) – The second dimension (column) of the matrix.
Returns:
A Matrix instance filled with zeros.
Return type:
Matrix
class taichi.VectorNdarray(n, dtype, shape)#
Bases: taichi.lang._ndarray.Ndarray
Taichi ndarray with vector elements.
Parameters:
n (int) – Size of the vector.
dtype (DataType) – Data type of each value.
shape (Tuple[int]) – Shape of the ndarray.
Example:
>>> a = ti.VectorNdarray(3, ti.f32, (3, 3))
copy_from(self, other)#
Copies all elements from another ndarray.
The shape of the other ndarray needs to be the same as self.
Parameters:
other (Ndarray) – The source ndarray.
fill(self, val)#
Fills ndarray with a specific scalar value.
Parameters:
val (Union[int, float]) – Value to fill.
from_numpy(self, arr)#
Copies the data from a numpy.ndarray into this ndarray.
The shape and data type of arr must match this ndarray.
Example:
>>> import numpy as np
>>> a = ti.VectorNdarray(3, ti.f32, (2, 2), 0)
>>> b = np.ones((2, 2, 3), dtype=np.float32)
>>> a.from_numpy(b)
get_type(self)#
to_numpy(self)#
Converts this vector ndarray to a numpy.ndarray.
Example:
>>> a = ti.VectorNdarray(3, ti.f32, (2, 2))
>>> a.to_numpy()
array([[[0., 0., 0.],
        [0., 0., 0.]],
       [[0., 0., 0.],
        [0., 0., 0.]]], dtype=float32)
taichi.WARN = warn#
The str ‘warn’, used for the warn logging level.
taichi.abs(x)#
Compute the absolute value \(|x|\) of x, element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
Returns:
The absolute value of each element in x.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Vector([-1.0, 0.0, 1.0])
>>>     y = ti.abs(x)
>>>     print(y)
>>>
>>> test()
[1.0, 0.0, 1.0]
taichi.acos(x)#
Trigonometric inverse cosine, element-wise.
The inverse of cos so that, if y = cos(x), then x = acos(y).
For input x not in the domain [-1, 1], this function returns nan if         it’s called in taichi scope, or raises exception if it’s called in python scope.
Parameters:
x (Union[primitive_types, Matrix]) – A scalar or a matrix with elements in [-1, 1].
Returns:
The inverse cosine of each element in x, in radians and in the closed             interval [0, pi]. This is a scalar if x is a scalar.
Example:
>>> from math import pi
>>> ti.acos(ti.Matrix([-1.0, 0.0, 1.0])) * 180 / pi
[180., 90., 0.]
taichi.activate(node, indices)#
Explicitly activate a cell of node at location indices.
Parameters:
node (SNode) – Must be a pointer, hash or bitmasked node.
indices (Union[int, Vector]) – the indices to activate.
taichi.amdgpu#
The AMDGPU backend.
taichi.append(node, indices, val)#
Append a value val to a SNode node at index indices.
Parameters:
node (SNode) – Input SNode.
indices (Union[int, Vector]) – the indices to visit.
val (Union[primitive_types, compound_types]) – the data to be appended.
taichi.arm64#
The ARM CPU backend.
taichi.asin(x)#
Trigonometric inverse sine, element-wise.
The inverse of sin so that, if y = sin(x), then x = asin(y).
For input x not in the domain [-1, 1], this function returns nan if         it’s called in taichi scope, or raises exception if it’s called in python scope.
Parameters:
x (Union[primitive_types, Matrix]) – A scalar or a matrix with elements in [-1, 1].
Returns:
The inverse sine of each element in x, in radians and in the closed             interval [-pi/2, pi/2].
Example:
>>> from math import pi
>>> ti.asin(ti.Matrix([-1.0, 0.0, 1.0])) * 180 / pi
[-90., 0., 90.]
taichi.assume_in_range(val, base, low, high)#
Hints the compiler that a value is between a specified range,
for the compiler to perform scatchpad optimization, and return the
value untouched.
The assumed range is [base + low, base + high).
Parameters:
val (Number) – The input value.
base (Number) – The base point for the range interval.
low (Number) – The lower offset relative to base (included).
high (Number) – The higher offset relative to base (excluded).
Returns:
Return the input value untouched.
Example:
>>> # hint the compiler that x is in range [8, 12).
>>> x = ti.assume_in_range(x, 10, -2, 2)
>>> x
10
taichi.atan2(x1, x2)#
Element-wise arc tangent of x1/x2.
Parameters:
x1 (Union[primitive_types, Matrix]) – y-coordinates.
x2 (Union[primitive_types, Matrix]) – x-coordinates.
Returns:
Angles in radians, in the range [-pi, pi].
This is a scalar if both x1 and x2 are scalars.
Example:
>>> from math import pi
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-1.0, 1.0, -1.0, 1.0])
>>>     y = ti.Matrix([-1.0, -1.0, 1.0, 1.0])
>>>     z = ti.atan2(y, x) * 180 / pi
>>>     print(z)
>>>
>>> test()
[-135.0, -45.0, 135.0, 45.0]
taichi.atomic_add(x, y)#
Atomically compute x + y, store the result in x,
and return the old value of x.
x must be a writable target, constant expressions or scalars
are not allowed.
Parameters:
x (Union[primitive_types, Matrix]) – The input.
y (Union[primitive_types, Matrix]) – The input.
Returns:
The old value of x.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Vector([0, 0, 0])
>>>     y = ti.Vector([1, 2, 3])
>>>     z = ti.atomic_add(x, y)
>>>     print(x)  # [1, 2, 3]  the new value of x
>>>     print(z)  # [0, 0, 0], the old value of x
>>>
>>>     ti.atomic_add(1, x)  # will raise TaichiSyntaxError
taichi.atomic_and(x, y)#
Atomically compute the bit-wise AND of x and y, element-wise.
Store the result in x, and return the old value of x.
x must be a writable target, constant expressions or scalars
are not allowed.
Parameters:
x (Union[primitive_types, Matrix]) – The input. When both are matrices they must have the same shape.
y (Union[primitive_types, Matrix]) – The input. When both are matrices they must have the same shape.
Returns:
The old value of x.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Vector([-1, 0, 1])
>>>     y = ti.Vector([1, 2, 3])
>>>     z = ti.atomic_and(x, y)
>>>     print(x)  # [1, 0, 1]  the new value of x
>>>     print(z)  # [-1, 0, 1], the old value of x
>>>
>>>     ti.atomic_and(1, x)  # will raise TaichiSyntaxError
taichi.atomic_max(x, y)#
Atomically compute the maximum of x and y, element-wise.
Store the result in x, and return the old value of x.
x must be a writable target, constant expressions or scalars
are not allowed.
Parameters:
x (Union[primitive_types, Matrix]) – The input.
y (Union[primitive_types, Matrix]) – The input.
Returns:
The old value of x.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = 1
>>>     y = 2
>>>     z = ti.atomic_max(x, y)
>>>     print(x)  # 2  the new value of x
>>>     print(z)  # 1, the old value of x
>>>
>>>     ti.atomic_max(1, x)  # will raise TaichiSyntaxError
taichi.atomic_min(x, y)#
Atomically compute the minimum of x and y, element-wise.
Store the result in x, and return the old value of x.
x must be a writable target, constant expressions or scalars
are not allowed.
Parameters:
x (Union[primitive_types, Matrix]) – The input.
y (Union[primitive_types, Matrix]) – The input.
Returns:
The old value of x.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = 2
>>>     y = 1
>>>     z = ti.atomic_min(x, y)
>>>     print(x)  # 1  the new value of x
>>>     print(z)  # 2, the old value of x
>>>
>>>     ti.atomic_min(1, x)  # will raise TaichiSyntaxError
taichi.atomic_or(x, y)#
Atomically compute the bit-wise OR of x and y, element-wise.
Store the result in x, and return the old value of x.
x must be a writable target, constant expressions or scalars
are not allowed.
Parameters:
x (Union[primitive_types, Matrix]) – The input. When both are matrices they must have the same shape.
y (Union[primitive_types, Matrix]) – The input. When both are matrices they must have the same shape.
Returns:
The old value of x.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Vector([-1, 0, 1])
>>>     y = ti.Vector([1, 2, 3])
>>>     z = ti.atomic_or(x, y)
>>>     print(x)  # [-1, 2, 3]  the new value of x
>>>     print(z)  # [-1, 0, 1], the old value of x
>>>
>>>     ti.atomic_or(1, x)  # will raise TaichiSyntaxError
taichi.atomic_sub(x, y)#
Atomically subtract x by y, store the result in x,
and return the old value of x.
x must be a writable target, constant expressions or scalars
are not allowed.
Parameters:
x (Union[primitive_types, Matrix]) – The input.
y (Union[primitive_types, Matrix]) – The input.
Returns:
The old value of x.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Vector([0, 0, 0])
>>>     y = ti.Vector([1, 2, 3])
>>>     z = ti.atomic_sub(x, y)
>>>     print(x)  # [-1, -2, -3]  the new value of x
>>>     print(z)  # [0, 0, 0], the old value of x
>>>
>>>     ti.atomic_sub(1, x)  # will raise TaichiSyntaxError
taichi.atomic_xor(x, y)#
Atomically compute the bit-wise XOR of x and y, element-wise.
Store the result in x, and return the old value of x.
x must be a writable target, constant expressions or scalars
are not allowed.
Parameters:
x (Union[primitive_types, Matrix]) – The input. When both are matrices they must have the same shape.
y (Union[primitive_types, Matrix]) – The input. When both are matrices they must have the same shape.
Returns:
The old value of x.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Vector([-1, 0, 1])
>>>     y = ti.Vector([1, 2, 3])
>>>     z = ti.atomic_xor(x, y)
>>>     print(x)  # [-2, 2, 2]  the new value of x
>>>     print(z)  # [-1, 0, 1], the old value of x
>>>
>>>     ti.atomic_xor(1, x)  # will raise TaichiSyntaxError
taichi.axes(*x: Iterable[int])#
Defines a list of axes to be used by a field.
Parameters:
*x – A list of axes to be activated
Note that Taichi has already provided a set of commonly used axes. For example,
ti.ij is just axes(0, 1) under the hood.
taichi.bit_cast(obj, dtype)#
Copy and cast a scalar to a specified data type with its underlying
bits preserved. Must be called in taichi scope.
This function is equivalent to reinterpret_cast in C++.
Parameters:
obj (primitive_types) – Input scalar.
dtype (primitive_types) – Target data type, must have             the same precision bits as the input (hence f32 -> f64 is not allowed).
Returns:
A copy of obj, casted to the specified data type dtype.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = 3.14
>>>     y = ti.bit_cast(x, ti.i32)
>>>     print(y)  # 1078523331
>>>
>>>     z = ti.bit_cast(y, ti.f32)
>>>     print(z)  # 3.14
taichi.bit_shr(x1, x2)#
Elements in x1 shifted to the right by number of bits in x2.
Both x1, x2 must have integer type.
Parameters:
x1 (Union[primitive_types, Matrix]) – Input data.
x2 (Union[primitive_types, Matrix]) – Number of bits to remove at the right of x1.
Returns:
Return x1 with bits shifted x2 times to the right.
This is a scalar if both x1 and x2 are scalars.
Example::>>> @ti.kernel
>>> def main():
>>>     x = ti.Matrix([7, 8])
>>>     y = ti.Matrix([1, 2])
>>>     print(ti.bit_shr(x, y))
>>>
>>> main()
[3, 2]
taichi.block_local(*args)#
Hints Taichi to cache the fields and to enable the BLS optimization.
Please visit https://docs.taichi-lang.org/docs/performance
for how BLS is used.
Parameters:
*args (List[Field]) – A list of sparse Taichi fields.
taichi.cache_read_only(*args)#
taichi.cast(obj, dtype)#
Copy and cast a scalar or a matrix to a specified data type.
Must be called in Taichi scope.
Parameters:
obj (Union[primitive_types, Matrix]) – Input scalar or matrix.
dtype (primitive_types) – A primitive type defined in primitive_types.
Returns:
A copy of obj, casted to the specified data type dtype.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([0, 1, 2], ti.i32)
>>>     y = ti.cast(x, ti.f32)
>>>     print(y)
>>>
>>> test()
[0.0, 1.0, 2.0]
taichi.cc#
taichi.ceil(x, dtype=None)#
Return the ceiling of the input, element-wise.
The ceil of the scalar x is the smallest integer k, such that k >= x.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
dtype – (primitive_types): the returned type, default to None. If             set to None the retuned value will have the same type with x.
Returns:
The ceiling of each element in x, with return value type dtype.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([3.14, -1.5])
>>>     y = ti.ceil(x)
>>>     print(y)  # [4.0, -1.0]
taichi.cos(x)#
Trigonometric cosine, element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – Angle, in radians.
Returns:
The cosine of each element of x.
Example:
>>> from math import pi
>>> x = ti.Matrix([-pi, 0, pi/2.])
>>> ti.cos(x)
[-1., 1., 0.]
taichi.cpu#
A list of CPU backends supported on the current system.
Currently contains ‘x64’, ‘x86_64’, ‘arm64’, ‘cc’.
When this is used, Taichi automatically picks the matching CPU backend.
taichi.cuda#
The CUDA backend.
taichi.data_oriented(cls)#
Marks a class as Taichi compatible.
To allow for modularized code, Taichi provides this decorator so that
Taichi kernels can be defined inside a class.
See also https://docs.taichi-lang.org/docs/odop
Example:
>>> @ti.data_oriented
>>> class TiArray:
>>>     def __init__(self, n):
>>>         self.x = ti.field(ti.f32, shape=n)
>>>
>>>     @ti.kernel
>>>     def inc(self):
>>>         for i in self.x:
>>>             self.x[i] += 1.0
>>>
>>> a = TiArray(32)
>>> a.inc()
Parameters:
cls (Class) – the class to be decorated
Returns:
The decorated class.
taichi.dataclass(cls)#
Converts a class with field annotations and methods into a taichi struct type.
This will return a normal custom struct type, with the functions added to it.
Struct fields can be generated in the normal way from the struct type.
Functions in the class can be run on the struct instance.
This class decorator inspects the class for annotations and methods and
Sets the annotations as fields for the struct
Attaches the methods to the struct type
Example:
>>> @ti.dataclass
>>> class Sphere:
>>>     center: vec3
>>>     radius: ti.f32
>>>
>>>     @ti.func
>>>     def area(self):
>>>         return 4 * 3.14 * self.radius * self.radius
>>>
>>> my_spheres = Sphere.field(shape=(n, ))
>>> my_sphere[2].area()
Parameters:
cls (Class) – the class with annotations and methods to convert to a struct
Returns:
A taichi struct with the annotations as fieldsand methods from the class attached.
taichi.deactivate(node, indices)#
Explicitly deactivate a cell of node at location indices.
After deactivation, the Taichi runtime automatically recycles and zero-fills
the memory of the deactivated cell.
Parameters:
node (SNode) – Must be a pointer, hash or bitmasked node.
indices (Union[int, Vector]) – the indices to deactivate.
taichi.deactivate_all_snodes()#
Recursively deactivate all SNodes.
taichi.dx11#
The DX11 backend.
taichi.dx12#
The DX11 backend.
taichi.eig(A, dt=None)#
Compute the eigenvalues and right eigenvectors of a real matrix.
Mathematical concept refers to https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix.
Parameters:
A (ti.Matrix(n, n)) – 2D Matrix for which the eigenvalues and right eigenvectors will be computed.
dt (DataType) – The datatype for the eigenvalues and right eigenvectors.
Returns:
The eigenvalues in complex form. Each row stores one eigenvalue. The first number of the eigenvalue represents the real part and the second number represents the imaginary part.
eigenvectors (ti.Matrix(n*2, n)): The eigenvectors in complex form. Each column stores one eigenvector. Each eigenvector consists of n entries, each of which is represented by two numbers for its real part and imaginary part.
Return type:
eigenvalues (ti.Matrix(n, 2))
taichi.exp(x)#
Compute the exponential of all elements in x, element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
Returns:
Element-wise exponential of x.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-1.0, 0.0, 1.0])
>>>     y = ti.exp(x)
>>>     print(y)
>>>
>>> test()
[0.367879, 1.000000, 2.718282]
taichi.extension#
An instance of Taichi extension.
The list of currently available extensions is [‘sparse’, ‘quant’,     ‘mesh’, ‘quant_basic’, ‘data64’, ‘adstack’, ‘bls’, ‘assertion’,         ‘extfunc’].
taichi.f16#
Alias for float16
taichi.f32#
Alias for float32
taichi.f64#
Alias for float64
taichi.field(dtype, *args, **kwargs)#
Defines a Taichi field.
A Taichi field can be viewed as an abstract N-dimensional array, hiding away
the complexity of how its underlying SNode are
actually defined. The data in a Taichi field can be directly accessed by
a Taichi kernel().
See also https://docs.taichi-lang.org/docs/field
Parameters:
dtype (DataType) – data type of the field. Note it can be vector or matrix types as well.
shape (Union[int, tuple[int]], optional) – shape of the field.
order (str, optional) – order of the shape laid out in memory.
name (str, optional) – name of the field.
offset (Union[int, tuple[int]], optional) – offset of the field domain.
needs_grad (bool, optional) – whether this field participates in autodiff (reverse mode)
and thus needs an adjoint field to store the gradients.
needs_dual (bool, optional) – whether this field participates in autodiff (forward mode)
and thus needs an dual field to store the gradients.
Example:
The code below shows how a Taichi field can be declared and defined::
    >>> x1 = ti.field(ti.f32, shape=(16, 8))
    >>> # Equivalently
    >>> x2 = ti.field(ti.f32)
    >>> ti.root.dense(ti.ij, shape=(16, 8)).place(x2)
    >>>
    >>> x3 = ti.field(ti.f32, shape=(16, 8), order='ji')
    >>> # Equivalently
    >>> x4 = ti.field(ti.f32)
    >>> ti.root.dense(ti.j, shape=8).dense(ti.i, shape=16).place(x4)
    >>>
    >>> x5 = ti.field(ti.math.vec3, shape=(16, 8))
taichi.float16#
16-bit precision floating point data type.
taichi.float32#
32-bit single precision floating point data type.
taichi.float64#
64-bit double precision floating point data type.
taichi.floor(x, dtype=None)#
Return the floor of the input, element-wise.
The floor of the scalar x is the largest integer k, such that k <= x.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
dtype – (primitive_types): the returned type, default to None. If             set to None the retuned value will have the same type with x.
Returns:
The floor of each element in x, with return value type dtype.
Example::>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-1.1, 2.2, 3.])
>>>     y = ti.floor(x, ti.f64)
>>>     print(y)  # [-2.000000000000, 2.000000000000, 3.000000000000]
taichi.frexp(x)#
taichi.func(fn, is_real_function=False)#
Marks a function as callable in Taichi-scope.
This decorator transforms a Python function into a Taichi one. Taichi
will JIT compile it into native instructions.
Parameters:
fn (Callable) – The Python function to be decorated
is_real_function (bool) – Whether the function is a real function
Returns:
The decorated function
Return type:
Callable
Example:
>>> @ti.func
>>> def foo(x):
>>>     return x + 2
>>>
>>> @ti.kernel
>>> def run():
>>>     print(foo(40))  # 42
taichi.get_addr(f, indices)#
Query the memory address (on CUDA/x64) of field f at index indices.
Currently, this function can only be called inside a taichi kernel.
Parameters:
f (Union[Field, MatrixField]) – Input taichi field for memory address query.
indices (Union[int, Vector]) – The specified field indices of the query.
Returns:
The memory address of f[indices].
Return type:
ti.u64
taichi.gles#
The OpenGL ES backend. OpenGL ES 3.1 required.
taichi.global_thread_idx()#
Returns the global thread id of this running thread,
only available for cpu and cuda backends.
For cpu backends this is equal to the cpu thread id,
For cuda backends this is equal to block_id * block_dim + thread_id.
Example:
>>> f = ti.field(ti.f32, shape=(16, 16))
>>> @ti.kernel
>>> def test():
>>>     for i in ti.grouped(f):
>>>         print(ti.global_thread_idx())
>>>
test()
taichi.gpu#
A list of GPU backends supported on the current system.
Currently contains ‘cuda’, ‘metal’, ‘opengl’, ‘vulkan’, ‘dx11’, ‘dx12’, ‘gles’, ‘amdgpu’.
When this is used, Taichi automatically picks the matching GPU backend. If no
GPU is detected, Taichi falls back to the CPU backend.
taichi.grouped(x)#
Groups the indices in the iterator returned by ndrange() into a 1-D vector.
This is often used when you want to iterate over all indices returned by ndrange()
in one for loop and a single index.
Parameters:
x (ndrange()) – an iterator object returned by ti.ndrange.
Example::>>> # without ti.grouped
>>> for I in ti.ndrange(2, 3):
>>>     print(I)
prints 0, 1, 2, 3, 4, 5
>>> # with ti.grouped
>>> for I in ti.grouped(ndrange(2, 3)):
>>>     print(I)
prints [0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2]
taichi.hex_to_rgb(color)#
Converts hex color format to rgb color format.
Parameters:
color (int) – The hex representation of color.
Returns:
The rgb representation of color.
taichi.i#
Axis 0. For multi-dimensional arrays it’s the direction downward the rows.
For a 1d array it’s the direction along this array.
taichi.i16#
Alias for int16
taichi.i32#
Alias for int32
taichi.i64#
Alias for int64
taichi.i8#
Alias for int8
taichi.ij#
Axes (0, 1).
taichi.ijk#
Axes (0, 1, 2).
taichi.ijkl#
Axes (0, 1, 2, 3).
taichi.ijl#
Axes (0, 1, 3).
taichi.ik#
Axes (0, 2).
taichi.ikl#
Axes (0, 2, 3).
taichi.il#
Axes (0, 3).
taichi.init(arch=None, default_fp=None, default_ip=None, _test_mode=False, enable_fallback=True, require_version=None, **kwargs)#
Initializes the Taichi runtime.
This should always be the entry point of your Taichi program. Most
importantly, it sets the backend used throughout the program.
Parameters:
arch – Backend to use. This is usually cpu or gpu.
default_fp (Optional[type]) – Default floating-point type.
default_ip (Optional[type]) – Default integral type.
require_version (Optional[string]) – A version string.
**kwargs – Taichi provides highly customizable compilation through
kwargs, which allows for fine grained control of Taichi compiler
behavior. Below we list some of the most frequently used ones. For a
complete list, please check out
https://github.com/taichi-dev/taichi/blob/master/taichi/program/compile_config.h.
cpu_max_num_threads (int): Sets the number of threads used by the CPU thread pool.
debug (bool): Enables the debug mode, under which Taichi does a few more things like boundary checks.
print_ir (bool): Prints the CHI IR of the Taichi kernels.
*offline_cache (bool): Enables offline cache of the compiled kernels. Default to True. When this is enabled Taichi will cache compiled kernel on your local disk to accelerate future calls.
*random_seed (int): Sets the seed of the random generator. The default is 0.
taichi.int16#
16-bit signed integer data type.
taichi.int32#
32-bit signed integer data type.
taichi.int64#
64-bit signed integer data type.
taichi.int8#
8-bit signed integer data type.
taichi.is_active(node, indices)#
Explicitly query whether a cell in a SNode node at location
indices is active or not.
Parameters:
node (SNode) – Must be a pointer, hash or bitmasked node.
indices (Union[int, list, Vector]) – the indices to visit.
Returns:
the cell node[indices] is active or not.
Return type:
bool
taichi.is_logging_effective(level)#
Check if the specified logging level is effective.
All levels below current level will be effective.
The default level is ‘info’.
See also https://docs.taichi-lang.org/docs/developer_utilities#logging.
Parameters:
level (str) – The string represents logging level.             Effective levels include: ‘trace’, ‘debug’, ‘info’, ‘warn’, ‘error’, ‘critical’.
Returns:
Indicate whether the logging level is effective.
Return type:
Bool
Example:
>>> # assume current level is 'info'
>>> print(ti.is_logging_effective("trace"))     # False
>>> print(ti.is_logging_effective("debug"))     # False
>>> print(ti.is_logging_effective("info"))      # True
>>> print(ti.is_logging_effective("warn"))      # True
>>> print(ti.is_logging_effective("error"))     # True
>>> print(ti.is_logging_effective("critical"))  # True
taichi.j#
Axis 1. For multi-dimensional arrays it’s the direction across the columns.
taichi.jk#
Axes (1, 2).
taichi.jkl#
Axes (1, 2, 3).
taichi.jl#
Axes (1, 3).
taichi.k#
Axis 2. For arrays of dimension d >= 3, view each cell as an array of
lower dimension d-2, it’s the first axis of this cell.
taichi.kernel(fn)#
Marks a function as a Taichi kernel.
A Taichi kernel is a function written in Python, and gets JIT compiled by
Taichi into native CPU/GPU instructions (e.g. a series of CUDA kernels).
The top-level for loops are automatically parallelized, and distributed
to either a CPU thread pool or massively parallel GPUs.
Kernel’s gradient kernel would be generated automatically by the AutoDiff system.
See also https://docs.taichi-lang.org/docs/syntax#kernel.
Parameters:
fn (Callable) – the Python function to be decorated
Returns:
The decorated function
Return type:
Callable
Example:
>>> x = ti.field(ti.i32, shape=(4, 8))
>>>
>>> @ti.kernel
>>> def run():
>>>     # Assigns all the elements of `x` in parallel.
>>>     for i in x:
>>>         x[i] = i
taichi.kl#
Axes (2, 3).
taichi.l#
Axis 3. For arrays of dimension d >= 4, view each cell as an array of
lower dimension d-2, it’s the second axis of this cell.
taichi.length(node, indices)#
Return the length of the dynamic SNode node at index indices.
Parameters:
node (SNode) – a dynamic SNode.
indices (Union[int, Vector]) – the indices to query.
Returns:
the length of cell node[indices].
Return type:
int
taichi.log(x)#
Compute the natural logarithm, element-wise.
The natural logarithm log is the inverse of the exponential function,
so that log(exp(x)) = x. The natural logarithm is logarithm in base e.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
Returns:
The natural logarithm of x, element-wise.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Vector([-1.0, 0.0, 1.0])
>>>     y = ti.log(x)
>>>     print(y)
>>>
>>> test()
[-nan, -inf, 0.000000]
taichi.loop_config(block_dim=None, serialize=False, parallelize=None, block_dim_adaptive=True, bit_vectorize=False)#
Sets directives for the next loop
Parameters:
block_dim (int) – The number of threads in a block on GPU
serialize (bool) – Whether to let the for loop execute serially, serialize=True equals to parallelize=1
parallelize (int) – The number of threads to use on CPU
block_dim_adaptive (bool) – Whether to allow backends set block_dim adaptively, enabled by default
bit_vectorize (bool) – Whether to enable bit vectorization of struct fors on quant_arrays.
Examples:
@ti.kernel
def break_in_serial_for() -> ti.i32:
    a = 0
    ti.loop_config(serialize=True)
    for i in range(100):  # This loop runs serially
        a += i
        if i == 10:
            break
    return a
break_in_serial_for()  # returns 55
n = 128
val = ti.field(ti.i32, shape=n)
@ti.kernel
def fill():
    ti.loop_config(parallelize=8, block_dim=16)
    # If the kernel is run on the CPU backend, 8 threads will be used to run it
    # If the kernel is run on the CUDA backend, each block will have 16 threads.
    for i in range(n):
        val[i] = i
u1 = ti.types.quant.int(bits=1, signed=False)
x = ti.field(dtype=u1)
y = ti.field(dtype=u1)
cell = ti.root.dense(ti.ij, (128, 4))
cell.quant_array(ti.j, 32).place(x)
cell.quant_array(ti.j, 32).place(y)
@ti.kernel
def copy():
    ti.loop_config(bit_vectorize=True)
    # 32 bits, instead of 1 bit, will be copied at a time
    for i, j in x:
        y[i, j] = x[i, j]
taichi.max(*args)#
Compute the maximum of the arguments, element-wise.
This function takes no effect on a single argument, even it’s array-like.
When there are both scalar and matrix arguments in args, the matrices
must have the same shape, and scalars will be broadcasted to the same shape as the matrix.
Parameters:
args – (List[primitive_types, Matrix]):             The input.
Returns:
Maximum of the inputs.
Example:
>>> @ti.kernel
>>> def foo():
>>>     x = ti.Vector([0, 1, 2])
>>>     y = ti.Vector([3, 4, 5])
>>>     z = ti.max(x, y, 4)
>>>     print(z)  # [4, 4, 5]
taichi.mesh_local(*args)#
Hints the compiler to cache the mesh attributes
and to enable the mesh BLS optimization,
only available for backends supporting ti.extension.mesh and to use with mesh-for loop.
Related to https://github.com/taichi-dev/taichi/issues/3608
Parameters:
*args (List[Attribute]) – A list of mesh attributes or fields accessed as attributes.
Examples:
# instantiate model
mesh_builder = ti.Mesh.tri()
mesh_builder.verts.place({
    'x' : ti.f32,
    'y' : ti.f32
})
model = mesh_builder.build(meta)
@ti.kernel
def foo():
    # hint the compiler to cache mesh vertex attribute `x` and `y`.
    ti.mesh_local(model.verts.x, model.verts.y)
    for v0 in model.verts: # mesh-for loop
        for v1 in v0.verts:
            v0.x += v1.y
taichi.mesh_patch_idx()#
Returns the internal mesh patch id of this running thread,
only available for backends supporting ti.extension.mesh and to use within mesh-for loop.
Related to https://github.com/taichi-dev/taichi/issues/3608
taichi.metal#
The Apple Metal backend.
taichi.min(*args)#
Compute the minimum of the arguments, element-wise.
This function takes no effect on a single argument, even it’s array-like.
When there are both scalar and matrix arguments in args, the matrices
must have the same shape, and scalars will be broadcasted to the same shape as the matrix.
Parameters:
args – (List[primitive_types, Matrix]):             The input.
Returns:
Minimum of the inputs.
Example:
>>> @ti.kernel
>>> def foo():
>>>     x = ti.Vector([0, 1, 2])
>>>     y = ti.Vector([3, 4, 5])
>>>     z = ti.min(x, y, 1)
>>>     print(z)  # [0, 1, 1]
taichi.ndarray(dtype, shape, needs_grad=False)#
Defines a Taichi ndarray with scalar elements.
Parameters:
dtype (Union[DataType, MatrixType]) – Data type of each element. This can be either a scalar type like ti.f32 or a compound type like ti.types.vector(3, ti.i32).
shape (Union[int, tuple[int]]) – Shape of the ndarray.
Example
The code below shows how a Taichi ndarray with scalar elements can be declared and defined:
>>> x = ti.ndarray(ti.f32, shape=(16, 8))  # ndarray of shape (16, 8), each element is ti.f32 scalar.
>>> vec3 = ti.types.vector(3, ti.i32)
>>> y = ti.ndarray(vec3, shape=(10, 2))  # ndarray of shape (10, 2), each element is a vector of 3 ti.i32 scalars.
>>> matrix_ty = ti.types.matrix(3, 4, float)
>>> z = ti.ndarray(matrix_ty, shape=(4, 5))  # ndarray of shape (4, 5), each element is a matrix of (3, 4) ti.float scalars.
taichi.ndrange(*args) → Iterable#
Return an immutable iterator object for looping over multi-dimensional indices.
This returned set of multi-dimensional indices is the direct product (in the set-theory sense)
of n groups of integers, where n equals the number of arguments in the input list, and looks like
range(x1, y1) x range(x2, y2) x … x range(xn, yn)
The k-th argument corresponds to the k-th range() factor in the above product, and each
argument must be an integer or a pair of two integers. An integer argument n will be interpreted
as range(0, n), and a pair of two integers (start, end) will be interpreted as range(start, end).
You can loop over these multi-dimensonal indices in different ways, see the examples below.
Parameters:
entries – (int, tuple): Must be either an integer, or a tuple/list of two integers.
Returns:
An immutable iterator object.
Example:
You can loop over 1-D integers in range [start, end), as in native Python
    >>> @ti.kernel
    >>> def loop_1d():
    >>>     start = 2
    >>>     end = 5
    >>>     for i in ti.ndrange((start, end)):
    >>>         print(i)  # will print 2 3 4
Note the braces around `(start, end)` in the above code. If without them,
the parameter `2` will be interpreted as `range(0, 2)`, `5` will be
interpreted as `range(0, 5)`, and you will get a set of 2-D indices which
contains 2x5=10 elements, and need two indices i, j to loop over them:
    >>> @ti.kernel
    >>> def loop_2d():
    >>>     for i, j in ti.ndrange(2, 5):
    >>>         print(i, j)
    0 0
    ...
    0 4
    ...
    1 4
But you do can use a single index i to loop over these 2-D indices, in this case
the indices are returned as a 1-D array `(0, 1, ..., 9)`:
    >>> @ti.kernel
    >>> def loop_2d_as_1d():
    >>>     for i in ti.ndrange(2, 5):
    >>>         print(i)
    will print 0 1 2 3 4 5 6 7 8 9
In general, you can use any `1 <= k <= n` iterators to loop over a set of n-D
indices. For `k=n` all the indices are n-dimensional, and they are returned in
lexical order, but for `k<n` iterators the last n-k+1 dimensions will be collapsed into
a 1-D array of consecutive integers `(0, 1, 2, ...)` whose length equals the
total number of indices in the last n-k+1 dimensions:
    >>> @ti.kernel
    >>> def loop_3d_as_2d():
    >>>     # use two iterators to loop over a set of 3-D indices
    >>>     # the last two dimensions for 4, 5 will collapse into
    >>>     # the array [0, 1, 2, ..., 19]
    >>>     for i, j in ti.ndrange(3, 4, 5):
    >>>         print(i, j)
    will print 0 0, 0 1, ..., 0 19, ..., 2 19.
A typical usage of `ndrange` is when you want to loop over a tensor and process
its entries in parallel. You should avoid writing nested `for` loops here since
only top level `for` loops are paralleled in taichi, instead you can use `ndrange`
to hold all entries in one top level loop:
    >>> @ti.kernel
    >>> def loop_tensor():
    >>>     for row, col, channel in ti.ndrange(image_height, image_width, channels):
    >>>         image[row, col, channel] = ...
taichi.no_activate(*args)#
Deactivates a SNode pointer.
taichi.one(x)#
Returns an array of ones with the same shape and type as the input. It’s also a scalar
if the input is a scalar.
Parameters:
x (Union[primitive_types, Matrix]) – The input.
Returns:
A new copy of the input but filled with ones.
Example:
>>> x = ti.Vector([0, 0])
>>> @ti.kernel
>>> def test():
>>>     y = ti.one(x)
>>>     print(y)
[1, 1]
taichi.opengl#
The OpenGL backend. OpenGL 4.3 required.
taichi.polar_decompose(A, dt=None)#
Perform polar decomposition (A=UP) for arbitrary size matrix.
Mathematical concept refers to https://en.wikipedia.org/wiki/Polar_decomposition.
Parameters:
A (ti.Matrix(n, n)) – input nxn matrix A.
dt (DataType) – date type of elements in matrix A, typically accepts ti.f32 or ti.f64.
Returns:
Decomposed nxn matrices U and P.
taichi.pow(base, exponent)#
First array elements raised to second array elements \({base}^{exponent}\), element-wise.
The result type of two scalar operands is determined as follows:
- If the exponent is an integral value, then the result type takes the type of the base.
- Otherwise, the result type follows
[Implicit type casting in binary operations](https://docs.taichi-lang.org/docs/type#implicit-type-casting-in-binary-operations).
With the above rules, an integral value raised to a negative integral value cannot have a
feasible type. Therefore, an exception will be raised if debug mode or optimization passes
are on; otherwise 1 will be returned.
In the following situations, the result is undefined:
- A negative value raised to a non-integral value.
- A zero value raised to a non-positive value.
Parameters:
base (Union[primitive_types, Matrix]) – The bases.
exponent (Union[primitive_types, Matrix]) – The exponents.
Returns:
base raised to exponent. This is a scalar if both base and exponent are scalars.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-2.0, 2.0])
>>>     y = -3
>>>     z = ti.pow(x, y)
>>>     print(z)
>>>
>>> test()
[-0.125000, 0.125000]
taichi.pyfunc(fn)#
Marks a function as callable in both Taichi and Python scopes.
When called inside the Taichi scope, Taichi will JIT compile it into
native instructions. Otherwise it will be invoked directly as a
Python function.
See also func().
Parameters:
fn (Callable) – The Python function to be decorated
Returns:
The decorated function
Return type:
Callable
taichi.randn(dt=None)#
Generate a random float sampled from univariate standard normal
(Gaussian) distribution of mean 0 and variance 1, using the
Box-Muller transformation. Must be called in Taichi scope.
Parameters:
dt (DataType) – Data type of the required random number. Default to None.
If set to None dt will be determined dynamically in runtime.
Returns:
The generated random float.
Example:
>>> @ti.kernel
>>> def main():
>>>     print(ti.randn())
>>>
>>> main()
-0.463608
taichi.random(dtype=float) → float | int#
Return a single random float/integer according to the specified data type.
Must be called in taichi scope.
If the required dtype is float type, this function returns a random number
sampled from the uniform distribution in the half-open interval [0, 1).
For integer types this function returns a random integer in the
half-open interval [0, 2^32) if a 32-bit integer is required,
or a random integer in the half-open interval [0, 2^64) if a
64-bit integer is required.
Parameters:
dtype (primitive_types) – Type of the required random value.
Returns:
A random value with type dtype.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.random(float)
>>>     print(x)  # 0.090257
>>>
>>>     y = ti.random(ti.f64)
>>>     print(y)  # 0.716101627301
>>>
>>>     i = ti.random(ti.i32)
>>>     print(i)  # -963722261
>>>
>>>     j = ti.random(ti.i64)
>>>     print(j)  # 73412986184350777
taichi.raw_div(x1, x2)#
Return x1 // x2 if both x1, x2 are integers, otherwise return x1/x2.
Parameters:
x1 (Union[primitive_types, Matrix]) – Dividend.
x2 (Union[primitive_types, Matrix]) – Divisor.
Returns:
Return x1 // x2 if both x1, x2 are integers, otherwise return x1/x2.
Example:
>>> @ti.kernel
>>> def main():
>>>     x = 5
>>>     y = 3
>>>     print(raw_div(x, y))  # 1
>>>     z = 4.0
>>>     print(raw_div(x, z))  # 1.25
taichi.raw_mod(x1, x2)#
Return the remainder of x1/x2, element-wise.
This is the C-style mod function.
Parameters:
x1 (Union[primitive_types, Matrix]) – The dividend.
x2 (Union[primitive_types, Matrix]) – The divisor.
Returns:
The remainder of x1 divided by x2.
Example:
>>> @ti.kernel
>>> def main():
>>>     print(ti.mod(-4, 3))  # 2
>>>     print(ti.raw_mod(-4, 3))  # -1
taichi.ref(tp)#
taichi.rescale_index(a, b, I)#
Rescales the index ‘I’ of field (or SNode) ‘a’ to match the shape of SNode ‘b’.
Parameters:
a (Union[Field, MatrixField) – Input taichi fields or snodes.
b (Union[Field, MatrixField) – Input taichi fields or snodes.
I (Union[list, Vector]) – grouped loop index.
Returns:
rescaled grouped loop index
Return type:
Ib (Vector)
taichi.reset()#
Resets Taichi to its initial state.
This will destroy all the allocated fields and kernels, and restore
the runtime to its default configuration.
Example:
>>> a = ti.field(ti.i32, shape=())
>>> a[None] = 1
>>> print("before reset: ", a)
before rest: 1
>>>
>>> ti.reset()
>>> print("after reset: ", a)
# will raise error because a is unavailable after reset.
taichi.rgb_to_hex(c)#
Converts rgb color format to hex color format.
Parameters:
c (List[int]) – The rgb representation of color.
Returns:
The hex representation of color.
taichi.root#
Root of the declared Taichi :func:`~taichi.lang.impl.field`s.
See also https://docs.taichi-lang.org/docs/layout
Example:
>>> x = ti.field(ti.f32)
>>> ti.root.pointer(ti.ij, 4).dense(ti.ij, 8).place(x)
taichi.round(x, dtype=None)#
Round to the nearest integer, element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – A scalar or a matrix.
dtype – (primitive_types): the returned type, default to None. If             set to None the retuned value will have the same type with x.
Returns:
The nearest integer of x, with return value type dtype.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Vector([-1.5, 1.2, 2.7])
>>>     print(ti.round(x))
[-2., 1., 3.]
taichi.rsqrt(x)#
The reciprocal of the square root function.
Parameters:
x (Union[primitive_types, Matrix]) – A scalar or a matrix.
Returns:
The reciprocal of sqrt(x).
taichi.select(cond, x1, x2)#
Return an array drawn from elements in x1 or x2,
depending on the conditions in cond.
Parameters:
cond (Union[primitive_types, Matrix]) – The array of conditions.
x1 (Union[primitive_types, Matrix]) – The arrays where the output elements are taken from.
x2 (Union[primitive_types, Matrix]) – The arrays where the output elements are taken from.
Returns:
The output at position k is the k-th element of x1 if the k-th element
in cond is True, otherwise it’s the k-th element of x2.
Example:
>>> @ti.kernel
>>> def main():
>>>     cond = ti.Matrix([0, 1, 0, 1])
>>>     x = ti.Matrix([1, 2, 3, 4])
>>>     y = ti.Matrix([-1, -2, -3, -4])
>>>     print(ti.select(cond, x, y))
>>>
>>> main()
[-1, 2, -3, 4]
taichi.set_logging_level(level)#
Setting the logging level to a specified value.
Available levels are: ‘trace’, ‘debug’, ‘info’, ‘warn’, ‘error’, ‘critical’.
Note that after calling this function, logging levels below the specified one will
also be effective. For example if level is set to ‘warn’, then the levels below
it, which are ‘error’ and ‘critical’ in this case, will also be effective.
See also https://docs.taichi-lang.org/docs/developer_utilities#logging.
Parameters:
level (str) – Logging level.
Example:
>>> set_logging_level('debug')
taichi.sin(x)#
Trigonometric sine, element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – Angle, in radians.
Returns:
The sine of each element of x.
Example:
>>> from math import pi
>>> x = ti.Matrix([-pi/2., 0, pi/2.])
>>> ti.sin(x)
[-1., 0., 1.]
taichi.solve(A, b, dt=None)#
Solve a matrix using Gauss elimination method.
Parameters:
A (ti.Matrix(n, n)) – input nxn matrix A.
b (ti.Vector(n, 1)) – input nx1 vector b.
dt (DataType) – The datatype for the A and b.
Returns:
the solution of Ax=b.
Return type:
x (ti.Vector(n, 1))
class taichi.sparse_matrix_builder#
taichi.sqrt(x)#
Return the non-negative square-root of a scalar or a matrix,
element wise. If x < 0 an exception is raised.
Parameters:
x (Union[primitive_types, Matrix]) – The scalar or matrix whose square-roots are required.
Returns:
The square-root y so that y >= 0 and y^2 = x. y has the same type as x.
Example:
>>> x = ti.Matrix([1., 4., 9.])
>>> y = ti.sqrt(x)
>>> y
[1.0, 2.0, 3.0]
taichi.static(x, *xs) → Any#
Evaluates a Taichi-scope expression at compile time.
static() is what enables the so-called metaprogramming in Taichi. It is
in many ways similar to constexpr in C++.
See also https://docs.taichi-lang.org/docs/meta.
Parameters:
x (Any) – an expression to be evaluated
*xs (Any) – for Python-ish swapping assignment
Example
The most common usage of static() is for compile-time evaluation:
>>> cond = False
>>>
>>> @ti.kernel
>>> def run():
>>>     if ti.static(cond):
>>>         do_a()
>>>     else:
>>>         do_b()
Depending on the value of cond, run() will be directly compiled
into either do_a() or do_b(). Thus there won’t be a runtime
condition check.
Another common usage is for compile-time loop unrolling:
>>> @ti.kernel
>>> def run():
>>>     for i in ti.static(range(3)):
>>>         print(i)
>>>
>>> # The above will be unrolled to:
>>> @ti.kernel
>>> def run():
>>>     print(0)
>>>     print(1)
>>>     print(2)
taichi.static_assert(cond, msg=None)#
Throw AssertionError when cond is False.
This function is called at compile time and has no runtime overhead.
The bool value in cond must can be determined at compile time.
Parameters:
cond (bool) – an expression with a bool value.
msg (str) – assertion message.
Example:
>>> year = 2001
>>> @ti.kernel
>>> def test():
>>>     ti.static_assert(year % 4 == 0, "the year must be a lunar year")
AssertionError: the year must be a lunar year
taichi.static_print(*args, __p=print, **kwargs)#
The print function in Taichi scope.
This function is called at compile time and has no runtime overhead.
taichi.stop_grad(x)#
Stops computing gradients during back propagation.
Parameters:
x (Field) – A field.
taichi.svd(A, dt=None)#
Perform singular value decomposition (A=USV^T) for arbitrary size matrix.
Mathematical concept refers to https://en.wikipedia.org/wiki/Singular_value_decomposition.
Parameters:
A (ti.Matrix(n, n)) – input nxn matrix A.
dt (DataType) – date type of elements in matrix A, typically accepts ti.f32 or ti.f64.
Returns:
Decomposed nxn matrices U, ‘S’ and V.
taichi.sym_eig(A, dt=None)#
Compute the eigenvalues and right eigenvectors of a real symmetric matrix.
Mathematical concept refers to https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix.
Parameters:
A (ti.Matrix(n, n)) – Symmetric Matrix for which the eigenvalues and right eigenvectors will be computed.
dt (DataType) – The datatype for the eigenvalues and right eigenvectors.
Returns:
The eigenvalues. Each entry store one eigen value.
eigenvectors (ti.Matrix(n, n)): The eigenvectors. Each column stores one eigenvector.
Return type:
eigenvalues (ti.Vector(n))
taichi.sync()#
Blocks the calling thread until all the previously
launched Taichi kernels have completed.
taichi.tan(x)#
Trigonometric tangent function, element-wise.
Equivalent to ti.sin(x)/ti.cos(x) element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
Returns:
The tangent values of x.
Example:
>>> from math import pi
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-pi, pi/2, pi])
>>>     y = ti.tan(x)
>>>     print(y)
>>>
>>> test()
[-0.0, -22877334.0, 0.0]
taichi.tanh(x)#
Compute the hyperbolic tangent of x, element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
Returns:
The corresponding hyperbolic tangent values.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-1.0, 0.0, 1.0])
>>>     y = ti.tanh(x)
>>>     print(y)
>>>
>>> test()
[-0.761594, 0.000000, 0.761594]
taichi.template#
Alias for Template.
taichi.u16#
Alias for uint16
taichi.u32#
Alias for uint32
taichi.u64#
Alias for uint64
taichi.u8#
Alias for uint8
taichi.uint16#
16-bit unsigned integer data type.
taichi.uint32#
32-bit unsigned integer data type.
taichi.uint64#
64-bit unsigned integer data type.
taichi.uint8#
8-bit unsigned integer data type.
taichi.vulkan#
The Vulkan backend.
taichi.warn_restricted_version()#
taichi.x64#
The X64 CPU backend.
taichi.x86_64#
The x64 CPU backend.
taichi.zero(x)#
Returns an array of zeros with the same shape and type as the input. It’s also a scalar
if the input is a scalar.
Parameters:
x (Union[primitive_types, Matrix]) – The input.
Returns:
A new copy of the input but filled with zeros.
Example:
>>> x = ti.Vector([1, 1])
>>> @ti.kernel
>>> def test():
>>>     y = ti.zero(x)
>>>     print(y)
[0, 0]
Was this helpful?BitpackedFieldsBitpackedFields.place()CRITICALDEBUGDeviceCapabilityDeviceCapability.spirv_has_atomic_floatDeviceCapability.spirv_has_atomic_float16DeviceCapability.spirv_has_atomic_float16_addDeviceCapability.spirv_has_atomic_float16_minmaxDeviceCapability.spirv_has_atomic_float64DeviceCapability.spirv_has_atomic_float64_addDeviceCapability.spirv_has_atomic_float64_minmaxDeviceCapability.spirv_has_atomic_float_addDeviceCapability.spirv_has_atomic_float_minmaxDeviceCapability.spirv_has_atomic_int64DeviceCapability.spirv_has_float16DeviceCapability.spirv_has_float64DeviceCapability.spirv_has_int16DeviceCapability.spirv_has_int64DeviceCapability.spirv_has_int8DeviceCapability.spirv_has_no_integer_wrap_decorationDeviceCapability.spirv_has_non_semantic_infoDeviceCapability.spirv_has_physical_storage_bufferDeviceCapability.spirv_has_subgroup_arithmeticDeviceCapability.spirv_has_subgroup_ballotDeviceCapability.spirv_has_subgroup_basicDeviceCapability.spirv_has_subgroup_voteDeviceCapability.spirv_has_variable_ptrDeviceCapability.spirv_version_1_3DeviceCapability.spirv_version_1_4DeviceCapability.spirv_version_1_5ERRORFieldField.copy_from()Field.from_paddle()Field.from_torch()Field.parent()FieldsBuilderFieldsBuilder.bitmasked()FieldsBuilder.deactivate_all()FieldsBuilder.dense()FieldsBuilder.dynamic()FieldsBuilder.finalize()FieldsBuilder.lazy_dual()FieldsBuilder.lazy_grad()FieldsBuilder.place()FieldsBuilder.pointer()FieldsBuilder.quant_array()FormatGUIGUI.EventGUI.EventFilterGUI.EventFilter.match()GUI.WidgetValueGUI.ALTGUI.BACKSPACEGUI.CAPSLOCKGUI.CTRLGUI.DOWNGUI.ESCAPEGUI.EXITGUI.LEFTGUI.LMBGUI.MMBGUI.MOTIONGUI.MOVEGUI.PRESSGUI.RELEASEGUI.RETURNGUI.RIGHTGUI.RMBGUI.SHIFTGUI.SPACEGUI.TABGUI.UPGUI.WHEELGUI.arrow()GUI.arrow_field()GUI.arrows()GUI.button()GUI.circle()GUI.circles()GUI.clear()GUI.close()GUI.contour()GUI.cook_image()GUI.get_bool_environ()GUI.get_cursor_pos()GUI.get_event()GUI.get_events()GUI.get_image()GUI.get_key_event()GUI.has_key_event()GUI.is_pressed()GUI.label()GUI.line()GUI.lines()GUI.point_field()GUI.rect()GUI.set_image()GUI.show()GUI.slider()GUI.text()GUI.triangle()GUI.triangles()GUI.vector_field()matchINFOLayoutMatrixMatrix.all()Matrix.any()Matrix.cast()Matrix.cols()Matrix.cross()Matrix.determinant()Matrix.diag()Matrix.dot()Matrix.field()Matrix.fill()Matrix.get_shape()Matrix.identity()Matrix.inverse()Matrix.max()Matrix.min()Matrix.ndarray()Matrix.norm()Matrix.norm_inv()Matrix.norm_sqr()Matrix.normalized()Matrix.one()Matrix.outer_product()Matrix.rows()Matrix.sum()Matrix.to_list()Matrix.to_numpy()Matrix.trace()Matrix.transpose()Matrix.unit()Matrix.zero()MatrixFieldMatrixField.fill()MatrixField.from_numpy()MatrixField.get_scalar_field()MatrixField.to_numpy()MatrixField.to_paddle()MatrixField.to_torch()MatrixNdarrayMatrixNdarray.copy_from()MatrixNdarray.fill()MatrixNdarray.from_numpy()MatrixNdarray.get_type()MatrixNdarray.to_numpy()MeshMesh.generate_meta()Mesh.load_meta()MeshInstanceMeshInstance.add_mesh_attribute()MeshInstance.get_position_as_numpy()MeshInstance.get_relation_access()MeshInstance.get_relation_size()MeshInstance.set_index_mapping()MeshInstance.set_num_patches()MeshInstance.set_owned_offset()MeshInstance.set_patch_max_element_num()MeshInstance.set_relation_dynamic()MeshInstance.set_relation_fixed()MeshInstance.set_total_offset()NdarrayNdarray.copy_from()Ndarray.fill()Ndarray.get_type()SNodeSNode.bitmasked()SNode.deactivate_all()SNode.dense()SNode.dynamic()SNode.lazy_dual()SNode.lazy_grad()SNode.parent()SNode.place()SNode.pointer()SNode.quant_array()ScalarFieldScalarField.copy_from()ScalarField.fill()ScalarField.from_numpy()ScalarField.from_paddle()ScalarField.from_torch()ScalarField.parent()ScalarField.to_numpy()ScalarField.to_paddle()ScalarField.to_torch()ScalarNdarrayScalarNdarray.copy_from()ScalarNdarray.fill()ScalarNdarray.from_numpy()ScalarNdarray.get_type()ScalarNdarray.to_numpy()StructStruct.field()Struct.to_dict()StructFieldStructField.copy_from()StructField.fill()StructField.from_numpy()StructField.from_paddle()StructField.from_torch()StructField.get_member_field()StructField.parent()StructField.to_numpy()StructField.to_paddle()StructField.to_torch()TRACETaichiAssertionErrorTaichiAssertionError.argsTaichiAssertionError.with_traceback()TaichiCompilationErrorTaichiCompilationError.argsTaichiCompilationError.with_traceback()TaichiNameErrorTaichiNameError.argsTaichiNameError.with_traceback()TaichiRuntimeErrorTaichiRuntimeError.argsTaichiRuntimeError.with_traceback()TaichiRuntimeTypeErrorTaichiRuntimeTypeError.argsTaichiRuntimeTypeError.get()TaichiRuntimeTypeError.with_traceback()TaichiSyntaxErrorTaichiSyntaxError.argsTaichiSyntaxError.filenameTaichiSyntaxError.linenoTaichiSyntaxError.msgTaichiSyntaxError.offsetTaichiSyntaxError.print_file_and_lineTaichiSyntaxError.textTaichiSyntaxError.with_traceback()TaichiTypeErrorTaichiTypeError.argsTaichiTypeError.with_traceback()TextureTexture.from_field()Texture.from_image()Texture.from_ndarray()Texture.to_image()VectorVector.all()Vector.any()Vector.cast()Vector.cols()Vector.cross()Vector.determinant()Vector.diag()Vector.dot()Vector.field()Vector.fill()Vector.get_shape()Vector.identity()Vector.inverse()Vector.max()Vector.min()Vector.ndarray()Vector.norm()Vector.norm_inv()Vector.norm_sqr()Vector.normalized()Vector.one()Vector.outer_product()Vector.rows()Vector.sum()Vector.to_list()Vector.to_numpy()Vector.trace()Vector.transpose()Vector.unit()Vector.zero()VectorNdarrayVectorNdarray.copy_from()VectorNdarray.fill()VectorNdarray.from_numpy()VectorNdarray.get_type()VectorNdarray.to_numpy()WARNabs()acos()activate()amdgpuappend()arm64asin()assume_in_range()atan2()atomic_add()atomic_and()atomic_max()atomic_min()atomic_or()atomic_sub()atomic_xor()axes()bit_cast()bit_shr()block_local()cache_read_only()cast()ccceil()cos()cpucudadata_oriented()dataclass()deactivate()deactivate_all_snodes()dx11dx12eig()exp()extensionf16f32f64field()float16float32float64floor()frexp()func()get_addr()glesglobal_thread_idx()gpugrouped()hex_to_rgb()ii16i32i64i8ijijkijklijlikiklilinit()int16int32int64int8is_active()is_logging_effective()jjkjkljlkkernel()klllength()log()loop_config()max()mesh_local()mesh_patch_idx()metalmin()ndarray()ndrange()no_activate()one()openglpolar_decompose()pow()pyfunc()randn()random()raw_div()raw_mod()ref()rescale_index()reset()rgb_to_hex()rootround()rsqrt()select()set_logging_level()sin()solve()sparse_matrix_buildersqrt()static()static_assert()static_print()stop_grad()svd()sym_eig()sync()tan()tanh()templateu16u32u64u8uint16uint32uint64uint8vulkanwarn_restricted_version()x64x86_64zero()BitpackedFieldsplaceCRITICALDEBUGDeviceCapabilityspirv_has_atomic_floatspirv_has_atomic_float16spirv_has_atomic_float16_addspirv_has_atomic_float16_minmaxspirv_has_atomic_float64spirv_has_atomic_float64_addspirv_has_atomic_float64_minmaxspirv_has_atomic_float_addspirv_has_atomic_float_minmaxspirv_has_atomic_int64spirv_has_float16spirv_has_float64spirv_has_int16spirv_has_int64spirv_has_int8spirv_has_no_integer_wrap_decorationspirv_has_non_semantic_infospirv_has_physical_storage_bufferspirv_has_subgroup_arithmeticspirv_has_subgroup_ballotspirv_has_subgroup_basicspirv_has_subgroup_votespirv_has_variable_ptrspirv_version_1_3spirv_version_1_4spirv_version_1_5ERRORFieldcopy_fromfrom_paddlefrom_torchparentFieldsBuilderbitmaskeddeactivate_alldensedynamicfinalizelazy_duallazy_gradplacepointerquant_arrayFormatGUIEventEventFilterWidgetValueALTBACKSPACECAPSLOCKCTRLDOWNESCAPEEXITLEFTLMBMMBMOTIONMOVEPRESSRELEASERETURNRIGHTRMBSHIFTSPACETABUPWHEELarrowarrow_fieldarrowsbuttoncirclecirclesclearclosecontourcook_imageget_bool_environget_cursor_posget_eventget_eventsget_imageget_key_eventhas_key_eventis_pressedlabellinelinespoint_fieldrectset_imageshowslidertexttriangletrianglesvector_fieldINFOLayoutMatrixallanycastcolscrossdeterminantdiagdotfieldfillget_shapeidentityinversemaxminndarraynormnorm_invnorm_sqrnormalizedoneouter_productrowssumto_listto_numpytracetransposeunitzeroMatrixFieldfillfrom_numpyget_scalar_fieldto_numpyto_paddleto_torchMatrixNdarraycopy_fromfillfrom_numpyget_typeto_numpyMeshgenerate_metaload_metaMeshInstanceadd_mesh_attributeget_position_as_numpyget_relation_accessget_relation_sizeset_index_mappingset_num_patchesset_owned_offsetset_patch_max_element_numset_relation_dynamicset_relation_fixedset_total_offsetNdarraycopy_fromfillget_typeSNodebitmaskeddeactivate_alldensedynamiclazy_duallazy_gradparentplacepointerquant_arrayScalarFieldcopy_fromfillfrom_numpyfrom_paddlefrom_torchparentto_numpyto_paddleto_torchScalarNdarraycopy_fromfillfrom_numpyget_typeto_numpyStructfieldto_dictStructFieldcopy_fromfillfrom_numpyfrom_paddlefrom_torchget_member_fieldparentto_numpyto_paddleto_torchTRACETaichiAssertionErrorargswith_tracebackTaichiCompilationErrorargswith_tracebackTaichiNameErrorargswith_tracebackTaichiRuntimeErrorargswith_tracebackTaichiRuntimeTypeErrorargsgetwith_tracebackTaichiSyntaxErrorargsfilenamelinenomsgoffsetprint_file_and_linetextwith_tracebackTaichiTypeErrorargswith_tracebackTexturefrom_fieldfrom_imagefrom_ndarrayto_imageVectorallanycastcolscrossdeterminantdiagdotfieldfillget_shapeidentityinversemaxminndarraynormnorm_invnorm_sqrnormalizedoneouter_productrowssumto_listto_numpytracetransposeunitzeroVectorNdarraycopy_fromfillfrom_numpyget_typeto_numpyWARNabsacosactivateamdgpuappendarm64asinassume_in_rangeatan2atomic_addatomic_andatomic_maxatomic_minatomic_oratomic_subatomic_xoraxesbit_castbit_shrblock_localcache_read_onlycastccceilcoscpucudadata_orienteddataclassdeactivatedeactivate_all_snodesdx11dx12eigexpextensionf16f32f64fieldfloat16float32float64floorfrexpfuncget_addrglesglobal_thread_idxgpugroupedhex_to_rgbii16i32i64i8ijijkijklijlikiklilinitint16int32int64int8is_activeis_logging_effectivejjkjkljlkkernelklllengthlogloop_configmaxmesh_localmesh_patch_idxmetalminndarrayndrangeno_activateoneopenglpolar_decomposepowpyfuncrandnrandomraw_divraw_modrefrescale_indexresetrgb_to_hexrootroundrsqrtselectset_logging_levelsinsolvesparse_matrix_buildersqrtstaticstatic_assertstatic_printstop_gradsvdsym_eigsynctantanhtemplateu16u32u64u8uint16uint32uint64uint8vulkanwarn_restricted_versionx64x86_64zeroCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
taichi.ad — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)On this page
taichi.ad#
class taichi.ad.FwdMode(loss, param, seed=None, clear_gradients=True)#
clear_seed(self)#
insert(self, func)#
recover_kernels(self)#
class taichi.ad.Tape(loss=None, clear_gradients=True, validation=False, grad_check=None)#
grad(self)#
insert(self, func, args)#
taichi.ad.clear_all_gradients(gradient_type=SNodeGradType.ADJOINT)#
Sets the gradients of all fields to zero.
taichi.ad.grad_for(primal)#
Generates a decorator to decorate primal’s customized gradient function.
See grad_replaced() for examples.
Parameters:
primal (Callable) – The primal function, must be decorated by grad_replaced().
Returns:
The decorator used to decorate customized gradient function.
Return type:
Callable
taichi.ad.grad_replaced(func)#
A decorator for python function to customize gradient with Taichi’s autodiff
system, e.g. ti.ad.Tape() and kernel.grad().
This decorator forces Taichi’s autodiff system to use a user-defined gradient
function for the decorated function. Its customized gradient must be decorated
by grad_for().
Parameters:
fn (Callable) – The python function to be decorated.
Returns:
The decorated function.
Return type:
Callable
Example:
>>> @ti.kernel
>>> def multiply(a: ti.float32):
>>>     for I in ti.grouped(x):
>>>         y[I] = x[I] * a
>>>
>>> @ti.kernel
>>> def multiply_grad(a: ti.float32):
>>>     for I in ti.grouped(x):
>>>         x.grad[I] = y.grad[I] / a
>>>
>>> @ti.ad.grad_replaced
>>> def foo(a):
>>>     multiply(a)
>>>
>>> @ti.ad.grad_for(foo)
>>> def foo_grad(a):
>>>     multiply_grad(a)
taichi.ad.no_grad(func)#
A decorator for python function to skip gradient calculation within Taichi’s
autodiff system, e.g. ti.ad.Tape() and kernel.grad().
This decorator forces Taichi’s autodiff system to use an empty gradient function
for the decorated function.
Parameters:
fn (Callable) – The python function to be decorated.
Returns:
The decorated function.
Return type:
Callable
Example:
>>> @ti.kernel
>>> def multiply(a: ti.float32):
>>>     for I in ti.grouped(x):
>>>         y[I] = x[I] * a
>>>
>>> @ti.no_grad
>>> def foo(a):
>>>     multiply(a)
Was this helpful?FwdModeFwdMode.clear_seed()FwdMode.insert()FwdMode.recover_kernels()TapeTape.grad()Tape.insert()clear_all_gradients()grad_for()grad_replaced()no_grad()FwdModeclear_seedinsertrecover_kernelsTapegradinsertclear_all_gradientsgrad_forgrad_replacedno_gradCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
taichi.aot — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)On this page
taichi.aot#
Taichi’s AOT (ahead of time) module.
Users can use Taichi as a GPU compute shader/kernel compiler by compiling their
Taichi kernels into an AOT module.
class taichi.aot.GfxRuntime140(metadata_json: Any, graphs_json: Any)#
static from_module(module_path: str) → GfxRuntime140#
to_graphs_json(self) → List[Any]#
to_metadata_json(self) → Any#
class taichi.aot.Module(arch=None, caps=None)#
An AOT module to save and load Taichi kernels.
This module serializes the Taichi kernels for a specific arch. The
serialized module can later be loaded to run on that backend, without the
Python environment.
Example
Usage:
m = ti.aot.Module(ti.metal)
m.add_kernel(foo)
m.add_kernel(bar)
m.save('/path/to/module')
# Now the module file '/path/to/module' contains the Metal kernels
# for running ``foo`` and ``bar``.
add_field(self, name, field)#
Add a taichi field to the AOT module.
Parameters:
name – name of taichi field
field – taichi field
Example:
>>> a = ti.field(ti.f32, shape=(4,4))
>>> b = ti.field("something")
>>>
>>> m.add_field(a)
>>> m.add_field(b)
>>>
>>> # Must add in sequence
add_graph(self, name, graph)#
add_kernel(self, kernel_fn, template_args=None, name=None)#
Add a taichi kernel to the AOT module.
Parameters:
kernel_fn (Function) – the function decorated by taichi kernel.
template_args (Dict[str, Any]) – a dict where key is the template
parameter name, and value is the instantiating arg. Note that this
works for both template and for
:class:`~taichi.types.ndarray.
name (str) – Name to identify this kernel in the module. If not
provided, uses the built-in __name__ attribute of kernel_fn.
add_kernel_template(self, kernel_fn)#
Add a taichi kernel (with template parameters) to the AOT module.
Parameters:
kernel_fn (Function) – the function decorated by taichi kernel.
Example:
>>> @ti.kernel
>>> def bar_tmpl(a: ti.template()):
>>>   x = a
>>>   # or y = a
>>>   # do something with `x` or `y`
>>>
>>> m = ti.aot.Module(arch)
>>> with m.add_kernel_template(bar_tmpl) as kt:
>>>   kt.instantiate(a=x)
>>>   kt.instantiate(a=y)
>>>
>>> @ti.kernel
>>> def bar_tmpl_multiple_args(a: ti.template(), b: ti.template())
>>>   x = a
>>>   y = b
>>>   # do something with `x` and `y`
>>>
>>> with m.add_kernel_template(bar_tmpl) as kt:
>>>   kt.instantiate(a=x, b=y)
archive(self, filepath: str)#
Parameters:
filepath (str) – path to the stored archive of aot artifacts, MUST
end with .tcm.
save(self, filepath)#
Parameters:
filepath (str) – path to a folder to store aot files.
taichi.aot.export(f)#
taichi.aot.export_as(name: str, *, template_types: Dict[str, Any] | None = None)#
taichi.aot.start_recording(filename)#
Starts recording kernel information to a yml file.
Parameters:
filename (str) – output yml file.
Example:
>>> ti.aot.start_recording('record.yml')
>>> ti.init(arch=ti.cc)
>>> loss = ti.field(float, (), needs_grad=True)
>>> x = ti.field(float, 233, needs_grad=True)
>>>
>>> @ti.kernel
>>> def compute_loss():
>>>     for i in x:
>>>         loss[None] += x[i]**2
>>>
>>> @ti.kernel
>>> def do_some_works():
>>>     for i in x:
>>>         x[i] -= x.grad[i]
>>>
>>> with ti.ad.Tape(loss):
>>>     compute_loss()
>>> do_some_works()
taichi.aot.stop_recording()#
Stops recording kernel information.
This function should be called in pair with start_recording().
Was this helpful?GfxRuntime140GfxRuntime140.from_module()GfxRuntime140.to_graphs_json()GfxRuntime140.to_metadata_json()ModuleModule.add_field()Module.add_graph()Module.add_kernel()Module.add_kernel_template()Module.archive()Module.save()export()export_as()start_recording()stop_recording()GfxRuntime140from_moduleto_graphs_jsonto_metadata_jsonModuleadd_fieldadd_graphadd_kerneladd_kernel_templatearchivesaveexportexport_asstart_recordingstop_recordingCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
taichi.linalg — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)On this page
taichi.linalg#
Taichi support module for sparse matrix operations.
class taichi.linalg.CG(A, b, x0=None, max_iter=50, atol=1e-06)#
solve(self)#
class taichi.linalg.LinearOperator(matvec_kernel)#
matvec(self, x, Ax)#
class taichi.linalg.SparseMatrix(n=None, m=None, sm=None, dtype=f32, storage_format='col_major')#
Taichi’s Sparse Matrix class
A sparse matrix allows the programmer to solve a large linear system.
Parameters:
n (int) – the first dimension of a sparse matrix.
m (int) – the second dimension of a sparse matrix.
sm (SparseMatrix) – another sparse matrix that will be built from.
build_from_ndarray(self, ndarray)#
Build the sparse matrix from a ndarray.
Parameters:
ndarray (Union[ti.ndarray, ti.Vector.ndarray, ti.Matrix.ndarray]) – the ndarray to build the sparse matrix from.
Raises:
TaichiRuntimeError – If the input is not a ndarray or the length is not divisible by 3.
Example::>>> N = 5
>>> triplets = ti.Vector.ndarray(n=3, dtype=ti.f32, shape=10, layout=ti.Layout.AOS)
>>> @ti.kernel
>>> def fill(triplets: ti.types.ndarray()):
>>>     for i in range(N):
>>>        triplets[i] = ti.Vector([i, (i + 1) % N, i+1], dt=ti.f32)
>>> fill(triplets)
>>> A = ti.linalg.SparseMatrix(n=N, m=N, dtype=ti.f32)
>>> A.build_from_ndarray(triplets)
>>> print(A)
[0, 1, 0, 0, 0]
[0, 0, 2, 0, 0]
[0, 0, 0, 3, 0]
[0, 0, 0, 0, 4]
[5, 0, 0, 0, 0]
mmwrite(self, filename)#
Writes the sparse matrix to Matrix Market file-like target.
Parameters:
filename (str) – the file name to write the sparse matrix to.
transpose(self)#
Sparse Matrix transpose.
Returns:
The transposed sparse mastrix.
class taichi.linalg.SparseMatrixBuilder(num_rows=None, num_cols=None, max_num_triplets=0, dtype=f32, storage_format='col_major')#
A python wrap around sparse matrix builder.
Use this builder to fill the sparse matrix.
Parameters:
num_rows (int) – the first dimension of a sparse matrix.
num_cols (int) – the second dimension of a sparse matrix.
max_num_triplets (int) – the maximum number of triplets.
dtype (ti.dtype) – the data type of the sparse matrix.
storage_format (str) – the storage format of the sparse matrix.
build(self, dtype=f32, _format='CSR')#
Create a sparse matrix using the triplets
print_triplets(self)#
Print the triplets stored in the builder
class taichi.linalg.SparseSolver(dtype=f32, solver_type='LLT', ordering='AMD')#
Sparse linear system solver
Use this class to solve linear systems represented by sparse matrices.
Parameters:
solver_type (str) – The factorization type.
ordering (str) – The method for matrices re-ordering.
analyze_pattern(self, sparse_matrix)#
Reorder the nonzero elements of the matrix, such that the factorization step creates less fill-in.
Parameters:
sparse_matrix (SparseMatrix) – The sparse matrix to be analyzed.
compute(self, sparse_matrix)#
This method is equivalent to calling both analyze_pattern and then factorize.
Parameters:
sparse_matrix (SparseMatrix) – The sparse matrix to be computed.
factorize(self, sparse_matrix)#
Do the factorization step
Parameters:
sparse_matrix (SparseMatrix) – The sparse matrix to be factorized.
info(self)#
Check if the linear systems are solved successfully.
Returns:
True if the solving process succeeded, False otherwise.
Return type:
bool
solve(self, b)#
Computes the solution of the linear systems.
:param b: The right-hand side of the linear systems.
:type b: numpy.array or Field
Returns:
The solution of linear systems.
Return type:
numpy.array
exception taichi.linalg.TaichiRuntimeError#
Bases: RuntimeError
Thrown when the compiled program cannot be executed due to unspecified reasons.
class args#
with_traceback()#
Exception.with_traceback(tb) –
set self.__traceback__ to tb and return self.
exception taichi.linalg.TaichiTypeError#
Bases: TaichiCompilationError, TypeError
Thrown when a type mismatch is found during compilation.
class args#
with_traceback()#
Exception.with_traceback(tb) –
set self.__traceback__ to tb and return self.
taichi.linalg.taichi_cg_solver(A, b, x, tol=1e-06, maxiter=5000, quiet=True)#
Was this helpful?CGCG.solve()LinearOperatorLinearOperator.matvec()SparseMatrixSparseMatrix.build_from_ndarray()SparseMatrix.mmwrite()SparseMatrix.transpose()SparseMatrixBuilderSparseMatrixBuilder.build()SparseMatrixBuilder.print_triplets()SparseSolverSparseSolver.analyze_pattern()SparseSolver.compute()SparseSolver.factorize()SparseSolver.info()SparseSolver.solve()TaichiRuntimeErrorTaichiRuntimeError.argsTaichiRuntimeError.with_traceback()TaichiTypeErrorTaichiTypeError.argsTaichiTypeError.with_traceback()taichi_cg_solver()CGsolveLinearOperatormatvecSparseMatrixbuild_from_ndarraymmwritetransposeSparseMatrixBuilderbuildprint_tripletsSparseSolveranalyze_patterncomputefactorizeinfosolveTaichiRuntimeErrorargswith_tracebackTaichiTypeErrorargswith_tracebacktaichi_cg_solverCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
taichi.math — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)On this page
taichi.math#
Taichi math module.
The math module supports glsl-style vectors, matrices and functions.
taichi.math.acos(x)#
Trigonometric inverse cosine, element-wise.
The inverse of cos so that, if y = cos(x), then x = acos(y).
For input x not in the domain [-1, 1], this function returns nan if         it’s called in taichi scope, or raises exception if it’s called in python scope.
Parameters:
x (Union[primitive_types, Matrix]) – A scalar or a matrix with elements in [-1, 1].
Returns:
The inverse cosine of each element in x, in radians and in the closed             interval [0, pi]. This is a scalar if x is a scalar.
Example:
>>> from math import pi
>>> ti.acos(ti.Matrix([-1.0, 0.0, 1.0])) * 180 / pi
[180., 90., 0.]
taichi.math.asin(x)#
Trigonometric inverse sine, element-wise.
The inverse of sin so that, if y = sin(x), then x = asin(y).
For input x not in the domain [-1, 1], this function returns nan if         it’s called in taichi scope, or raises exception if it’s called in python scope.
Parameters:
x (Union[primitive_types, Matrix]) – A scalar or a matrix with elements in [-1, 1].
Returns:
The inverse sine of each element in x, in radians and in the closed             interval [-pi/2, pi/2].
Example:
>>> from math import pi
>>> ti.asin(ti.Matrix([-1.0, 0.0, 1.0])) * 180 / pi
[-90., 0., 90.]
taichi.math.atan2(x1, x2)#
Element-wise arc tangent of x1/x2.
Parameters:
x1 (Union[primitive_types, Matrix]) – y-coordinates.
x2 (Union[primitive_types, Matrix]) – x-coordinates.
Returns:
Angles in radians, in the range [-pi, pi].
This is a scalar if both x1 and x2 are scalars.
Example:
>>> from math import pi
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-1.0, 1.0, -1.0, 1.0])
>>>     y = ti.Matrix([-1.0, -1.0, 1.0, 1.0])
>>>     z = ti.atan2(y, x) * 180 / pi
>>>     print(z)
>>>
>>> test()
[-135.0, -45.0, 135.0, 45.0]
taichi.math.cconj(z)#
Returns the complex conjugate of a 2d vector.
If z=(x, y) then the conjugate of z is (x, -y).
Parameters:
z (vec2) – The input.
Returns:
The complex conjugate of z.
Return type:
vec2
taichi.math.cdiv(z1, z2)#
Performs complex division between two 2d vectors.
This is equivalent to the division in the complex number field
when z1 and z2 are treated as complex numbers.
Parameters:
z1 (vec2) – The first input.
z2 (vec2) – The second input.
Example:
>>> @ti.kernel
>>> def test():
>>>     z1 = ti.math.vec2(1, 1)
>>>     z2 = ti.math.vec2(0, 1)
>>>     ti.math.cdiv(z1, z2)  # [1, -1]
Returns:
the complex division of z1 / z2.
Return type:
vec2
taichi.math.ceil(x, dtype=None)#
Return the ceiling of the input, element-wise.
The ceil of the scalar x is the smallest integer k, such that k >= x.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
dtype – (primitive_types): the returned type, default to None. If             set to None the retuned value will have the same type with x.
Returns:
The ceiling of each element in x, with return value type dtype.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([3.14, -1.5])
>>>     y = ti.ceil(x)
>>>     print(y)  # [4.0, -1.0]
taichi.math.cexp(z)#
Returns the complex exponential \(e^z\).
z is a 2d vector treated as a complex number.
Parameters:
z (vec2) – The exponent.
Example:
>>> @ti.kernel
>>> def test():
>>>     z = ti.math.vec2(1, 1)
>>>     w = ti.math.cexp(z)  # [1.468694, 2.287355]
Returns:
The power \(exp(z)\)
Return type:
vec2
taichi.math.cinv(z)#
Computes the reciprocal of a complex z.
Parameters:
z (vec2) – The input.
Example:
>>> @ti.kernel
>>> def test():
>>>     z = ti.math.vec2(1, 1)
>>>     w = ti.math.cinv(z)  # [0.5, -0.5]
Returns:
The reciprocal of z.
Return type:
vec2
taichi.math.clamp(x, xmin, xmax)#
Constrain a value to lie between two further values, element-wise.
The returned value is computed as min(max(x, xmin), xmax).
The arguments can be scalars or Matrix,
as long as they can be broadcasted to a common shape.
Parameters:
x (primitive_types, Matrix) – Specify
the value to constrain.
y (primitive_types, Matrix) – Specify
the lower end of the range into which to constrain x.
a (primitive_types, Matrix) – Specify
the upper end of the range into which to constrain x.
Returns:
The value of x constrained to the range xmin to xmax.
Example:
>>> v = ti.Vector([0, 0.5, 1.0, 1.5])
>>> ti.math.clamp(v, 0.5, 1.0)
[0.500000, 0.500000, 1.000000, 1.000000]
>>> x = ti.Matrix([[0, 1], [-2, 2]], ti.f32)
>>> y = ti.Matrix([[1, 2], [1, 2]], ti.f32)
>>> ti.math.clamp(x, 0.5, y)
[[0.500000, 1.000000], [0.500000, 2.000000]]
taichi.math.clog(z)#
Returns the complex logarithm of z, so that if \(e^w = z\),
then \(log(z) = w\).
z is a 2d vector treated as a complex number. The argument of \(w\)
lies in the range (-pi, pi].
Parameters:
z (vec2) – The input.
Example:
>>> @ti.kernel
>>> def test():
>>>     z = ti.math.vec2(1, 1)
>>>     w = ti.math.clog(z)  # [0.346574, 0.785398]
Returns:
The logarithm of z.
Return type:
vec2
taichi.math.cmul(z1, z2)#
Performs complex multiplication between two 2d vectors.
This is equivalent to the multiplication in the complex number field
when z1 and z2 are treated as complex numbers.
Parameters:
z1 (vec2) – The first input.
z2 (vec2) – The second input.
Example:
>>> @ti.kernel
>>> def test():
>>>     z1 = ti.math.vec2(1, 1)
>>>     z2 = ti.math.vec2(0, 1)
>>>     ti.math.cmul(z1, z2)  # [-1, 1]
Returns:
the complex multiplication z1 * z2.
Return type:
vec2
taichi.math.cos(x)#
Trigonometric cosine, element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – Angle, in radians.
Returns:
The cosine of each element of x.
Example:
>>> from math import pi
>>> x = ti.Matrix([-pi, 0, pi/2.])
>>> ti.cos(x)
[-1., 1., 0.]
taichi.math.cpow(z, n)#
Computes the power of a complex z: \(z^a\).
Parameters:
z (vec2) – The base.
a (float) – The exponent.
Example:
>>> @ti.kernel
>>> def test():
>>>     z = ti.math.vec2(1, 1)
>>>     w = ti.math.cpow(z)  # [-2, 2]
Returns:
The power \(z^a\).
Return type:
vec2
taichi.math.cross(x, y)#
Calculate the cross product of two vectors.
The two input vectors must have the same dimension \(d <= 3\).
This function calls the cross method of Vector.
Parameters:
x (Matrix) – The first input vector.
y (Matrix) – The second input vector.
Returns:
The cross product of two vectors.
Example:
>>> x = ti.Vector([1., 0., 0.])
>>> y = ti.Vector([0., 1., 0.])
>>> ti.math.cross(x, y)
[0.000000, 0.000000, 1.000000]
taichi.math.csqrt(z)#
Returns the complex square root of a 2d vector z, so that
if w^2=z, then w = csqrt(z).
Among the two square roots of z, if their real parts are non-zero,
the one with positive real part is returned. If both their real parts
are zero, the one with non-negative imaginary part is returned.
Parameters:
z (vec2) – The input.
Example:
>>> @ti.kernel
>>> def test():
>>>     z = ti.math.vec2(-1, 0)
>>>     w = ti.math.csqrt(z)  # [0, 1]
Returns:
The complex square root.
Return type:
vec2
taichi.math.degrees(x)#
Convert x in radians to degrees, element-wise.
Parameters:
x (Matrix) – The input angle in radians.
Returns:
angle in degrees.
Example:
>>> x = ti.Vector([-pi/2, pi/2])
>>> ti.math.degrees(x)
[-90.000000, 90.000000]
taichi.math.determinant(m)#
Alias for taichi.Matrix.determinant().
taichi.math.distance(x, y)#
Calculate the distance between two points.
This function is equivalent to the distance function is GLSL.
Parameters:
x (primitive_types, Matrix) – The first input point.
y (primitive_types, Matrix) – The second input point.
Returns:
The distance between the two points.
Example:
>>> x = ti.Vector([0, 0, 0])
>>> y = ti.Vector([1, 1, 1])
>>> ti.math.distance(x, y)
1.732051
taichi.math.dot(x, y)#
Calculate the dot product of two vectors.
Parameters:
x (Matrix) – The first input vector.
y (Matrix) – The second input vector.
Returns:
The dot product of two vectors.
Example:
>>> x = ti.Vector([1., 1., 0.])
>>> y = ti.Vector([0., 1., 1.])
>>> ti.math.dot(x, y)
1.000000
taichi.math.exp(x)#
Compute the exponential of all elements in x, element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
Returns:
Element-wise exponential of x.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-1.0, 0.0, 1.0])
>>>     y = ti.exp(x)
>>>     print(y)
>>>
>>> test()
[0.367879, 1.000000, 2.718282]
taichi.math.eye(n: template())#
Returns the nxn identity matrix.
Alias for identity().
taichi.math.floor(x, dtype=None)#
Return the floor of the input, element-wise.
The floor of the scalar x is the largest integer k, such that k <= x.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
dtype – (primitive_types): the returned type, default to None. If             set to None the retuned value will have the same type with x.
Returns:
The floor of each element in x, with return value type dtype.
Example::>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-1.1, 2.2, 3.])
>>>     y = ti.floor(x, ti.f64)
>>>     print(y)  # [-2.000000000000, 2.000000000000, 3.000000000000]
taichi.math.fract(x)#
Compute the fractional part of the argument, element-wise.
It’s equivalent to x - ti.floor(x).
Parameters:
x (primitive_types, Matrix) – The
input value.
Returns:
The fractional part of x.
Example:
>>> x = ti.Vector([-1.2, -0.7, 0.3, 1.2])
>>> ti.math.fract(x)
[0.800000, 0.300000, 0.300000, 0.200000]
taichi.math.inverse(mat)#
Calculate the inverse of a matrix.
This function is equivalent to the inverse function in GLSL.
Parameters:
mat (taichi.Matrix) – The matrix of which to take the inverse.
Returns:
Inverse of the input matrix.
Example:
>>> m = ti.math.mat3([(1, 1, 0), (0, 1, 1), (0, 0, 1)])
>>> ti.math.inverse(m)
[[1.000000, -1.000000, 1.000000],
 [0.000000, 1.000000, -1.000000],
 [0.000000, 0.000000, 1.000000]]
taichi.math.isinf(x)#
Determines whether the parameter is positive or negative infinity, element-wise.
Parameters:
x (primitive_types, taichi.Matrix) – The input.
Example
>>> x = ti.math.vec4(inf, -inf, nan, 1)
>>> ti.math.isinf(x)
[1, 1, 0, 0]
Returns:
For each element i of the result, returns 1 if x[i] is posititve or negative floating point infinity and 0 otherwise.
taichi.math.isnan(x)#
Determines whether the parameter is a number, element-wise.
Parameters:
x (primitive_types, taichi.Matrix) – The input.
Example
>>> x = ti.math.vec4(nan, -nan, inf, 1)
>>> ti.math.isnan(x)
[1, 1, 0, 0]
Returns:
For each element i of the result, returns 1 if x[i] is posititve or negative floating point NaN (Not a Number) and 0 otherwise.
taichi.math.ivec2#
2D signed int vector type.
taichi.math.ivec3#
3D signed int vector type.
taichi.math.ivec4#
3D signed int vector type.
taichi.math.length(x)#
Calculate the length of a vector.
This function is equivalent to the length function in GLSL.
:param x: The vector of which to calculate the length.
:type x: Matrix
Returns:
The Euclidean norm of the vector.
Example:
>>> x = ti.Vector([1, 1, 1])
>>> ti.math.length(x)
1.732051
taichi.math.log(x)#
Compute the natural logarithm, element-wise.
The natural logarithm log is the inverse of the exponential function,
so that log(exp(x)) = x. The natural logarithm is logarithm in base e.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
Returns:
The natural logarithm of x, element-wise.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Vector([-1.0, 0.0, 1.0])
>>>     y = ti.log(x)
>>>     print(y)
>>>
>>> test()
[-nan, -inf, 0.000000]
taichi.math.log2(x)#
Return the base 2 logarithm of x, so that if \(2^y=x\),
then \(y=\log2(x)\).
This is equivalent to the log2 function is GLSL.
Parameters:
x (Matrix) – The input value.
Returns:
The base 2 logarithm of x.
Example:
>>> x = ti.Vector([1., 2., 3.])
>>> ti.math.log2(x)
[0.000000, 1.000000, 1.584962]
taichi.math.mat2#
2x2 floating matrix type.
taichi.math.mat3#
3x3 floating matrix type.
taichi.math.mat4#
4x4 floating matrix type.
taichi.math.max(*args)#
Compute the maximum of the arguments, element-wise.
This function takes no effect on a single argument, even it’s array-like.
When there are both scalar and matrix arguments in args, the matrices
must have the same shape, and scalars will be broadcasted to the same shape as the matrix.
Parameters:
args – (List[primitive_types, Matrix]):             The input.
Returns:
Maximum of the inputs.
Example:
>>> @ti.kernel
>>> def foo():
>>>     x = ti.Vector([0, 1, 2])
>>>     y = ti.Vector([3, 4, 5])
>>>     z = ti.max(x, y, 4)
>>>     print(z)  # [4, 4, 5]
taichi.math.min(*args)#
Compute the minimum of the arguments, element-wise.
This function takes no effect on a single argument, even it’s array-like.
When there are both scalar and matrix arguments in args, the matrices
must have the same shape, and scalars will be broadcasted to the same shape as the matrix.
Parameters:
args – (List[primitive_types, Matrix]):             The input.
Returns:
Minimum of the inputs.
Example:
>>> @ti.kernel
>>> def foo():
>>>     x = ti.Vector([0, 1, 2])
>>>     y = ti.Vector([3, 4, 5])
>>>     z = ti.min(x, y, 1)
>>>     print(z)  # [0, 1, 1]
taichi.math.mix(x, y, a)#
Performs a linear interpolation between x and y using
a to weight between them. The return value is computed as
x * (1 - a) + a * y.
The arguments can be scalars or Matrix,
as long as the operation can be performed.
This function is similar to the mix function in GLSL.
Parameters:
x (primitive_types, Matrix) – Specify
the start of the range in which to interpolate.
y (primitive_types, Matrix) – Specify
the end of the range in which to interpolate.
a (primitive_types, Matrix) – Specify
the weight to use to interpolate between x and y.
Returns:
The linearinterpolation of x and y by weight a.
Return type:
(primitive_types, Matrix)
Example:
>>> x = ti.Vector([1, 1, 1])
>>> y = ti.Vector([2, 2, 2])
>>> a = ti.Vector([1, 0, 0])
>>> ti.math.mix(x, y, a)
[2.000000, 1.000000, 1.000000]
>>> x = ti.Matrix([[1, 2], [2, 3]], ti.f32)
>>> y = ti.Matrix([[3, 5], [4, 5]], ti.f32)
>>> a = 0.5
>>> ti.math.mix(x, y, a)
[[2.000000, 3.500000], [3.000000, 4.000000]]
taichi.math.mod(x, y)#
Compute value of one parameter modulo another, element-wise.
Parameters:
x (primitive_types, Matrix) – The first input.
y (primitive_types, Matrix) – The second input.
Returns:
the value of x modulo y. This is computed as x - y * floor(x/y).
Example:
>>> x = ti.Vector([-0.5, 0.5, 1.])
>>> y = 1.0
>>> ti.math.mod(x, y)
[0.500000, 0.500000, 0.000000]
taichi.math.normalize(v)#
Calculates the unit vector in the same direction as the
original vector v.
It’s equivalent to the normalize function is GLSL.
Parameters:
x (Matrix) – The vector to normalize.
Returns:
The normalized vector \(v/|v|\).
Example:
>>> v = ti.Vector([1, 2, 3])
>>> ti.math.normalize(v)
[0.267261, 0.534522, 0.801784]
taichi.math.popcnt(x)#
taichi.math.pow(base, exponent)#
First array elements raised to second array elements \({base}^{exponent}\), element-wise.
The result type of two scalar operands is determined as follows:
- If the exponent is an integral value, then the result type takes the type of the base.
- Otherwise, the result type follows
[Implicit type casting in binary operations](https://docs.taichi-lang.org/docs/type#implicit-type-casting-in-binary-operations).
With the above rules, an integral value raised to a negative integral value cannot have a
feasible type. Therefore, an exception will be raised if debug mode or optimization passes
are on; otherwise 1 will be returned.
In the following situations, the result is undefined:
- A negative value raised to a non-integral value.
- A zero value raised to a non-positive value.
Parameters:
base (Union[primitive_types, Matrix]) – The bases.
exponent (Union[primitive_types, Matrix]) – The exponents.
Returns:
base raised to exponent. This is a scalar if both base and exponent are scalars.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-2.0, 2.0])
>>>     y = -3
>>>     z = ti.pow(x, y)
>>>     print(z)
>>>
>>> test()
[-0.125000, 0.125000]
taichi.math.radians(x)#
Convert x in degrees to radians, element-wise.
Parameters:
x (Matrix) – The input angle in degrees.
Returns:
angle in radians.
Example:
>>> x = ti.Vector([-90., 45., 90.])
>>> ti.math.radians(x) / pi
[-0.500000, 0.250000, 0.500000]
taichi.math.reflect(x, n)#
Calculate the reflection direction for an incident vector.
For a given incident vector x and surface normal n this
function returns the reflection direction calculated as
\(x - 2.0 * dot(x, n) * n\).
This is equivalent to the reflect function is GLSL.
n should be normalized in order to achieve the desired result.
Parameters:
x (Matrix) – The incident vector.
n (Matrix) – The normal vector.
Returns:
The reflected vector.
Example:
>>> x = ti.Vector([1., 2., 3.])
>>> n = ti.Vector([0., 1., 0.])
>>> ti.math.reflect(x, n)
[1.000000, -2.000000, 3.000000]
taichi.math.refract(x, n, eta)#
Calculate the refraction direction for an incident vector.
This function is equivalent to the refract function in GLSL.
Parameters:
x (Matrix) – The incident vector.
n (Matrix) – The normal vector.
eta (float) – The ratio of indices of refraction.
Returns:
The refraction direction vector.
Return type:
Matrix
Example:
>>> x = ti.Vector([1., 1., 1.])
>>> y = ti.Vector([0, 1., 0])
>>> ti.math.refract(x, y, 2.0)
[2.000000, -1.000000, 2.000000]
taichi.math.rot_by_axis(axis, ang)#
Returns the 4x4 matrix representation of a 3d rotation with given axis axis and angle ang.
Parameters:
axis (vec3) – rotation axis
ang (float) – angle in radians unit
Returns:
rotation matrix
Return type:
mat4
taichi.math.rot_yaw_pitch_roll(yaw, pitch, roll)#
Returns a 4x4 homogeneous rotation matrix representing the 3d rotation with Euler angles (rotate with Y axis first, X axis second, Z axis third).
Parameters:
yaw (float) – yaw angle in radians unit
pitch (float) – pitch angle in radians unit
roll (float) – roll angle in radians unit
Returns:
rotation matrix
Return type:
mat4
taichi.math.rotation2d(ang)#
Returns the matrix representation of a 2d counter-clockwise rotation,
given the angle of rotation.
Parameters:
ang (float) – Angle of rotation in radians.
Returns:
2x2 rotation matrix.
Return type:
mat2
Example:
>>>ti.math.rotation2d(ti.math.radians(30))
[[0.866025, -0.500000], [0.500000, 0.866025]]
taichi.math.rotation3d(ang_x, ang_y, ang_z)#
Returns a 4x4 homogeneous rotation matrix representing the 3d rotation with Euler angles (rotate with Y axis first, X axis second, Z axis third).
Parameters:
ang_x (float) – angle in radians unit around X axis
ang_y (float) – angle in radians unit around Y axis
ang_z (float) – angle in radians unit around Z axis
Returns:
rotation matrix
Return type:
mat4
Example
>>> ti.math.rotation3d(0.52, -0.785, 1.046)
[[ 0.05048351 -0.61339645 -0.78816002  0.        ]
[ 0.65833154  0.61388511 -0.4355969   0.        ]
[ 0.75103329 -0.49688014  0.4348093   0.        ]
[ 0.          0.          0.          1.        ]]
taichi.math.round(x, dtype=None)#
Round to the nearest integer, element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – A scalar or a matrix.
dtype – (primitive_types): the returned type, default to None. If             set to None the retuned value will have the same type with x.
Returns:
The nearest integer of x, with return value type dtype.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Vector([-1.5, 1.2, 2.7])
>>>     print(ti.round(x))
[-2., 1., 3.]
taichi.math.scale(sx, sy, sz)#
Constructs a scale Matrix with shape (4, 4).
Parameters:
sx (float) – scale x.
sy (float) – scale y.
sz (float) – scale z.
Returns:
scale matrix.
Return type:
mat4
Example:
>>> ti.math.scale(1, 2, 3)
[[ 1. 0. 0. 0.]
 [ 0. 2. 0. 0.]
 [ 0. 0. 3. 0.]
 [ 0. 0. 0. 1.]]
taichi.math.sign(x)#
Extract the sign of the parameter, element-wise.
Parameters:
x (primitive_types, Matrix) – The
input value.
Returns:
-1.0 if x is less than 0.0, 0.0 if x is equal to 0.0,
and +1.0 if x is greater than 0.0.
Example:
>>> x = ti.Vector([-1.0, 0.0, 1.0])
>>> ti.math.sign(x)
[-1.000000, 0.000000, 1.000000]
taichi.math.sin(x)#
Trigonometric sine, element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – Angle, in radians.
Returns:
The sine of each element of x.
Example:
>>> from math import pi
>>> x = ti.Matrix([-pi/2., 0, pi/2.])
>>> ti.sin(x)
[-1., 0., 1.]
taichi.math.smoothstep(edge0, edge1, x)#
Performs smooth Hermite interpolation between 0 and 1 when
edge0 < x < edge1, element-wise.
The arguments can be scalars or Matrix,
as long as they can be broadcasted to a common shape.
This function is equivalent to the smoothstep in GLSL.
Parameters:
edge0 (primitive_types, Matrix) – Specifies
the value of the lower edge of the Hermite function.
edge1 (primitive_types, Matrix) – Specifies
the value of the upper edge of the Hermite function.
x (primitive_types, Matrix) – Specifies
the source value for interpolation.
Returns:
The smoothly interpolated value.
Example:
>>> edge0 = ti.Vector([0, 1, 2])
>>> edge1 = 1
>>> x = ti.Vector([0.5, 1.5, 2.5])
>>> ti.math.smoothstep(edge0, edge1, x)
[0.500000, 1.000000, 0.000000]
taichi.math.sqrt(x)#
Return the non-negative square-root of a scalar or a matrix,
element wise. If x < 0 an exception is raised.
Parameters:
x (Union[primitive_types, Matrix]) – The scalar or matrix whose square-roots are required.
Returns:
The square-root y so that y >= 0 and y^2 = x. y has the same type as x.
Example:
>>> x = ti.Matrix([1., 4., 9.])
>>> y = ti.sqrt(x)
>>> y
[1.0, 2.0, 3.0]
taichi.math.step(edge, x)#
Generate a step function by comparing two values, element-wise.
step generates a step function by comparing x to edge.
For element i of the return value, 0.0 is returned if x[i] < edge[i],
and 1.0 is returned otherwise.
The two arguments can be scalars or Matrix,
as long as they can be broadcasted to a common shape.
Parameters:
edge (primitive_types, Matrix) – Specify
the location of the edge of the step function.
x (primitive_types, Matrix) – Specify
the value to be used to generate the step function.
Returns:
The return value is computed as x >= edge, with type promoted.
Example:
>>> x = ti.Matrix([[0, 1], [2, 3]], ti.f32)
>>> y = 1
>>> ti.math.step(x, y)
[[1.000000, 1.000000], [0.000000, 0.000000]]
taichi.math.tan(x)#
Trigonometric tangent function, element-wise.
Equivalent to ti.sin(x)/ti.cos(x) element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
Returns:
The tangent values of x.
Example:
>>> from math import pi
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-pi, pi/2, pi])
>>>     y = ti.tan(x)
>>>     print(y)
>>>
>>> test()
[-0.0, -22877334.0, 0.0]
taichi.math.tanh(x)#
Compute the hyperbolic tangent of x, element-wise.
Parameters:
x (Union[primitive_types, Matrix]) – Input scalar or matrix.
Returns:
The corresponding hyperbolic tangent values.
Example:
>>> @ti.kernel
>>> def test():
>>>     x = ti.Matrix([-1.0, 0.0, 1.0])
>>>     y = ti.tanh(x)
>>>     print(y)
>>>
>>> test()
[-0.761594, 0.000000, 0.761594]
taichi.math.translate(dx, dy, dz)#
Constructs a translation Matrix with shape (4, 4).
Parameters:
dx (float) – delta x.
dy (float) – delta y.
dz (float) – delta z.
Returns:
translation matrix.
Return type:
mat4
Example:
>>> ti.math.translate(1, 2, 3)
[[ 1. 0. 0. 1.]
 [ 0. 1. 0. 2.]
 [ 0. 0. 1. 3.]
 [ 0. 0. 0. 1.]]
taichi.math.uvec2#
2D unsigned int vector type.
taichi.math.uvec3#
3D unsigned int vector type.
taichi.math.uvec4#
4D unsigned int vector type.
taichi.math.vdir(ang)#
Returns the 2d unit vector with argument equals ang.
x (primitive_types): The input angle in radians.
Example
>>> x = pi / 2
>>> ti.math.vdir(x)
[0, 1]
Returns:
a 2d vector with argument equals ang.
taichi.math.vec2#
2D floating vector type.
taichi.math.vec3#
3D floating vector type.
taichi.math.vec4#
4D floating vector type.
Was this helpful?acos()asin()atan2()cconj()cdiv()ceil()cexp()cinv()clamp()clog()cmul()cos()cpow()cross()csqrt()degrees()determinant()distance()dot()exp()eye()floor()fract()inverse()isinf()isnan()ivec2ivec3ivec4length()log()log2()mat2mat3mat4max()min()mix()mod()normalize()popcnt()pow()radians()reflect()refract()rot_by_axis()rot_yaw_pitch_roll()rotation2d()rotation3d()round()scale()sign()sin()smoothstep()sqrt()step()tan()tanh()translate()uvec2uvec3uvec4vdir()vec2vec3vec4acosasinatan2cconjcdivceilcexpcinvclampclogcmulcoscpowcrosscsqrtdegreesdeterminantdistancedotexpeyefloorfractinverseisinfisnanivec2ivec3ivec4lengthloglog2mat2mat3mat4maxminmixmodnormalizepopcntpowradiansreflectrefractrot_by_axisrot_yaw_pitch_rollrotation2drotation3droundscalesignsinsmoothstepsqrtsteptantanhtranslateuvec2uvec3uvec4vdirvec2vec3vec4Copyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
taichi.profiler — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)On this page
taichi.profiler#
class taichi.profiler.CuptiMetric(name='', header='unnamed_header', val_format='     {:8.0f} ', scale=1.0)#
A class to add CUPTI metric for KernelProfiler.
This class is designed to add user selected CUPTI metrics.
Only available for the CUDA backend now, i.e. you need ti.init(kernel_profiler=True, arch=ti.cuda).
For usage of this class, see examples in func set_kernel_profiler_metrics() and collect_kernel_profiler_metrics().
Parameters:
name (str) – name of metric that collected by CUPTI toolkit. used by set_kernel_profiler_metrics() and collect_kernel_profiler_metrics().
header (str) – column header of this metric, used by print_kernel_profiler_info().
val_format (str) – format for print metric value (and unit of this value), used by print_kernel_profiler_info().
scale (float) – scale of metric value, used by print_kernel_profiler_info().
Example:
>>> import taichi as ti
>>> ti.init(kernel_profiler=True, arch=ti.cuda)
>>> num_elements = 128*1024*1024
>>> x = ti.field(ti.f32, shape=num_elements)
>>> y = ti.field(ti.f32, shape=())
>>> y[None] = 0
>>> @ti.kernel
>>> def reduction():
>>>     for i in x:
>>>         y[None] += x[i]
>>> global_op_atom = ti.profiler.CuptiMetric(
>>>     name='l1tex__t_set_accesses_pipe_lsu_mem_global_op_atom.sum',
>>>     header=' global.atom ',
>>>     val_format='    {:8.0f} ')
>>> # add and set user defined metrics
>>> profiling_metrics = ti.profiler.get_predefined_cupti_metrics('global_access') + [global_op_atom]
>>> ti.profiler.set_kernel_profile_metrics(profiling_metrics)
>>> for i in range(16):
>>>     reduction()
>>> ti.profiler.print_kernel_profiler_info('trace')
Note
For details about using CUPTI in Taichi, please visit https://docs.taichi-lang.org/docs/profiler#advanced-mode.
taichi.profiler.clear_kernel_profiler_info()#
Clear all KernelProfiler records.
taichi.profiler.clear_scoped_profiler_info()#
Clear profiler’s records about time elapsed on the host tasks.
Call function imports from C++ : _ti_core.clear_profile_info()
taichi.profiler.collect_kernel_profiler_metrics(metric_list=default_cupti_metrics)#
Set temporary metrics that will be collected by the CUPTI toolkit within this context.
Parameters:
metric_list (list) – a list of CuptiMetric() instances, default value: default_cupti_metrics.
Example:
>>> import taichi as ti
>>> ti.init(kernel_profiler=True, arch=ti.cuda)
>>> ti.profiler.set_kernel_profiler_toolkit('cupti')
>>> num_elements = 128*1024*1024
>>> x = ti.field(ti.f32, shape=num_elements)
>>> y = ti.field(ti.f32, shape=())
>>> y[None] = 0
>>> @ti.kernel
>>> def reduction():
>>>     for i in x:
>>>         y[None] += x[i]
>>> # In the case of not parameter, Taichi will print its pre-defined metrics list
>>> ti.profiler.get_predefined_cupti_metrics()
>>> # get Taichi pre-defined metrics
>>> profiling_metrics = ti.profiler.get_predefined_cupti_metrics('device_utilization')
>>> global_op_atom = ti.profiler.CuptiMetric(
>>>     name='l1tex__t_set_accesses_pipe_lsu_mem_global_op_atom.sum',
>>>     header=' global.atom ',
>>>     format='    {:8.0f} ')
>>> # add user defined metrics
>>> profiling_metrics += [global_op_atom]
>>> # metrics setting is temporary, and will be clear when exit from this context.
>>> with ti.profiler.collect_kernel_profiler_metrics(profiling_metrics):
>>>     for i in range(16):
>>>         reduction()
>>>     ti.profiler.print_kernel_profiler_info('trace')
Note
The configuration of the metric_list will be clear when exit from this context.
taichi.profiler.get_kernel_profiler_total_time()#
Get elapsed time of all kernels recorded in KernelProfiler.
Returns:
total time in second.
Return type:
time (float)
taichi.profiler.get_predefined_cupti_metrics(name='')#
Returns the specified cupti metric.
Accepted arguments are ‘global_access’, ‘shared_access’, ‘atomic_access’,
‘cache_hit_rate’, ‘device_utilization’.
Parameters:
name (str) – cupti metri name.
taichi.profiler.print_kernel_profiler_info(mode='count')#
Print the profiling results of Taichi kernels.
To enable this profiler, set kernel_profiler=True in ti.init().
'count' mode: print the statistics (min,max,avg time) of launched kernels,
'trace' mode: print the records of launched kernels with specific profiling metrics (time, memory load/store and core utilization etc.),
and defaults to 'count'.
Parameters:
mode (str) – the way to print profiling results.
Example:
>>> import taichi as ti
>>> ti.init(ti.cpu, kernel_profiler=True)
>>> var = ti.field(ti.f32, shape=1)
>>> @ti.kernel
>>> def compute():
>>>     var[0] = 1.0
>>> compute()
>>> ti.profiler.print_kernel_profiler_info()
>>> # equivalent calls :
>>> # ti.profiler.print_kernel_profiler_info('count')
>>> ti.profiler.print_kernel_profiler_info('trace')
Note
Currently the result of KernelProfiler could be incorrect on OpenGL
backend due to its lack of support for ti.sync().
For advanced mode of KernelProfiler, please visit https://docs.taichi-lang.org/docs/profiler#advanced-mode.
taichi.profiler.print_memory_profiler_info()#
Memory profiling tool for LLVM backends with full sparse support.
This profiler is automatically on.
taichi.profiler.print_scoped_profiler_info()#
Print time elapsed on the host tasks in a hierarchical format.
This profiler is automatically on.
Call function imports from C++ : _ti_core.print_profile_info()
Example:
>>> import taichi as ti
>>> ti.init(arch=ti.cpu)
>>> var = ti.field(ti.f32, shape=1)
>>> @ti.kernel
>>> def compute():
>>>     var[0] = 1.0
>>>     print("Setting var[0] =", var[0])
>>> compute()
>>> ti.profiler.print_scoped_profiler_info()
taichi.profiler.query_kernel_profiler_info(name)#
Query kernel elapsed time(min,avg,max) on devices using the kernel name.
To enable this profiler, set kernel_profiler=True in ti.init.
Parameters:
name (str) – kernel name.
Returns:
with member variables(counter, min, max, avg)
Return type:
KernelProfilerQueryResult (class)
Example:
>>> import taichi as ti
>>> ti.init(ti.cpu, kernel_profiler=True)
>>> n = 1024*1024
>>> var = ti.field(ti.f32, shape=n)
>>> @ti.kernel
>>> def fill():
>>>     for i in range(n):
>>>         var[i] = 0.1
>>> fill()
>>> ti.profiler.clear_kernel_profiler_info() #[1]
>>> for i in range(100):
>>>     fill()
>>> query_result = ti.profiler.query_kernel_profiler_info(fill.__name__) #[2]
>>> print("kernel executed times =",query_result.counter)
>>> print("kernel elapsed time(min_in_ms) =",query_result.min)
>>> print("kernel elapsed time(max_in_ms) =",query_result.max)
>>> print("kernel elapsed time(avg_in_ms) =",query_result.avg)
Note
[1] To get the correct result, query_kernel_profiler_info() must be used in conjunction with
clear_kernel_profiler_info().
[2] Currently the result of KernelProfiler could be incorrect on OpenGL
backend due to its lack of support for ti.sync().
taichi.profiler.set_kernel_profiler_metrics(metric_list=default_cupti_metrics)#
Set metrics that will be collected by the CUPTI toolkit.
Parameters:
metric_list (list) – a list of CuptiMetric() instances, default value: default_cupti_metrics.
Example:
>>> import taichi as ti
>>> ti.init(kernel_profiler=True, arch=ti.cuda)
>>> ti.profiler.set_kernel_profiler_toolkit('cupti')
>>> num_elements = 128*1024*1024
>>> x = ti.field(ti.f32, shape=num_elements)
>>> y = ti.field(ti.f32, shape=())
>>> y[None] = 0
>>> @ti.kernel
>>> def reduction():
>>>     for i in x:
>>>         y[None] += x[i]
>>> # In the case of not parameter, Taichi will print its pre-defined metrics list
>>> ti.profiler.get_predefined_cupti_metrics()
>>> # get Taichi pre-defined metrics
>>> profiling_metrics = ti.profiler.get_predefined_cupti_metrics('shared_access')
>>> global_op_atom = ti.profiler.CuptiMetric(
>>>     name='l1tex__t_set_accesses_pipe_lsu_mem_global_op_atom.sum',
>>>     header=' global.atom ',
>>>     format='    {:8.0f} ')
>>> # add user defined metrics
>>> profiling_metrics += [global_op_atom]
>>> # metrics setting will be retained until the next configuration
>>> ti.profiler.set_kernel_profiler_metrics(profiling_metrics)
>>> for i in range(16):
>>>     reduction()
>>> ti.profiler.print_kernel_profiler_info('trace')
Note
Metrics setting will be retained until the next configuration.
taichi.profiler.set_kernel_profiler_toolkit(toolkit_name='default')#
Set the toolkit used by KernelProfiler.
Currently, we only support toolkits: 'default' and 'cupti'.
Parameters:
toolkit_name (str) – string of toolkit name.
Returns:
whether the setting is successful or not.
Return type:
status (bool)
Example:
>>> import taichi as ti
>>> ti.init(arch=ti.cuda, kernel_profiler=True)
>>> x = ti.field(ti.f32, shape=1024*1024)
>>> @ti.kernel
>>> def fill():
>>>     for i in x:
>>>         x[i] = i
>>> ti.profiler.set_kernel_profiler_toolkit('cupti')
>>> for i in range(100):
>>>     fill()
>>> ti.profiler.print_kernel_profiler_info()
>>> ti.profiler.set_kernel_profiler_toolkit('default')
>>> for i in range(100):
>>>     fill()
>>> ti.profiler.print_kernel_profiler_info()
Was this helpful?CuptiMetricclear_kernel_profiler_info()clear_scoped_profiler_info()collect_kernel_profiler_metrics()get_kernel_profiler_total_time()get_predefined_cupti_metrics()print_kernel_profiler_info()print_memory_profiler_info()print_scoped_profiler_info()query_kernel_profiler_info()set_kernel_profiler_metrics()set_kernel_profiler_toolkit()CuptiMetricclear_kernel_profiler_infoclear_scoped_profiler_infocollect_kernel_profiler_metricsget_kernel_profiler_total_timeget_predefined_cupti_metricsprint_kernel_profiler_infoprint_memory_profiler_infoprint_scoped_profiler_infoquery_kernel_profiler_infoset_kernel_profiler_metricsset_kernel_profiler_toolkitCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
taichi.tools — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)On this page
taichi.tools#
Taichi utility module.
image submodule for image io.
video submodule for exporting results to video files.
diagnose submodule for printing system environment information.
class taichi.tools.PLYWriter(num_vertices: int, num_faces=0, face_type='tri', comment='created by PLYWriter')#
Writes numpy.array data to ply files.
Parameters:
num_vertices (int) – number of vertices.
num_faces (int, optional) – number of faces.
face_type (str) – tri or quad.
comment (str) – comment message.
add_face_channel(self, key: str, data_type: str, data: numpy.array)#
add_face_id(self)#
add_face_piece(self, piece: numpy.array)#
add_faces(self, indices: numpy.array)#
add_vertex_alpha(self, alpha: numpy.array)#
Sets the alpha-channel (transparent) of the vertex colors.
Parameters:
alpha (numpy.array(float)) – the alpha-channel (transparent) of the colors.
add_vertex_channel(self, key: str, data_type: str, data: numpy.array)#
add_vertex_color(self, r: numpy.array, g: numpy.array, b: numpy.array)#
Sets the (r, g, b) channels of the colors at the vertices.
The three arguments are all numpy arrays of float type and have
the same length.
Parameters:
r (numpy.array(float)) – the r-channel (red) of the colors.
g (numpy.array(float)) – the g-channel (green) of the color.
b (numpy.array(float)) – the b-channel (blue) of the colors.
add_vertex_id(self)#
Sets the ids of the vertices.
The id of a vertex is equal to its index in the vertex array.
add_vertex_normal(self, nx: numpy.array, ny: numpy.array, nz: numpy.array)#
Add normal vectors at the vertices.
The three arguments are all numpy arrays of float type and have
the same length.
Parameters:
nx (numpy.array(float)) – x-coordinates of the normal vectors.
ny (numpy.array(float)) – y-coordinates of the normal vectors.
nz (numpy.array(float)) – z-coordinates of the normal vectors.
add_vertex_piece(self, piece: numpy.array)#
add_vertex_pos(self, x: numpy.array, y: numpy.array, z: numpy.array)#
Set the (x, y, z) coordinates of the vertices.
Parameters:
x (numpy.array(float)) – x-coordinates of the vertices.
y (numpy.array(float)) – y-coordinates of the vertices.
z (numpy.array(float)) – z-coordinates of the vertices.
add_vertex_rgba(self, r: numpy.array, g: numpy.array, b: numpy.array, a: numpy.array)#
Sets the (r, g, b, a) channels of the colors at the vertices.
Parameters:
r (numpy.array(float)) – the r-channel (red) of the colors.
g (numpy.array(float)) – the g-channel (green) of the color.
b (numpy.array(float)) – the b-channel (blue) of the colors.
a (numpy.array(float)) – the a-channel (alpha) of the colors.
add_vertex_vel(self, vx: numpy.array, vy: numpy.array, vz: numpy.array)#
Add velocity vectors at the vertices.
Parameters:
vx (numpy.array(float)) – x-coordinates of the velocity vectors.
vy (numpy.array(float)) – y-coordinates of the velocity vectors.
vz (numpy.array(float)) – z-coordinates of the velocity vectors.
export(self, path)#
export_ascii(self, path)#
export_frame(self, series_num: int, path: str)#
export_frame_ascii(self, series_num: int, path: str)#
print_header(self, path: str, _format: str)#
sanity_check(self)#
class taichi.tools.VideoManager(output_dir, video_filename=None, width=None, height=None, post_processor=None, framerate=24, automatic_build=True)#
Utility class for exporting results to mp4 and gif formats.
This class relies on ffmpeg.
Parameters:
output_dir (str) – directory to save the frames.
video_filename (str) – filename for the video. default filename is video.mp4.
width (int) – resolution of the video.
height (int) – resolution of the video.
post_processor (object) – any object that implements the process(img)
method, which accepts an image as a numpy.ndarray and returns
the process image.
framerate (int) – frame rate of the video.
automatic_build (bool) – automatically generate the resulting video or not.
Example:
>>> video_manager = ti.tools.VideoManager(output_dir="./output", framerate=24, automatic_build=False)
>>> for i in range(50):
>>>     render()
>>>     img = pixels.to_numpy()
>>>     video_manager.write_frame(img)
>>>
>>> video_manager.make_video(gif=True, mp4=True)
Returns:
An instance of taichi.tools.VideoManager class.
clean_frames(self)#
Delete all previous image files in the saved directory.
get_frame_directory(self)#
Returns path to the directory where the image files are located in.
get_output_filename(self, suffix)#
make_video(self, mp4=True, gif=True)#
Convert the image files to a mp4 or gif animation.
write_frame(self, img)#
Write an numpy.ndarray img to an image file.
The filename will be automatically determined by this manager
and the frame counter.
write_frames(self, images)#
Write a list of numpy.ndarray images to image files.
taichi.tools.imread(filename, channels=0)#
Load image from a specific file.
Parameters:
filename (str) – An image filename to load from.
channels (int, optional) – The channels hint of input image, Default to 0.
Returns:
An output image loaded from given filename.
Return type:
np.ndarray
taichi.tools.imresize(img, w, h=None)#
Resize an image to a specific size.
Parameters:
img (Union[ti.field, np.ndarray]) – A field of of array with shape (width, height, …)
w (int) – The output width after resize.
h (int, optional) – The output height after resize, will be the same as width if not set. Default to None.
Returns:
An output image after resize input.
Return type:
np.ndarray
taichi.tools.imshow(img, title='imshow')#
Display a taichi.field or a numpy.ndarray in a Taichi GUI window or an interactive Ipython notebook.
For an interactive Ipython environment, the image will be shown in the notebook.
Parameters:
img (Union[ti.field, np.ndarray]) – A field of of array with shape (width, height) or (height, width, 3) or (height, width, 4).
title (str, optional) – The title of GUI window. Default to imshow.
taichi.tools.imwrite(img, filename)#
Save a field to a a specific file.
Parameters:
img (Union[ti.field, np.ndarray]) – A field of shape (height, width) or (height, width, 3) or (height, width, 4),             if dtype is float-type (ti.f16, ti.f32, np.float32 etc), the value of each pixel should be float between [0.0, 1.0]. Otherwise ti.tools.imwrite will first clip them into [0.0, 1.0]                if dtype is int-type (ti.u8, ti.u16, np.uint8 etc), , the value of each pixel can be any valid integer in its own bounds. These integers in this field will be scaled to [0, 255] by being divided over the upper bound of its basic type accordingly.
filename (str) – The filename to save to.
taichi.tools.write_vtk(scalar_field, filename)#
Was this helpful?PLYWriterPLYWriter.add_face_channel()PLYWriter.add_face_id()PLYWriter.add_face_piece()PLYWriter.add_faces()PLYWriter.add_vertex_alpha()PLYWriter.add_vertex_channel()PLYWriter.add_vertex_color()PLYWriter.add_vertex_id()PLYWriter.add_vertex_normal()PLYWriter.add_vertex_piece()PLYWriter.add_vertex_pos()PLYWriter.add_vertex_rgba()PLYWriter.add_vertex_vel()PLYWriter.export()PLYWriter.export_ascii()PLYWriter.export_frame()PLYWriter.export_frame_ascii()PLYWriter.print_header()PLYWriter.sanity_check()VideoManagerVideoManager.clean_frames()VideoManager.get_frame_directory()VideoManager.get_output_filename()VideoManager.make_video()VideoManager.write_frame()VideoManager.write_frames()imread()imresize()imshow()imwrite()write_vtk()PLYWriteradd_face_channeladd_face_idadd_face_pieceadd_facesadd_vertex_alphaadd_vertex_channeladd_vertex_coloradd_vertex_idadd_vertex_normaladd_vertex_pieceadd_vertex_posadd_vertex_rgbaadd_vertex_velexportexport_asciiexport_frameexport_frame_asciiprint_headersanity_checkVideoManagerclean_framesget_frame_directoryget_output_filenamemake_videowrite_framewrite_framesimreadimresizeimshowimwritewrite_vtkCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
taichi.types — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)On this page
taichi.types#
This module defines data types in Taichi:
primitive: int, float, etc.
compound: matrix, vector, struct.
template: for reference types.
ndarray: for arbitrary arrays.
quant: for quantized types, see “https://yuanming.taichi.graphics/publication/2021-quantaichi/quantaichi.pdf”
taichi.types.f16#
Alias for float16
taichi.types.f32#
Alias for float32
taichi.types.f64#
Alias for float64
taichi.types.float16#
16-bit precision floating point data type.
taichi.types.float32#
32-bit single precision floating point data type.
taichi.types.float64#
64-bit double precision floating point data type.
taichi.types.i16#
Alias for int16
taichi.types.i32#
Alias for int32
taichi.types.i64#
Alias for int64
taichi.types.i8#
Alias for int8
taichi.types.int16#
16-bit signed integer data type.
taichi.types.int32#
32-bit signed integer data type.
taichi.types.int64#
64-bit signed integer data type.
taichi.types.int8#
8-bit signed integer data type.
taichi.types.is_integral#
taichi.types.is_real#
taichi.types.is_signed#
taichi.types.is_tensor#
taichi.types.matrix(n, m, dtype)#
Creates a matrix type with given shape and data type.
Parameters:
n (int) – number of rows of the matrix.
m (int) – number of columns of the matrix.
dtype (primitive_types) – matrix data type.
Returns:
A matrix type.
Example:
>>> mat2x2 = ti.types.matrix(2, 2, ti.f32)  # 2x2 matrix type
>>> M = mat2x2([[1., 2.], [3., 4.]])  # an instance of this type
taichi.types.ndarray#
Alias for NdarrayType.
Example:
>>> @ti.kernel
>>> def to_numpy(x: ti.types.ndarray(), y: ti.types.ndarray()):
>>>     for i in range(n):
>>>         x[i] = y[i]
>>>
>>> y = ti.ndarray(ti.f64, shape=n)
>>> ... # calculate y
>>> x = numpy.zeros(n)
>>> to_numpy(x, y)  # `x` will be filled with `y`'s data.
taichi.types.ref(tp)#
taichi.types.rw_texture#
Alias for TextureType.
class taichi.types.sparse_matrix_builder#
taichi.types.struct(**kwargs)#
Creates a struct type with given members.
Parameters:
kwargs (dict) – a dictionary contains the names and types of the
struct members.
Returns:
A struct type.
Example:
>>> vec3 = ti.types.vector(3, ti.f32)
>>> sphere = ti.types.struct(center=vec3, radius=float)
>>> s = sphere(center=vec3([0., 0., 0.]), radius=1.0)
taichi.types.template#
Alias for Template.
taichi.types.texture#
taichi.types.u16#
Alias for uint16
taichi.types.u32#
Alias for uint32
taichi.types.u64#
Alias for uint64
taichi.types.u8#
Alias for uint8
taichi.types.uint16#
16-bit unsigned integer data type.
taichi.types.uint32#
32-bit unsigned integer data type.
taichi.types.uint64#
64-bit unsigned integer data type.
taichi.types.uint8#
8-bit unsigned integer data type.
taichi.types.vector(n, dtype)#
Creates a vector type with given shape and data type.
Parameters:
n (int) – dimension of the vector.
dtype (primitive_types) – vector data type.
Returns:
A vector type.
Example:
>>> vec3 = ti.types.vector(3, ti.f32)  # 3d vector type
>>> v = vec3([1., 2., 3.])  # an instance of this type
Was this helpful?f16f32f64float16float32float64i16i32i64i8int16int32int64int8is_integralis_realis_signedis_tensormatrix()ndarrayref()rw_texturesparse_matrix_builderstruct()templatetextureu16u32u64u8uint16uint32uint64uint8vector()f16f32f64float16float32float64i16i32i64i8int16int32int64int8is_integralis_realis_signedis_tensormatrixndarrayrefrw_texturesparse_matrix_builderstructtemplatetextureu16u32u64u8uint16uint32uint64uint8vectorCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
taichi.types.quant — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)On this page
taichi.types.quant#
This module defines generators of quantized types.
For more details, read https://yuanming.taichi.graphics/publication/2021-quantaichi/quantaichi.pdf.
taichi.types.quant.fixed(bits, signed=True, max_value=1.0, compute=None, scale=None)#
Generates a quantized type for fixed-point real numbers.
Parameters:
bits (int) – Number of bits.
signed (bool) – Signed or unsigned.
max_value (float) – Maximum value of the number.
compute (DataType) – Type for computation.
scale (float) – Scaling factor. The argument is prioritized over range.
Returns:
The specified type.
Return type:
DataType
taichi.types.quant.float(exp, frac, signed=True, compute=None)#
Generates a quantized type for floating-point real numbers.
Parameters:
exp (int) – Number of exponent bits.
frac (int) – Number of fraction bits.
signed (bool) – Signed or unsigned.
compute (DataType) – Type for computation.
Returns:
The specified type.
Return type:
DataType
taichi.types.quant.int(bits, signed=True, compute=None)#
Generates a quantized type for integers.
Parameters:
bits (int) – Number of bits.
signed (bool) – Signed or unsigned.
compute (DataType) – Type for computation.
Returns:
The specified type.
Return type:
DataType
Was this helpful?fixed()float()int()fixedfloatintCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
taichi.ui — taichi-api-docstring  documentation
Skip to main contentStarDoc HomeAPIResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatv1.6.0developv1.6.0v1.5.0EnglishEnglish简体中文Français (France)Searchtaichi: The main taichi moduletaichi.ad: Taichi Autodiff systemtaichi.aot: Taichi AOT systemtaichi.linalg: Taichi Linear algebra librarytaichi.math: Taichi math librarytaichi.profiler: Taichi profiler toolstaichi.tools: Miscellaneous tools shipped with Taichitaichi.types: Taichi typestaichi.types.quant: Taichi quantized typestaichi.ui: Taichi UI components (GUI and GGUI)On this page
taichi.ui#
Taichi gui module for visualization.
This module contains a cpu based GUI system, a vulkan based GGUI system,
and other helper utilities like adding widgets and exporting video files.
taichi.ui.ALT = Alt#
taichi.ui.BACKSPACE = BackSpace#
taichi.ui.CAPSLOCK = CapsLock#
taichi.ui.CTRL = Control#
class taichi.ui.Camera#
The Camera class.
You should also manually set the camera parameters like camera.position,
camera.lookat, camera.up, etc. The default settings may not work for
your scene.
Example:
>>> scene = ti.ui.Scene()  # assume you have a scene
>>>
>>> camera = ti.ui.Camera()
>>> camera.position(1, 1, 1)  # set camera position
>>> camera.lookat(0, 0, 0)  # set camera lookat
>>> camera.up(0, 1, 0)  # set camera up vector
>>> scene.set_camera(camera)
>>>
>>> # you can also control camera movement in a window
>>> window = ti.ui.Window("GGUI Camera", res=(640, 480), vsync=True)
>>> camera.track_user_inputs(window, movement_speed=0.03, hold_key=ti.ui.RMB)
bottom(self, bottom)#
Set the offset of the bottom clipping plane in camera frustum.
Parameters:
bottom (taichi.types.primitive_types) – offset of the bottom clipping plane.
Example:
>>> camera.bottom(1.0)
fov(self, fov)#
Set the camera fov angle (field of view) in degrees.
Parameters:
fov (taichi.types.primitive_types) – Angle in range (0, 180).
Example:
>>> camera.fov(45)
get_projection_matrix(self, aspect)#
Get the projection matrix(in row major) of the camera.
Parameters:
aspect (taichi.types.primitive_types) – aspect ratio of the camera
Example:
>>> camera.get_projection_matrix(1080/720)
get_view_matrix(self)#
Get the view matrix(in row major) of the camera.
Example:
>>> camera.get_view_matrix()
left(self, left)#
Set the offset of the left clipping plane in camera frustum.
Parameters:
left (taichi.types.primitive_types) – offset of the left clipping plane.
Example:
>>> camera.left(-1.0)
lookat(self, x, y, z)#
Set the camera lookat.
Parameters:
args (taichi.types.primitive_types) – 3D coordinates.
Example:
>>> camera.lookat(0, 0, 0)
position(self, x, y, z)#
Set the camera position.
Parameters:
args (taichi.types.primitive_types) – 3D coordinates.
Example:
>>> camera.position(1, 1, 1)
projection_mode(self, mode)#
Camera projection mode, 0 for perspective and 1 for orthogonal.
right(self, right)#
Set the offset of the right clipping plane in camera frustum.
Parameters:
right (taichi.types.primitive_types) – offset of the right clipping plane.
Example:
>>> camera.right(1.0)
top(self, top)#
Set the offset of the top clipping plane in camera frustum.
Parameters:
top (taichi.types.primitive_types) – offset of the top clipping plane.
Example:
>>> camera.top(-1.0)
track_user_inputs(self, window, movement_speed: float = 1.0, yaw_speed: float = 2.0, pitch_speed: float = 2.0, hold_key=None)#
Move the camera according to user inputs.
Press w, s, a, d, e, q to move the camera
formard, back, left, right, head up, head down, accordingly.
Parameters:
window (Window) – a windown instance.
movement_speed (primitive_types) – camera movement speed.
yaw_speed (primitive_types) – speed of changes in yaw angle.
pitch_speed (primitive_types) – speed of changes in pitch angle.
hold_key (ui) – User defined key for holding the camera movement.
up(self, x, y, z)#
Set the camera up vector.
Parameters:
args (taichi.types.primitive_types) – 3D coordinates.
Example:
>>> camera.up(0, 1, 0)
z_far(self, z_far)#
Set the offset of the far clipping plane in camera frustum.
Parameters:
far (taichi.types.primitive_types) – offset of the far clipping plane.
Example:
>>> camera.left(1000.0)
z_near(self, z_near)#
Set the offset of the near clipping plane in camera frustum.
Parameters:
near (taichi.types.primitive_types) – offset of the near clipping plane.
Example:
>>> camera.near(0.1)
class taichi.ui.Canvas(canvas)#
The Canvas class.
This is the context manager for managing drawing commands on a window.
You should not instantiate this class directly via __init__, instead
please call the get_canvas() method of Window.
circles(self, centers, radius, color=(0.5, 0.5, 0.5), per_vertex_color=None)#
Draw a set of 2D circles on this canvas.
Parameters:
centers – a taichi 2D Vector field, where each element indicate the                 3D location of a vertex.
radius (Number) – radius of the circles in pixels.
color – a global color for the triangles as 3 floats representing                 RGB values. If per_vertex_color is provided, this is ignored.
per_vertex_color (Tuple[float]) – a taichi 3D vector field, where                 each element indicate the RGB color of a circle.
contour(self, scalar_field, cmap_name='plasma', normalize=False)#
Plot a contour view of a scalar field.
The input scalar_field will be converted to a Numpy array first, and then plotted
using Matplotlib’s colormap. Users can specify the color map through the cmap_name
argument.
Parameters:
scalar_field (ti.field) – The scalar field being plotted. Must be 2D.
cmap_name (str, Optional) – The name of the color map in Matplotlib.
normalize (bool, Optional) – Display the normalized scalar field if set to True.
False. (Default is) – 
lines(self, vertices, width, indices=None, color=(0.5, 0.5, 0.5), per_vertex_color=None)#
Draw a set of 2D lines on this canvas.
Parameters:
vertices – a taichi 2D Vector field, where each element indicate the                 3D location of a vertex.
width (float) – width of the lines, relative to the height of the screen.
indices – a taichi int field of shape (2 * #lines), which indicate                 the vertex indices of the lines. If this is None, then it is                 assumed that the vertices are already arranged in lines order.
color – a global color for the triangles as 3 floats representing                 RGB values. If per_vertex_color is provided, this is ignored.
per_vertex_color (tuple[float]) – a taichi 3D vector field, where                 each element indicate the RGB color of a vertex.
scene(self, scene)#
Draw a 3D scene on the canvas
Parameters:
scene (Scene) – an instance of Scene.
set_background_color(self, color)#
Set the background color of this canvas.
Parameters:
color (tuple(float)) – RGB triple in the range [0, 1].
set_image(self, img)#
Set the content of this canvas to an img.
Parameters:
img (numpy.ndarray, MatrixField, Field, Texture) – the image to be shown.
triangles(self, vertices, color=(0.5, 0.5, 0.5), indices=None, per_vertex_color=None)#
Draw a set of 2D triangles on this canvas.
Parameters:
vertices – a taichi 2D Vector field, where each element indicate                 the 3D location of a vertex.
indices – a taichi int field of shape (3 * #triangles), which                 indicate the vertex indices of the triangles. If this is None,                 then it is assumed that the vertices are already arranged in                 triangles order.
color – a global color for the triangles as 3 floats representing                 RGB values. If per_vertex_color is provided, this is ignored.
per_vertex_color (Tuple[float]) – a taichi 3D vector field,                 where each element indicate the RGB color of a vertex.
vector_field(self, vector_field, arrow_spacing=5, scale=0.1, width=0.002, color=(0, 0, 0))#
Draw a vector field on this canvas.
Parameters:
vector_field – The vector field to be plotted on the canvas.
arrow_spacing (int) – Spacing used when sample on the vector field.
scale (float) – Maximum vector length proportional to the canvas.
width (float) – Line width when drawing the arrow.
color (tuple[float]) – The RGB color of arrows.
taichi.ui.DOWN = Down#
taichi.ui.ESCAPE = Escape#
class taichi.ui.GUI(name='Taichi', res=512, background_color=0, show_gui=True, fullscreen=False, fast_gui=False)#
Taichi Graphical User Interface class.
Parameters:
name (str, optional) – The name of the GUI to be constructed.
Default is ‘Taichi’.
res (Union[int, List[int]], optional) – The resolution of created
GUI. Default is 512*512. If res is scalar, then width will be equal to height.
background_color (int, optional) – The background color of created GUI.
Default is 0x000000.
show_gui (bool, optional) – Specify whether to render the GUI. Default is True.
fullscreen (bool, optional) – Specify whether to render the GUI in
fullscreen mode. Default is False.
fast_gui (bool, optional) – Specify whether to use fast gui mode of
Taichi. Default is False.
Returns:
The created taichi GUI object.
Return type:
GUI
class Event#
Class for holding a gui event.
An event is represented by:
type (PRESS, MOTION, RELEASE)
modifier (modifier keys like ctrl, shift, etc)
pos (mouse position)
key (event key)
delta (for holding mouse wheel)
class EventFilter(*e_filter)#
A set to store detected user events.
match(self, e)#
Check if a specified event e is among the detected events.
class WidgetValue(gui, wid)#
Class for maintaining id of gui widgets.
ALT = Alt#
BACKSPACE = BackSpace#
CAPSLOCK = Caps_Lock#
CTRL = Control#
DOWN = Down#
ESCAPE = Escape#
EXIT = WMClose#
LEFT = Left#
LMB = LMB#
MMB = MMB#
MOTION#
MOVE = Motion#
PRESS#
RELEASE#
RETURN = Return#
RIGHT = Right#
RMB = RMB#
SHIFT = Shift#
SPACE =#
TAB = Tab#
UP = Up#
WHEEL = Wheel#
arrow(self, orig, direction, radius=1, color=16777215, **kwargs)#
Draws a single arrow on canvas.
Parameters:
orig (List[Number]) – The position where arrow starts. Shape must be 2.
direction (List[Number]) – The direction where arrow points to. Shape must be 2.
radius (Number, optional) – The width of arrow. Default is 1.
color (int, optional) – The color of arrow. Default is 0xFFFFFF.
arrow_field(self, direction, radius=1, color=16777215, bound=0.5, **kwargs)#
Draw a field of arrows on canvas.
Parameters:
direction (np.array) – The pattern and direction of the field of arrows.
color (Union[int, np.array], optional) – The color or colors of arrows.
Default is 0xFFFFFF.
bound (Number, optional) – The boundary of the field. Default is 0.5.
arrows(self, orig, direction, radius=1, color=16777215, **kwargs)#
Draw a list arrows on canvas.
Parameters:
orig (numpy.array) – The positions where arrows start.
direction (numpy.array) – The directions where arrows point to.
radius (Union[Number, np.array], optional) – The width of arrows. Default is 1.
color (Union[int, np.array], optional) – The color or colors of arrows. Default is 0xffffff.
button(self, text, event_name=None)#
Create a button object on canvas to be manipulated with.
Parameters:
text (str) – The title of button.
event_name (str, optional) – The event name associated with button.
Default is WidgetButton_{text}
Returns:
The event name associated with created button.
circle(self, pos, color=16777215, radius=1)#
Draws a circle on canvas.
Parameters:
pos (Union[List[int], numpy.array]) – The position of the circle.
color (int, Optional) – The color of the circle. Default is 0xFFFFFF.
radius (Number, Optional) – The radius of the circle in pixel. Default is 1.
circles(self, pos, radius=1, color=16777215, palette=None, palette_indices=None)#
Draws a list of circles on canvas.
Parameters:
pos (numpy.array) – The positions of the circles.
radius (Union[Number, numpy.array], optional) – The radius of the circles in pixel.                 Can be either a number, which will be applied to all circles, or a 1D NumPy array of the same length as pos.                 The default is 1.
color (int, optional) – The color of the circles. Default is 0xFFFFFF.
palette (list[int], optional) – The List of colors from which to
choose to draw. Default is None.
palette_indices (Union[list[int], ti.field, numpy.array], optional) – The List of indices that choose color from palette for each
circle. Shape must match pos. Default is None.
clear(self, color=None)#
Clears the canvas with the color provided.
Parameters:
color (int, optional) – Specify the color to clear the canvas. Default
is the background color of GUI.
close(self)#
Close this GUI.
Example:
>>> while gui.running:
>>>     if gui.get_event(gui.PRESS, ti.GUI.ESCAPE):
>>>         gui.close()
>>>     gui.show()
contour(self, scalar_field, normalize=False)#
Plot a contour view of a scalar field.
The input scalar_field will be converted to a Numpy array first, and then plotted
by the Matplotlib colormap ‘Plasma’. Notice this method will automatically perform
a bilinear interpolation on the field if the size of the field does not match with
the GUI window size.
Parameters:
scalar_field (ti.field) – The scalar field being plotted.
normalize (bool, Optional) – Display the normalized scalar field if set to True.
False. (Default is) – 
cook_image(self, img)#
Converts an img to range [0, 1] for display.
The input image is stored in a numpy.ndarray, if it’s dtype
is int it will be rescaled and mapped into range [0, 1]. If
the dtype is float it will be directly casted to 32-bit float type.
static get_bool_environ(key, default)#
Get an environment variable and cast it to bool.
Parameters:
key (str) – The environment variable key.
default (bool) – The default value.
Returns:
The environment variable value cast to bool.             If the value is not found, directly return argument ‘default’.
get_cursor_pos(self)#
Returns the current position of mouse as a pair of floats
in the range [0, 1] x [0, 1].
The origin of the coordinates system is located at the lower left
corner, with +x direction points to the right, and +y direcntion
points upward.
Returns:
The current position of mouse.
get_event(self, *e_filter)#
Checks if the specified event is triggered.
Parameters:
*e_filter (ti.GUI.EVENT) – The specific event to be checked.
Returns:
whether or not the specified event is triggered.
Return type:
bool
get_events(self, *e_filter)#
Gets a list of events that are triggered.
Parameters:
*e_filter (List[ti.GUI.EVENT]) – The type of events to be filtered.
Returns:
A list of events that are triggered.
Return type:
EVENT
get_image(self)#
Return the window content as an numpy.ndarray.
Returns:
The image data in numpy contiguous array type.
Return type:
numpy.array
get_key_event(self)#
Gets keyboard triggered event.
Returns:
The keyboard triggered event.
Return type:
EVENT
has_key_event(self)#
Check if there is any key event registered.
Returns:
whether or not there is any key event registered.
Return type:
bool
is_pressed(self, *keys)#
Checks if any key among a set of specified keys is pressed.
Parameters:
*keys (Union[str, List[str]]) – The keys to be listened to.
Returns:
whether or not any key among the specified keys is pressed.
Return type:
bool
label(self, text)#
Creates a label object on canvas.
Parameters:
text (str) – The title of label.
Returns:
The created label object.
Return type:
WidgetValue
line(self, begin, end, radius=1, color=16777215)#
Draws a single line on canvas.
Parameters:
begin (List[Number]) – The position of one end of line. Shape must be 2.
end (List[Number]) – The position of the other end of line. Shape must be 2.
radius (Number, optional) – The width of line. Default is 1.
color (int, optional) – The color of line. Default is 0xFFFFFF.
lines(self, begin, end, radius=1, color=16777215)#
Draw a list of lines on canvas.
Parameters:
begin (numpy.array) – The positions of one end of lines.
end (numpy.array) – The positions of the other end of lines.
radius (Union[Number, numpy.array], optional) – The width of lines.
Can be either a single width or a list of width whose shape matches
the shape of begin & end. Default is 1.
color (Union[int, numpy.array], optional) – The color or colors of lines.
Can be either a single color or a list of colors whose shape matches
the shape of begin & end. Default is 0xFFFFFF.
point_field(self, radius, color=16777215, bound=0.5)#
Draws a field of points on canvas.
Parameters:
radius (np.array) – The pattern and radius of the field of points.
color (Union[int, np.array], optional) – The color or colors of points.
Default is 0xFFFFFF.
bound (Number, optional) – The boundary of the field. Default is 0.5.
rect(self, topleft, bottomright, radius=1, color=16777215)#
Draws a single rectangle on canvas.
Parameters:
topleft (List[Number]) – The position of the topleft corner of rectangle.
Shape must be 2.
bottomright (List[Number]) – The position of the bottomright corner
of rectangle. Shape must be 2.
radius (Number, optional) – The width of rectangle’s sides. Default is 1.
color (int, optional) – The color of rectangle. Default is 0xFFFFFF.
set_image(self, img)#
Sets an image to display on the window.
The image pixels are set from the values of img[i, j], where i indicates
the horizontal coordinates (from left to right) and j the vertical coordinates
(from bottom to top).
If the window size is (x, y), then img must be one of:
ti.field(shape=(x, y)), a gray-scale image
ti.field(shape=(x, y, 3)), where 3 is for (r, g, b) channels
ti.field(shape=(x, y, 2)), where 2 is for (r, g) channels
ti.Vector.field(3, shape=(x, y)) (r, g, b) channels on each component
ti.Vector.field(2, shape=(x, y)) (r, g) channels on each component
np.ndarray(shape=(x, y))
np.ndarray(shape=(x, y, 3))
np.ndarray(shape=(x, y, 2))
The data type of img must be one of:
uint8, range [0, 255]
uint16, range [0, 65535]
uint32, range [0, 4294967295]
float32, range [0, 1]
float64, range [0, 1]
Parameters:
img (Union[taichi.field, numpy.array]) – The color array                 representing the image to be drawn. Support greyscale, RG, RGB,                 and RGBA color representations. Its shape must match GUI resolution.
show(self, file=None)#
Shows the frame content in the gui window, or save the content to an
image file.
Parameters:
file (str, optional) – output filename. The default is None, and
the frame content is displayed in the gui window. If it’s a valid
image filename the frame will be saved as the specified image.
slider(self, text, minimum, maximum, step=1)#
Creates a slider object on canvas to be manipulated with.
Parameters:
text (str) – The title of slider.
minimum (int, float) – The minimum value of slider.
maximum (int, float) – The maximum value of slider.
step (int, float) – The changing step of slider. Optional and default to 1.
Returns:
The created slider object.
Return type:
WidgetValue
text(self, content, pos, font_size=15, color=16777215)#
Draws texts on canvas.
Parameters:
content (str) – The text to be drawn on canvas.
pos (List[Number]) – The position where the text is to be put.
font_size (Number, optional) – The font size of the text.
color (int, optional) – The color of the text. Default is 0xFFFFFF.
triangle(self, a, b, c, color=16777215)#
Draws a single triangle on canvas.
Parameters:
a (List[Number]) – The position of the first point of triangle. Shape must be 2.
b (List[Number]) – The position of the second point of triangle. Shape must be 2.
c (List[Number]) – The position of the third point of triangle. Shape must be 2.
color (int, optional) – The color of the triangle. Default is 0xFFFFFF.
triangles(self, a, b, c, color=16777215)#
Draws a list of triangles on canvas.
Parameters:
a (numpy.array) – The positions of the first points of triangles.
b (numpy.array) – The positions of the second points of triangles.
c (numpy.array) – The positions of the third points of triangles.
color (Union[int, numpy.array], optional) – The color or colors of triangles.
Can be either a single color or a list of colors whose shape matches
the shape of a & b & c. Default is 0xFFFFFF.
vector_field(self, vector_field, arrow_spacing=5, color=16777215)#
Display a vector field on canvas.
Parameters:
vector_field (ti.Vector.field) – The vector field being displayed.
arrow_spacing (int, optional) – The spacing between vectors.
color (Union[int, np.array], optional) – The color of vectors.
class taichi.ui.Gui(gui)#
For declaring IMGUI components in a taichi.ui.Window
created by the GGUI system.
Parameters:
gui – reference to a PyGui.
begin(self, name, x, y, width, height)#
Creates a subwindow that holds imgui widgets.
All widget function calls (e.g. text, button) after the begin
and before the next end will describe the widgets within this subwindow.
Parameters:
x (float) – The x-coordinate (between 0 and 1) of the top-left                 corner of the subwindow, relative to the full window.
y (float) – The y-coordinate (between 0 and 1) of the top-left                 corner of the subwindow, relative to the full window.
width (float) – The width of the subwindow relative to the full window.
height (float) – The height of the subwindow relative to the full window.
button(self, text)#
Declares a button, and returns whether or not it had just been clicked.
Parameters:
text (str) – a line of text to be shown next to the button.
checkbox(self, text, old_value)#
Declares a checkbox, and returns whether or not it has been checked.
Parameters:
text (str) – a line of text to be shown next to the checkbox.
old_value (bool) – whether the checkbox is currently checked.
color_edit_3(self, text, old_value)#
Declares a color edit palate.
Parameters:
text (str) – a line of text to be shown next to the palate.
old_value (Tuple[float]) – the current value of the color, this                 should be a tuple of floats in [0,1] that indicates RGB values.
end(self)#
End the description of the current subwindow.
slider_float(self, text, old_value, minimum, maximum)#
Declares a slider, and returns its newest value.
Parameters:
text (str) – a line of text to be shown next to the slider
old_value (float) – the current value of the slider.
minimum (float) – the minimum value of the slider.
maximum (float) – the maximum value of the slider.
slider_int(self, text, old_value, minimum, maximum)#
Declares a slider, and returns its newest value.
Parameters:
text (str) – a line of text to be shown next to the slider
old_value (int) – the current value of the slider.
minimum (int) – the minimum value of the slider.
maximum (int) – the maximum value of the slider.
Returns:
the updated value of the slider.
Return type:
int
sub_window(self, name, x, y, width, height)#
Creating a context manager for subwindow.
Note
All args of this method should align with begin.
Parameters:
x (float) – The x-coordinate (between 0 and 1) of the top-left                 corner of the subwindow, relative to the full window.
y (float) – The y-coordinate (between 0 and 1) of the top-left                 corner of the subwindow, relative to the full window.
width (float) – The width of the subwindow relative to the full window.
height (float) – The height of the subwindow relative to the full window.
Example:
>>> with gui.sub_window(name, x, y, width, height) as g:
>>>     g.text("Hello, World!")
text(self, text, color=None)#
Declares a line of text.
taichi.ui.LEFT = Left#
taichi.ui.LMB = LMB#
taichi.ui.MMB = MMB#
taichi.ui.PRESS = Press#
taichi.ui.ProjectionMode#
Camera projection mode, 0 for perspective and 1 for orthogonal.
taichi.ui.RELEASE = Release#
taichi.ui.RETURN = Return#
taichi.ui.RIGHT = Right#
taichi.ui.RMB = RMB#
taichi.ui.SHIFT = Shift#
taichi.ui.SPACE =#
class taichi.ui.Scene#
The 3D scene class, which can contain meshes and particles,
and can be rendered on a canvas.
ambient_light(self, color)#
Set the ambient color of this scene.
Example:
>>> scene = ti.ui.Scene()
>>> scene.ambient_light([0.2, 0.2, 0.2])
lines(self, vertices, width, indices=None, color=(0.5, 0.5, 0.5), per_vertex_color=None, vertex_offset: int = 0, vertex_count: int = None, index_offset: int = 0, index_count: int = None)#
Declare multi-lines inside the scene.
Note that under current situation, for example, there you have 4 vertices,
vertices.shape[0] is 4. So there will be 2 lines, the first line’s two points
are vertices[0] and vertices[1], and the second line’s two points are
vertices[2] and vertices[3].
Parameters:
vertices – a taichi 3D Vector field, where each element indicate the
3D location of points of lines.
width – the line width (maybe different on different systems).
indices – a taichi int field of shape (2 * #points), which indicate
the points indices of the lines. If this is None, then it is
assumed that the points are already arranged in lines order.
color – a global color of the mesh as 3 floats representing RGB values.
If per_vertex_color is provided, this is ignored.
per_vertex_color (Tuple[float]) – a taichi 3D vector field, where each
element indicate the RGB color of the line.
vertex_offset (int, optional) – if ‘indices’ is provided, this refers to the value added to the vertex
index before indexing into the vertex buffer, else this refers to the
index of the first vertex to draw.
vertex_count (int, optional) – only available when indices is not provided, which is the number
of vertices to draw. There are 2 cases that we will change your
vertex_count. [1] If the vertex_count is an odd number, then we
will change it to vertex_count - 1. [2] If vertex_offset plus
vertex_count greater than vertices.shape[0], then we will reduce
vertex_count to no more than vertices.shape[0].
index_offset (int, optional) – Only available when indices is provided, which is the base index
within the index buffer.
index_count (int, optional) – Only available when indices is provided, which is the number
of vertices to draw.
mesh(self, vertices, indices=None, normals=None, color=(0.5, 0.5, 0.5), per_vertex_color=None, two_sided=False, vertex_offset: int = 0, vertex_count: int = None, index_offset: int = 0, index_count: int = None, show_wireframe: bool = False)#
Declare a mesh inside the scene.
if you indicate the index_offset and index_count, the normals will also
be sliced by the args, and the shading resultes will not be affected.
(It is equal to make a part of the mesh visible)
Parameters:
vertices – a taichi 3D Vector field, where each element indicate the
3D location of a vertex.
indices – a taichi int field of shape (3 * #triangles), which indicate
the vertex indices of the triangles. If this is None, then it is
assumed that the vertices are already arranged in triangles order.
normals – a taichi 3D Vector field, where each element indicate the
normal of a vertex. If this is none, normals will be automatically
inferred from vertex positions.
color – a global color of the mesh as 3 floats representing RGB values.
If per_vertex_color is provided, this is ignored.
per_vertex_color (Tuple[float]) – a taichi 3D vector field, where each
element indicate the RGB color of a vertex.
two_sided (bool) – whether or not the triangles should be able to be
seen from both sides.
vertex_offset (int, optional) – if ‘indices’ is provided, this refers to the value added to the vertex
index before indexing into the vertex buffer, else this refers to the
index of the first vertex to draw.
vertex_count (int, optional) – only available when indices is not provided, which is the number
of vertices to draw.
index_offset (int, optional) – only available when indices is provided, which is the base index
within the index buffer.
index_count (int, optional) – only available when indices is provided, which is the the number
of vertices to draw.
show_wireframe (bool, optional) – turn on/off WareFrame mode.
mesh_instance(self, vertices, indices=None, normals=None, color=(0.5, 0.5, 0.5), per_vertex_color=None, two_sided=False, transforms=None, instance_offset: int = 0, instance_count: int = None, vertex_offset: int = 0, vertex_count: int = None, index_offset: int = 0, index_count: int = None, show_wireframe: bool = False)#
Declare mesh instances inside the scene.
If transforms is given, then according to the shape of transforms, it will
draw mesh instances based on the transforms, and you can indicate which instance
to draw first. If you indicate the index_offset and index_count, the normals will also
be sliced by the args, and the shading resultes will not be affected.
(It is equal to make a part of the mesh visible)
Parameters:
vertices – a taichi 3D Vector field, where each element indicate the
3D location of a vertex.
indices – a taichi int field of shape (3 * #triangles), which indicate
the vertex indices of the triangles. If this is None, then it is
assumed that the vertices are already arranged in triangles order.
normals – a taichi 3D Vector field, where each element indicate the
normal of a vertex. If this is none, normals will be automatically
inferred from vertex positions.
color – a global color of the mesh as 3 floats representing RGB values.
If per_vertex_color is provided, this is ignored.
per_vertex_color (Tuple[float]) – a taichi 3D vector field, where each
element indicate the RGB color of a vertex.
two_sided (bool) – whether or not the triangles should be able to be
seen from both sides.
transforms (ti.Matrix.field, optional) – The Matrix must be 4x4 size with N instances, and data type should
be ti.f32, ti.i32, ti.u32. If None, then it behaves like raw mesh (no copy).
instance_offset (int, optional) – Default value is 0 which means no offset to show mesh instances. Otherwise,
the mesh instances will show from the instance_offset.
instance_count (int, optional) – The default value is None. If this parameter is not provided, instance_count = transforms.shape[0] - instance_offset.
vertex_offset (int, optional) – if ‘indices’ is provided, this refers to the value added to the vertex
index before indexing into the vertex buffer, else this refers to the
index of the first vertex to draw.
vertex_count (int, optional) – only available when indices is not provided, which is the number
of vertices to draw.
index_offset (int, optional) – only available when indices is provided, which is the base index
within the index buffer.
index_count (int, optional) – only available when indices is provided, which is the the number
of indices to draw.
show_wireframe (bool, optional) – turn on/off WareFrame mode.
particles(self, centers, radius, color=(0.5, 0.5, 0.5), per_vertex_color=None, index_offset: int = 0, index_count: int = None)#
Declare a set of particles within the scene.
Parameters:
centers – a taichi 3D Vector field, where each element indicate the
3D location of the center of a triangle.
color – a global color for the particles as 3 floats representing RGB
values. If per_vertex_color is provided, this is ignored.
per_vertex_color (Tuple[float]) – a taichi 3D vector field, where each
element indicate the RGB color of a particle.
index_offset (int, optional) – the index of the first vertex to draw.
index_count (int, optional) – the number of vertices to draw.
point_light(self, pos, color)#
Set a point light in this scene.
Parameters:
pos (list, tuple, vector(3, float)) – 3D vector for light position.
color (list, tuple, vector(3, float)) – (r, g, b) triple for the color of the light, in the range [0, 1].
set_camera(self, camera)#
Set the camera for this scene.
Parameters:
camera (Camera) – A camera instance.
taichi.ui.TAB = Tab#
taichi.ui.UP = Up#
class taichi.ui.Window(name, res, vsync=False, show_window=True, fps_limit=1000, pos=(100, 100))#
The window class.
Parameters:
name (str) – Window title.
res (tuple[int]) – resolution (width, height) of the window, in pixels.
vsync (bool) – whether or not vertical sync should be enabled.
show_window (bool) – where or not display the window after initialization.
pos (tuple[int]) – position (left to right, up to bottom) of the window which origins from the left-top of your main screen, in pixels.
destroy(self)#
Destroy this window. The window will be unavailable then.
get_canvas(self)#
Returns a canvas handle. See :class`~taichi.ui.canvas.Canvas`
get_cursor_pos(self)#
Get current cursor position, in the range [0, 1] x [0, 1].
get_depth_buffer(self, depth)#
fetch the depth information of current scene to ti.ndarray/ti.field(support copy from vulkan to cuda/cpu which is a faster version)
Parameters:
depth (ti.ndarray/ti.field) – [window_width, window_height] carries depth information.
get_depth_buffer_as_numpy(self)#
Get the depth information of current scene to numpy array.
Returns:
[width, height] with (0.0~1.0) float-format.
Return type:
2d numpy array
get_event(self, tag=None)#
Returns whether or not a event that matches tag has occurred.
If tag is None, then no filters are applied. If this function
returns True, the event property of the window will be set
to the corresponding event.
get_events(self, tag=None)#
Get the current list of unprocessed events.
Parameters:
tag (str) – A tag used for filtering events.                 If it is None, then all events are returned.
get_gui(self)#
Returns a IMGUI handle. See :class`~taichi.ui.ui.Gui`
get_image_buffer_as_numpy(self)#
Get the window content to numpy array.
Returns:
[width, height, channels] with (0.0~1.0) float-format color.
Return type:
3d numpy array
get_window_shape(self)#
Return the shape of window.
:returns: (width, height)
:rtype: tuple
is_pressed(self, *keys)#
Checks if any of a set of specified keys is pressed.
Parameters:
keys (list[constants]) – The keys to be matched.
Returns:
True if any key among keys is pressed, else False.
Return type:
bool
save_image(self, filename)#
Save the window content to an image file.
Parameters:
filename (str) – output filename.
show(self)#
Display this window.
taichi.ui.check_ggui_availability()#
Checks if the GGUI environment is available.
taichi.ui.hex_to_rgb(color)#
Converts hex color format to rgb color format.
Parameters:
color (int) – The hex representation of color.
Returns:
The rgb representation of color.
taichi.ui.rgb_to_hex(c)#
Converts rgb color format to hex color format.
Parameters:
c (List[int]) – The rgb representation of color.
Returns:
The hex representation of color.
Was this helpful?ALTBACKSPACECAPSLOCKCTRLCameraCamera.bottom()Camera.fov()Camera.get_projection_matrix()Camera.get_view_matrix()Camera.left()Camera.lookat()Camera.position()Camera.projection_mode()Camera.right()Camera.top()Camera.track_user_inputs()Camera.up()Camera.z_far()Camera.z_near()CanvasCanvas.circles()Canvas.contour()Canvas.lines()Canvas.scene()Canvas.set_background_color()Canvas.set_image()Canvas.triangles()Canvas.vector_field()DOWNESCAPEGUIGUI.EventGUI.EventFilterGUI.EventFilter.match()GUI.WidgetValueGUI.ALTGUI.BACKSPACEGUI.CAPSLOCKGUI.CTRLGUI.DOWNGUI.ESCAPEGUI.EXITGUI.LEFTGUI.LMBGUI.MMBGUI.MOTIONGUI.MOVEGUI.PRESSGUI.RELEASEGUI.RETURNGUI.RIGHTGUI.RMBGUI.SHIFTGUI.SPACEGUI.TABGUI.UPGUI.WHEELGUI.arrow()GUI.arrow_field()GUI.arrows()GUI.button()GUI.circle()GUI.circles()GUI.clear()GUI.close()GUI.contour()GUI.cook_image()GUI.get_bool_environ()GUI.get_cursor_pos()GUI.get_event()GUI.get_events()GUI.get_image()GUI.get_key_event()GUI.has_key_event()GUI.is_pressed()GUI.label()GUI.line()GUI.lines()GUI.point_field()GUI.rect()GUI.set_image()GUI.show()GUI.slider()GUI.text()GUI.triangle()GUI.triangles()GUI.vector_field()matchGuiGui.begin()Gui.button()Gui.checkbox()Gui.color_edit_3()Gui.end()Gui.slider_float()Gui.slider_int()Gui.sub_window()Gui.text()LEFTLMBMMBPRESSProjectionModeRELEASERETURNRIGHTRMBSHIFTSPACESceneScene.ambient_light()Scene.lines()Scene.mesh()Scene.mesh_instance()Scene.particles()Scene.point_light()Scene.set_camera()TABUPWindowWindow.destroy()Window.get_canvas()Window.get_cursor_pos()Window.get_depth_buffer()Window.get_depth_buffer_as_numpy()Window.get_event()Window.get_events()Window.get_gui()Window.get_image_buffer_as_numpy()Window.get_window_shape()Window.is_pressed()Window.save_image()Window.show()check_ggui_availability()hex_to_rgb()rgb_to_hex()ALTBACKSPACECAPSLOCKCTRLCamerabottomfovget_projection_matrixget_view_matrixleftlookatpositionprojection_moderighttoptrack_user_inputsupz_farz_nearCanvascirclescontourlinessceneset_background_colorset_imagetrianglesvector_fieldDOWNESCAPEGUIEventEventFilterWidgetValueALTBACKSPACECAPSLOCKCTRLDOWNESCAPEEXITLEFTLMBMMBMOTIONMOVEPRESSRELEASERETURNRIGHTRMBSHIFTSPACETABUPWHEELarrowarrow_fieldarrowsbuttoncirclecirclesclearclosecontourcook_imageget_bool_environget_cursor_posget_eventget_eventsget_imageget_key_eventhas_key_eventis_pressedlabellinelinespoint_fieldrectset_imageshowslidertexttriangletrianglesvector_fieldGuibeginbuttoncheckboxcolor_edit_3endslider_floatslider_intsub_windowtextLEFTLMBMMBPRESSProjectionModeRELEASERETURNRIGHTRMBSHIFTSPACESceneambient_lightlinesmeshmesh_instanceparticlespoint_lightset_cameraTABUPWindowdestroyget_canvasget_cursor_posget_depth_bufferget_depth_buffer_as_numpyget_eventget_eventsget_guiget_image_buffer_as_numpyget_window_shapeis_pressedsave_imageshowcheck_ggui_availabilityhex_to_rgbrgb_to_hexCopyright © 2023 Taichi Graphics Technology, Inc.ResourcesBlogsNewslettersUser StoriesTaichi Graphics CourseCommunityGlobal Forum中文论坛DiscordWechatLEGALCookie Policy Privacy Policy SubscriptionSubscribeCopyright © 2023 Taichi Graphics Technology, Inc.
